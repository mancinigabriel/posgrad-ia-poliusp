{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <font face=\"Verdana\" size=6 color='#6495ED'> IAD-004 APRENDIZAGEM DE MÁQUINA 1\n",
        "<font face=\"Verdana\" size=3 color='#40E0D0'> Professores Larissa Driemeier e Thiago Martins\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1J3dF7v9apzpj27oOsrT8aEagtNIYwq7J' width=\"600\"></center>\n",
        "\n",
        "O objetivo deste Notebook é fornecer uma descrição fundamental de aprendizado Não supervisionado, abordando conceitos fundamentais e mesclando teoria com códigos em Python. Baseado na aula [IAD-007](https://alunoweb.net/moodle/pluginfile.php/117113/mod_resource/content/6/Aula07_PCA.pdf), ano 2024."
      ],
      "metadata": {
        "id": "SoZ4nKE70kst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Os seguintes trabalhos foram extremamente importantes na elaboração deste material:\n",
        "* Clusterização\n",
        "1. Suraj Yadav, [Silhouette Coefficient Explained with a Practical Example: Assessing Cluster Fit](https://medium.com/@Suraj_Yadav/silhouette-coefficient-explained-with-a-practical-example-assessing-cluster-fit-c0bb3fdef719), 2023.\n",
        "\n",
        "* PCA\n",
        "1. [Dimensionality Reduction: Principal Component Analysis in-depth](https://nbviewer.jupyter.org/github/jakevdp/sklearn_pycon2015/blob/master/notebooks/04.1-Dimensionality-PCA.ipynb), de Jake Vanderplas, 2015.\n",
        "2. [A tutorial on Principal Components Analysis](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf), de Lindsay I Smith, 2002.\n",
        "3. [Implementing a Principal Component Analysis (PCA)](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html#what-is-a-good-subspace), de Sebastian Raschka, 2014.\n",
        "4. [A Structural Risk Minimization Approach to Principal Component Analysis](https://colab.research.google.com/github/foxtrotmike/PCA-Tutorial/blob/master/pca.ipynb#scrollTo=isMQxiFkxf_w), de Fayyaz Minhas.\n",
        "5. [PCA and SVD](https://intoli.com/blog/pca-and-svd/), de Andre Perunicic, 2017.\n",
        "6. [Principal Component Analysis Made Easy: A Step-by-Step Tutorial](https://towardsdatascience.com/principal-component-analysis-made-easy-a-step-by-step-tutorial-184f295e97fe), de Marcus Sena, 2024.\n",
        "\n",
        "> <small>*The world doesn't need a yet another PCA tutorial, just like the world doesn't need another silly love song.  But sometimes you still get the urge to write your own.*, Scott H. Hawley, 2019</small>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZkRD_m4Z1ows"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from numpy.random import uniform\n",
        "from sklearn.datasets import make_blobs\n",
        "import seaborn as sns\n",
        "import random\n",
        "from numpy import exp, sqrt, array, abs\n",
        "sns.set()\n",
        "import math\n",
        "from mpl_toolkits import mplot3d"
      ],
      "metadata": {
        "id": "Xnh99Fq1drDw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aprendizado não supervisionado\n",
        "\n",
        "A aprendizagem não supervisionada é um conjunto de ferramentas estatísticas para cenários em que existe apenas um conjunto de recursos e nenhum rótulo.\n",
        "Portanto, não podemos fazer previsões, uma vez que não existem respostas associadas a cada observação. Em vez disso, estamos interessados em encontrar uma forma interessante de visualizar dados ou em descobrir subgrupos de observações semelhantes.\n",
        "\n",
        "Duas técnicas serão o foco desta aula:  agrupamento (clustering) e análise de componentes principais ou PCA (do inglês, Principal Component Analysis)."
      ],
      "metadata": {
        "id": "VDMQNBbN0yw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clusterização por K-means\n",
        "\n",
        "Um dos algoritmos mais populares e amplamente utilizados para tarefas de cluster é o K-means. Um algoritmo de agrupamento K-means tenta agrupar itens semelhantes na forma de clusters. O número de grupos é representado por K.\n",
        "\n",
        "O algoritmo iterativo baseado em centróide que cria clusters não sobrepostos.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=13ylU4sxfjMKoD72fRtCk0XNXnWH3UvhQ' width=\"600\"></center>\n"
      ],
      "metadata": {
        "id": "j9j5NLme01bm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classe `KMeans` foi adaptada do [link](https://towardsdatascience.com/create-your-own-k-means-clustering-algorithm-in-python-d7d4c9077670).\n",
        "\n",
        "Para um determinado conjunto de dados `X_train`, `n_cluster` é especificado como o número de clusters distintos aos quais os dados pertencem e `max_iter` será o número máximo de iterações para otimizar a clusterização.\n",
        "\n",
        "Os `n_cluster` centroides são primeiro inicializados aleatoriamente e, em seguida, iterações são realizadas para otimizar as suas localizações da seguinte forma:\n",
        "* A distância Euclidiana de cada ponto a cada centróide é calculada:\n",
        "$$\n",
        "d_{ij} = \\sqrt{\\left(x_1^{(i)}-c_j\\right)^2+\\left(x_2^{(i)}-c_j\\right)^2+ \\cdots +\\left(x_n^{(i)}-c_j\\right)^2}\n",
        "$$\n",
        " onde $n$ é a dimensão dos dados, $i$ é cada ponto de dado, e $j$ cada centroide.\n",
        "* Os pontos são atribuídos ao centróide mais próximo.\n",
        "* Os centróides são deslocados para serem o valor médio dos pontos pertencentes a ele.\n",
        "* Repete-se o algoritmo até os centróides não se moverem, ouser atingido o número máximo de iterações."
      ],
      "metadata": {
        "id": "HvjLIBsIfJdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KMeans:\n",
        "\n",
        "    def __init__(self, n_clusters=8, max_iter=1000, centroid_init = 'firstRandom'):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iter = max_iter\n",
        "        self.centroid_init = centroid_init\n",
        "\n",
        "    def _euclidean(self,x,X):\n",
        "        '''\n",
        "        Calcula a distância euclidiana entre um ponto de dados x e\n",
        "        todos os outros pontos no conjunto de dados, denotado como X\n",
        "\n",
        "        Argumentos:\n",
        "          x: ponto de dados para o qual as distâncias serão calculadas\n",
        "          X: o conjunto de dados incluindo todos os pontos de dados\n",
        "        '''\n",
        "        return np.sqrt(np.sum((x - X)**2, axis=1))\n",
        "\n",
        "    def _initialise_centroids(self,data):\n",
        "        ## 3 ways to initialize centroides\n",
        "        if(self.centroid_init == 'random'):\n",
        "            initial_centroids = np.random.permutation(data.shape[0])[:self.n_clusters]\n",
        "            self.centroids = data[initial_centroids]\n",
        "        elif(self.centroid_init == 'firstRandom'):\n",
        "            # Um ponto aleatório dos dados é escolhido para o primeiro centróide\n",
        "            # então o restante é inicializado com probabilidades proporcionais às suas distâncias até o primeiro\n",
        "            self.centroids = [random.choice(data)]\n",
        "            for _ in range(self.n_clusters-1):\n",
        "                # Calcular distâncias dos pontos aos centróides\n",
        "                dists = np.sum([self._euclidean(centroid, data) for centroid in self.centroids], axis=0)\n",
        "                # Normalizar as distâncias\n",
        "                dists /= np.sum(dists)\n",
        "                # Escolha os centroides restantes com base em suas distâncias\n",
        "                # veja que, na linha a seguir, a probabilidade de um ponto ser escolhido é\n",
        "                # a distância normalizada ao centroide \"dists\"\n",
        "                new_centroid_idx = np.random.choice(range(len(data)), p=dists)\n",
        "                self.centroids += [data[new_centroid_idx]]\n",
        "        else:\n",
        "        ## primeiros k valores do dataset\n",
        "            self.centroids = data[:k]\n",
        "        return self.centroids\n",
        "\n",
        "    def fit(self, X_train):\n",
        "        '''\n",
        "        Aloca os dados em clusters\n",
        "        Input : os dados de treinamento (X_train)\n",
        "        Output: a posição dos centroides (self.centroids) e\n",
        "        '''\n",
        "        self._initialise_centroids(X_train)\n",
        "\n",
        "        # Iterate, adjusting centroids until converged or until passed max_iter\n",
        "        iteration = 0\n",
        "        prev_centroids = None\n",
        "        while np.not_equal(self.centroids, prev_centroids).any() and iteration < self.max_iter:\n",
        "            # Classificar cada ponto de dados, alocando-o no centroide mais próximo\n",
        "            sorted_points = [[] for _ in range(self.n_clusters)]\n",
        "            centroid_idxs = []\n",
        "            for x in X_train:\n",
        "                dists = self._euclidean(x,self.centroids)\n",
        "                centroid_idx = np.argmin(dists)\n",
        "                sorted_points[centroid_idx].append(x)\n",
        "\n",
        "                centroid_idxs.append(centroid_idx)\n",
        "            # Atualize centroides prévios\n",
        "            #  reatribua os centróides como média dos pontos pertencentes a eles\n",
        "            prev_centroids = self.centroids\n",
        "            self.centroids = [np.mean(cluster, axis=0) for cluster in sorted_points]\n",
        "            for i, centroid in enumerate(self.centroids):\n",
        "                if np.isnan(centroid).any():  # Capturar qualquer np.nans, resultante de um centroide sem pontos\n",
        "                    self.centroids[i] = prev_centroids[i]\n",
        "            iteration += 1\n",
        "        return self.centroids, sorted_points, centroid_idxs\n",
        "\n",
        "    def evaluate(self, X_test):\n",
        "            centroid_idxs = []\n",
        "            for x in X_test:\n",
        "                dists = self._euclidean(x,self.centroids)\n",
        "                centroid_idx = np.argmin(dists)\n",
        "                centroid_idxs.append(centroid_idx)\n",
        "            return centroid_idxs"
      ],
      "metadata": {
        "id": "0zPzHyVzd3WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "centers = 5\n",
        "X_train, true_labels_train = make_blobs(n_samples=2000, centers=centers, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "sns.scatterplot(x=[X[0] for X in X_train],\n",
        "                y=[X[1] for X in X_train],\n",
        "                hue=true_labels_train,\n",
        "                palette=\"Set1\", alpha = 0.6,\n",
        "                legend=None\n",
        "                );"
      ],
      "metadata": {
        "id": "Cx8XxIYtIBjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajustar os centroides ao dataset\n",
        "kmeans = KMeans(n_clusters=centers)\n",
        "centroids, sorted_points, labels_train = kmeans.fit(X_train)"
      ],
      "metadata": {
        "id": "nJDci8O0e6YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver resultados\n",
        "sns.scatterplot(x=[X[0] for X in X_train],\n",
        "                y=[X[1] for X in X_train],\n",
        "                hue=labels_train,\n",
        "                palette=\"Set1\",\n",
        "                legend=None\n",
        "                )\n",
        "plt.plot([x for x, _ in centroids],\n",
        "         [y for _, y in centroids],\n",
        "         '+',\n",
        "         markersize=20,\n",
        "         )\n",
        "plt.title(\"k-means\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "98O_7vA6BqyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E agora, vamos escolher 20 pontos, e tentar classificar.\n",
        "\n",
        "Para escolher os 20 pontos, geramos de novo os dados artificiais. Porém, desta vez, geramos 30 dados a mais com mesmo `random_state=42` e pegamos somente os últimos 20, que não foram utilizados no treinamento.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gWuvk2NkEosi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "centers = 5\n",
        "t1, t2 = make_blobs(n_samples=2030, centers=centers, random_state=42)\n",
        "X_test =t1[-20:]\n",
        "true_labels_test = t2[-20:]\n",
        "\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Plotagem dos dados de teste\n",
        "sns.scatterplot(x=[X[0] for X in X_test],\n",
        "                y=[X[1] for X in X_test],\n",
        "                hue=true_labels_test,\n",
        "                palette=\"Set1\",\n",
        "                legend = None\n",
        "                )\n",
        "plt.plot([x for x, _ in centroids],\n",
        "         [y for _, y in centroids],\n",
        "         '+',\n",
        "         markersize=10,\n",
        "         )\n",
        "plt.title(\"k-means\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YVDX1IyXm4TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui vamos concatenar os dados de treino e teste, simplesmente para plotá-los em um único gráfico.\n",
        "\n",
        "Para melhor visualização dos dados de teste, usamos somente os 100 primeiros dados de treino.\n",
        "\n",
        "Veja que foi adicionada uma terceira coluna aos dados, além das coordenadas: a coluna é completada com `1` para dados de teste, que serão plotados com `+`, e com `0` para dados de treino, que serão plotados com `o`."
      ],
      "metadata": {
        "id": "dOn2az_Haxlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_conc = X_train[0:101].copy()\n",
        "c = [0]*len(X_conc)\n",
        "X_conc = np.insert(X_conc, 2, c, axis=1)\n",
        "X1 =  X_test.copy()\n",
        "c = [1]*len(X1)\n",
        "X1 = np.insert(X1, 2, c, axis=1)\n",
        "X_conc = np.insert(X_conc, 0, X1, axis=0)\n",
        "true_labels_conc = true_labels_train[0:101].copy()\n",
        "true_labels_conc = np.insert(true_labels_conc, 0, true_labels_test, axis=0)"
      ],
      "metadata": {
        "id": "X4M30FFuNGxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotagem dos dados de treino e teste juntos\n",
        "sns.scatterplot(x=[X[0] for X in X_conc],\n",
        "                y=[X[1] for X in X_conc],\n",
        "                style=X_conc[:,2],\n",
        "                hue=true_labels_conc,\n",
        "                palette=\"Set1\", alpha= 0.6,\n",
        "                legend = None\n",
        "                );"
      ],
      "metadata": {
        "id": "TfyMUMLFGEgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, vamos usar o método `evaluate` da classe `KMeans` para classificar os dados. Vejamos como será o desempenho de nosso modelo."
      ],
      "metadata": {
        "id": "VeTHDTyhbnsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = kmeans.evaluate(X_test)"
      ],
      "metadata": {
        "id": "GDCHlWxscD66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos plotar nosso gráfico novamente. Porém, ao invés de usar o `true_labels_test` (que, na verdade, não teríamos em uma situação real), e vamos usar o resultado do nosso modelo. Para isso, temos que reescrever o vetor de labels:"
      ],
      "metadata": {
        "id": "03w8TRXecYO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_conc = labels_train[0:101].copy()\n",
        "labels_conc = np.insert(labels_conc, 0, labels_test, axis=0)"
      ],
      "metadata": {
        "id": "Q7mz2iGlcUhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotagem dos dados de treino e teste juntos\n",
        "sns.scatterplot(x=[X[0] for X in X_conc],\n",
        "                y=[X[1] for X in X_conc],\n",
        "                style=X_conc[:,2],\n",
        "                hue=labels_conc,\n",
        "                palette=\"Set1\", alpha = 0.6,\n",
        "                legend = None\n",
        "                );"
      ],
      "metadata": {
        "id": "7GbFt861cQeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Escolha do hiperparâmetro $K$\n",
        "\n",
        "Os dados com os quais você trabalhará nem sempre terão demarcações distintas quando plotados. Aliás, muitas vezes, você lidará com dados de dimensões altas, que não podem ser plotados. Ou mesmo que sejam plotados, você não poderá determinar o número ótimo de agrupamentos.\n",
        "\n",
        "Os métodos do cotovelo e da silhueta são técnicas usadas para determinar o número ótimo de clusters em um algoritmo de agrupamento como o K-means.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T_LXtpMffeNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Método cotovelo\n",
        "\n",
        "A Soma das Quadrados Dentro do Agrupamento (WCSS, do inglês Within-Cluster Sum of Squares) é uma medida utilizada na análise de agrupamentos (clustering) para quantificar a variabilidade dos pontos de dados dentro de cada cluster. Em outras palavras, WCSS calcula a soma dos quadrados das distâncias de cada ponto de dados ao centróide do seu respectivo cluster,\n",
        "$$\n",
        "\\text{WCSS} = \\sum_{i=1}^{k} \\sum_{\\textbf{x}_j \\in C_i} \\| \\textbf{x}_j - \\boldsymbol{\\mu}_i \\|^2\n",
        "$$\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1bJlQd1mMjHLrCTS2_caJQzVQ4u10XejE' width=\"400\"></center>\n",
        "\n",
        "Onde:\n",
        "\n",
        "* $k$ é o número de clusters,\n",
        "* $C_i$ é o $i$-ésimo cluster,\n",
        "* $\\textbf{x}_j=\\begin{bmatrix}x_{j} & y_{j}\\end{bmatrix}^T$ é o $j$-ésimo ponto de dados,\n",
        "* $ \\boldsymbol{\\mu}_i =\\begin{bmatrix}x_{C_i} & y_{C_i}\\end{bmatrix}^T$ é o centróide do $i$-ésimo cluster,\n",
        "* $\\| \\textbf{x}_j - \\boldsymbol{\\mu}_i \\|^2$ é a distância quadrada entre o ponto de dados $\\textbf{x}_j$ e o centróide $ \\boldsymbol{\\mu}_i$.\n",
        "\n",
        "\n",
        "Quanto menor for o valor do WCSS, mais compactos e coesos são os clusters."
      ],
      "metadata": {
        "id": "gTYFeMZ2LE_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_wcss(data, labels, centroids,k):\n",
        "   \"\"\"\n",
        "   Calcula WCSS para diferentes valores de K no agrupamento K-means.\n",
        "   Parâmetros:\n",
        "   dados (array): os dados a serem agrupados.\n",
        "   max_k (int): O número máximo de clusters a serem testados.\n",
        "   Retorna:\n",
        "   valor WCSS para K\n",
        "   \"\"\"\n",
        "   wcss = 0\n",
        "   for i in range(k):\n",
        "        cluster_points = data[i]\n",
        "        wcss += np.sum((cluster_points - centroids[i]) ** 2)\n",
        "   return wcss"
      ],
      "metadata": {
        "id": "8nQaih-etTRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "centers = 5\n",
        "X_train, true_labels_train = make_blobs(n_samples=2000, centers=centers, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "qp3VrRxZ9eX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faixa de valores de K\n",
        "K_range = range(1, 11)\n",
        "\n",
        "# Lista para armazenar os valores WCSS para cada K\n",
        "wcss = []\n",
        "\n",
        "# Cálculo de WCSS de cada K\n",
        "wcss_values = []\n",
        "for k in K_range:\n",
        "   kmeans = KMeans(n_clusters=k)\n",
        "   centroids, sorted_points, labels = kmeans.fit(X_train)\n",
        "   wcss = calculate_wcss(sorted_points, labels, centroids,k)\n",
        "   wcss_values.append(wcss)\n",
        "\n",
        "# Plotagem do gráfico\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(K_range, wcss_values, 'bo-', markersize=8)\n",
        "plt.xlabel('Number of clusters (K)')\n",
        "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d6J86U8isk8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veja que, a partir de $K=3,4,5$ não há significante mudança na variação dos dados dentro dos clusters."
      ],
      "metadata": {
        "id": "QCl7ti4im81L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Método Silhueta\n",
        "\n",
        "O método da silhueta mede a similaridade e a dissimilaridade dos pontos de dados dentro dos clusters e entre clusters, respectivamente. A ideia é encontrar um número de clusters que maximize a similaridade dentro dos clusters e minimize a similaridade entre clusters.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1Z7TBDT6vbowbAUQITIb9zEAjo5Zd-aqd' width=\"800\"></center>\n",
        "\n",
        "__1. Cálculo da Similaridade Intra-cluster__\n",
        "\n",
        "Para cada ponto $i$ em um cluster, calcula-se a média das distâncias entre $i$ e todos os outros pontos no mesmo cluster. Essa medida é conhecida como $a(i)$:\n",
        "$$\n",
        "a(i) = \\frac{1}{|C| - 1} \\sum_{j \\in C, j \\neq i} d(i, j)\n",
        "$$\n",
        "onde $|C|$ é o número de pontos no cluster $C$ e $d(i, j)$ é a distância entre os pontos $i$ e $j$.\n",
        "\n",
        "__2. Cálculo da Dissimilaridade Inter-cluster__\n",
        "\n",
        "Para cada ponto $i$, calcula-se a média das distâncias entre $i$ e todos os pontos no cluster mais próximo ao qual $i$ não pertence. Essa medida é conhecida como $b(i)$:\n",
        "$$\n",
        "b(i) = \\min_{k \\neq C} \\frac{1}{|C_k|} \\sum_{j \\in C_k} d(i, j)\n",
        "$$\n",
        "onde $C_k$ é um cluster diferente do cluster $C$ ao qual o ponto $i$ pertence.\n",
        "\n",
        "__3. Cálculo do Coeficiente de Silhueta__\n",
        "\n",
        "Para cada ponto $i$, o coeficiente de silhueta é dado por:\n",
        "$$\n",
        "s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
        "$$\n",
        "O valor de $s(i)$ varia entre -1 e 1. Um valor próximo de 1 indica que o ponto está bem agrupado, enquanto um valor próximo de -1 indica que o ponto pode estar no cluster errado.\n",
        "\n",
        "__4. Cálculo da Média do Coeficiente de Silhueta__\n",
        "\n",
        "A média dos coeficientes de silhueta para todos os pontos em um cluster dá uma medida da qualidade do agrupamento. A média dos coeficientes de silhueta para todos os pontos no conjunto de dados dá uma medida global da qualidade do agrupamento:\n",
        "$$\n",
        "\\bar{s} = \\frac{1}{N} \\sum_{i=1}^{N} s(i)\n",
        "$$\n",
        "onde $N$ é o número total de pontos no conjunto de dados.\n",
        "\n",
        "__5. Escolha do Número Ótimo de Clusters__\n",
        "\n",
        "O número ótimo de clusters é aquele que maximiza a média dos coeficientes de silhueta. Em outras palavras, é o número de clusters que resulta na maior média de similaridade intra-cluster e dissimilaridade inter-cluster.\n",
        "\n",
        "Um coeficiente de silhueta de 1 denota que o ponto de dados está bem  *compactado*  do cluster ao qual pertence e distante dos outros clusters. O pior valor é -1. Valores próximos a 0 denotam clusters sobrepostos."
      ],
      "metadata": {
        "id": "vKCYSRgMSluT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance(point1, point2):\n",
        "    '''\n",
        "    Calcular a distância euclidiana entre dois pontos\n",
        "    INPUT: pontos (point2 e point2) e vetor com todos os pontos do cluster (cluster_points)\n",
        "    OUTPUT: distância Euclidiana entre point1 e point2\n",
        "    '''\n",
        "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
        "\n",
        "\n",
        "def mean_intra_cluster_distance(point, cluster_points):\n",
        "    '''\n",
        "    Calcular a média da distância entre um ponto e todos os pontos em um cluster\n",
        "    INPUT: ponto (point) e vetor com todos os pontos do cluster (cluster_points)\n",
        "    OUTPUT: média entre a distância do \"point\" e dos demais \"cluster_points\"\n",
        "    '''\n",
        "    return np.mean([euclidean_distance(point, other) for other in cluster_points if not np.array_equal(point, other)])\n",
        "\n",
        "\n",
        "def mean_nearest_cluster_distance(point, point_cluster, clusters):\n",
        "    '''\n",
        "    Calcular a média da distância entre um ponto e todos os pontos do cluster mais próximo\n",
        "    INPUT: ponto (point), vetor com todos os pontos do cluster (cluster_points) e demais clusters (clusters)\n",
        "    OUTPUT: média entre a distância do \"point\" e dos pontos do cluster mais próximo\n",
        "    '''\n",
        "    distances = []\n",
        "    for cluster_label, cluster_points in clusters.items():\n",
        "        if cluster_label != point_cluster:\n",
        "            distances.append(np.mean([euclidean_distance(point, other) for other in cluster_points]))\n",
        "    return np.min(distances)\n",
        "\n",
        "def silhouette_coefficient(point, point_cluster, clusters):\n",
        "    '''\n",
        "    Calcular a,b, e o coeficiente de silhueta de um ponto\n",
        "    INPUT: ponto (point), vetor com todos os pontos do cluster (cluster_points) e demais clusters (clusters)\n",
        "    OUTPUT: (b-a)/max(a,b)\n",
        "    '''\n",
        "    a = mean_intra_cluster_distance(point, clusters[point_cluster])\n",
        "    b = mean_nearest_cluster_distance(point, point_cluster, clusters)\n",
        "    return (b - a) / max(a, b)\n",
        "\n",
        "def silhouette_score(data, labels):\n",
        "    '''\n",
        "    Calcular a média dos coeficientes de silhueta para todos os pontos\n",
        "    INPUT: dados (data) e o cluster a que pertence cada dado (labels)\n",
        "    OUTPUT: média de todos valores de (b-a)/max(a,b) e score de cada ponto, em cada cluster\n",
        "    '''\n",
        "    unique_labels = np.unique(labels)\n",
        "    clusters = {label: data[label] for label in unique_labels}\n",
        "\n",
        "    silhouette_scores = []\n",
        "    for label in unique_labels:\n",
        "        for point in clusters[label]:\n",
        "            silhouette_scores.append([label,silhouette_coefficient(point, label, clusters)])\n",
        "\n",
        "    return np.mean(silhouette_scores,axis=0), silhouette_scores"
      ],
      "metadata": {
        "id": "K2A-0IBN0pio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "centers = 5\n",
        "X_train, true_labels_train = make_blobs(n_samples=2000, centers=centers, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "O-Dbgf85IujL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faixa de valores de K\n",
        "K_range = range(2, 8)\n",
        "\n",
        "# Lista para armazenar os valores SILHUETA para cada K\n",
        "silhouette = []\n",
        "\n",
        "# Calcular Silhueta para cada K\n",
        "silhouette_values = []\n",
        "silhouette_profile = []\n",
        "for k in K_range:\n",
        "   kmeans = KMeans(n_clusters=k)\n",
        "   centroids, sorted_points, labels = kmeans.fit(X_train)\n",
        "   silhouette, silhouette_scores = silhouette_score(sorted_points,labels)\n",
        "   silhouette_values.append(silhouette[1])\n",
        "   silhouette_profile.append([k,silhouette_scores])\n",
        "\n",
        "# Calculando o coeficiente de silhueta\n",
        "print(f\"Silhueta: {silhouette_values}\")"
      ],
      "metadata": {
        "id": "v2W-No9yJonA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the elbow graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(K_range, silhouette_values, 'bo-', markersize=8)\n",
        "plt.xlabel('Número de clusters (K)')\n",
        "plt.ylabel('Silhueta')\n",
        "plt.title('Silhueta')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "168p7l8dPSxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veja que uma plotagem comum da silhueta é o perfil de cada nó para cada número de clusters $k$, conforme ilustra a figura abaixo.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1b2E98m5jiwEUj48rP61ACHqAuvuibqaz' width=\"600\"></center>\n",
        "\n",
        "<small> Figura extraída do [link](https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html) <small>\n",
        "\n",
        "`silhouette_profile[0][0]` armazena o número de clusters da análise, enquanto `silhouette_profile[0][1]` armazena o número do label (`silhouette_profile[0][1][i][0]`) e o valor da silhueta (`silhouette_profile[0][1][i][1]`) de cada ponto $i$."
      ],
      "metadata": {
        "id": "3iSB_ORo0mdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_profile[1][1][1999][1]"
      ],
      "metadata": {
        "id": "jiQpNTDdq7Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_profile[1][1]"
      ],
      "metadata": {
        "id": "OjhsebSgJf9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parece que 4 clusters é uma boa ideia...."
      ],
      "metadata": {
        "id": "tvPzUKSnDv4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dim_sil = np.array([silhouette_profile[0][1][i][1] for i in range(len(silhouette_profile[0][1]))])\n",
        "print(dim_sil.shape)"
      ],
      "metadata": {
        "id": "8Ba-Bc3_KXfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k= 4\n",
        "index_k = range(2,8).index(k)\n",
        "colors = ['DarkOliveGreen', 'steelblue']\n",
        "colors = cm.rainbow(np.linspace(0, 1, k))\n",
        "sil={}\n",
        "y = {}\n",
        "for i in range(k):\n",
        "    sil[i] = []\n",
        "for i in range(len(silhouette_profile[0][1])):\n",
        "    sil[silhouette_profile[index_k][1][i][0]].append(silhouette_profile[index_k][1][i][1])\n",
        "l = 0\n",
        "for i in range(k):\n",
        "     y[i] = [j for j in range(l,len(sil[i])+l)]\n",
        "     l += 1\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "width = 1. # the width of the bars\n",
        "tot = 0\n",
        "space = 100\n",
        "tpos = []\n",
        "tlabels = []\n",
        "siltot = 0\n",
        "n = 0\n",
        "for j in range(k):\n",
        "  ax.barh(np.array(y[j])+tot, np.sort(sil[j]), width, color=colors[j], alpha = 0.6)\n",
        "  siltot += sum(sil[j])\n",
        "  n += len(y[j])\n",
        "  tpos.append(tot + len(y[j])/2)\n",
        "  tlabels.append(str(j+1))\n",
        "  tot += len(y[j]) + space\n",
        "plt.axvline(siltot/n, 0,tot, linestyle=\"--\", color=\"r\")\n",
        "\n",
        "ax.set_yticks(tpos)\n",
        "ax.set_yticklabels(tlabels, minor=False)\n",
        "plt.title('Silhueta com {:d} núcleos'.format(k))\n",
        "plt.xlabel('Índice de silhueta')\n",
        "plt.ylabel('cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8q4D5eATNv-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Clusterização por MeanShift\n",
        "\n",
        "O modelo de clusterização *mean-shift* atua *deslocando a média* de um conjunto de pontos no espaço de características até convergir para uma região densa, chamada de modo (do inglês, *mode*). __Cada modo encontrado pelo algoritmo é considerado um cluster.__ O algoritmo é baseado na ideia de estimativa de densidade de kernel, que é uma maneira de estimar a função de densidade de probabilidade de um conjunto de dados.\n",
        "\n",
        "O modelo de estimativa de densidade de kernel 1D (KDE) usa a função:\n",
        "$$\n",
        "p(x^i) = \\frac{1}{n} \\sum_{j=1}^{n} k_\\sigma (x^i-x^j)\n",
        "$$\n",
        "onde $k_\\sigma$ refere-se ao modelo de kernel $k$ com hiperparâmetro (largura de banda) $\\sigma$.\n",
        "A primeira etapa do algoritmo mean-shift é definir hiperparâmetros, que se resumem a:\n",
        "\n",
        "1. o raio de atuação do kernel, para definir a *região local* de análise;\n",
        "2. uma função kernel, que é usada para calcular a densidade de probabilidade dos pontos de dados. Uma escolha comum para a função kernel é a função Gaussiana, embora outros tipos de kernels também possam ser utilizados.\n",
        "\n",
        "A próxima etapa é inicializar a média dos pontos de dados da região local e iniciar o processo iterativo de deslocar a média em direção às regiões densas do conjunto de dados.\n",
        "\n",
        "A cada iteração, o algoritmo calcula a densidade de probabilidade dos pontos de dados dada a média atual. Em seguida, a média é deslocada para a média ponderada dos pontos de dados, onde os pesos são determinados pela densidade de probabilidade. Esse processo é repetido até que a média convirja ou até que seja alcançado um número máximo de iterações. A média final é atribuída a um cluster, e os pontos de dados mais próximos da média são atribuídos a esse cluster também.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1Vav1AEjV7ysS21jz9utUgDQuG9xJyXaf' width=\"800\"></center>\n",
        "\n",
        "Uma vez que todos os clusters foram encontrados, o algoritmo pode ser usado para classificar novos pontos de dados atribuindo-os ao cluster mais próximo. O algoritmo mean-shift é particularmente robusto para conjunto de dados contém clusters de diferentes formas e tamanhos. Além disso, o algoritmo mean-shift não requer que o número de clusters seja especificado com antecedência, tornando-o mais flexível para aprendizado de máquina não supervisionado."
      ],
      "metadata": {
        "id": "51CMTjGhj8zP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algoritmo\n",
        "\n",
        "O algoritmo propõe o uso de dois kernels:\n",
        "- *Gaussian*, onde é adicionado o hiperparâmetro largura de banda (desvio padrão) $\\sigma$:\n",
        "$$\n",
        "k_\\sigma(d) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} exp (- \\frac{d^2}{\\sigma^2})\n",
        "$$\n",
        "$d$ é a distância entre os pontos $(\\mathbf x^i- \\mathbf x^j)$.\n",
        "\n",
        "- *Flat*:\n",
        "$$\n",
        "k_\\sigma(d) =\n",
        "\\begin{align}\n",
        " & 1 \\text{ se  } d\\leq \\sigma \\\\\n",
        " &  0 \\text{ se  } d> \\sigma\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "5FWSmuiSPRCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanShift(object):\n",
        "    '''\n",
        "    Implementação do algoritmo MeanShift com um kernel gaussiano\n",
        "    link: https://christianmaxmike.github.io/mindnotes/ml-clustering-meanshift\n",
        "    Argumentos (propriedades):\n",
        "      max_iter: número máximo de iterações\n",
        "      bandwidth: desvio padrão do kernel gaussiano\n",
        "      tol: limite de tolerância\n",
        "    '''\n",
        "\n",
        "    def __init__(self, max_iter=100, bandwidth=.3, tol=1e-4):\n",
        "        self.max_iter = max_iter\n",
        "        self.bandwidth = bandwidth\n",
        "        self.tol = tol\n",
        "\n",
        "    def _euclidean(self, x, X):\n",
        "        '''\n",
        "        Calcula a distância euclidiana entre um ponto de dados x e\n",
        "        todos os outros pontos no conjunto de dados, denotado como X\n",
        "\n",
        "        INPUT:\n",
        "          x: ponto de dados para o qual as distâncias serão calculadas\n",
        "          X: o conjunto de dados incluindo todos os pontos de dados\n",
        "        '''\n",
        "        return (sqrt(((x-X)**2).sum(1)))\n",
        "\n",
        "    def gaussian_kernel(self, d, bandwidth):\n",
        "        '''\n",
        "        Define o kernel gaussiano\n",
        "\n",
        "        INPUT:\n",
        "          d: distância\n",
        "          bandwidth: largura de banda que define o desvio padrão do kernel gaussiano\n",
        "        '''\n",
        "        f_1 = 1.0/(bandwidth * math.sqrt(2*math.pi))\n",
        "        f_2 = exp(-0.5*((d/bandwidth))**2)\n",
        "        return f_1 * f_2\n",
        "\n",
        "    def R_kernel(self, d, R):\n",
        "        '''\n",
        "        Define o kernel gaussiano\n",
        "\n",
        "        INPUT:\n",
        "          d: distância\n",
        "          bandwidth: largura de banda que define o desvio padrão do kernel gaussiano\n",
        "        '''\n",
        "        range_R = []\n",
        "        for i in range(len(d)):\n",
        "          if d[i] <= R:\n",
        "            range_R.append(1)\n",
        "          else:\n",
        "            range_R.append(0)\n",
        "        return np.array(range_R)\n",
        "\n",
        "    def _calc (self, x, X, bandwidth):\n",
        "        '''\n",
        "        Calcula a média ponderada de todos os pontos de dados em X com relação a um certo ponto de dados x em X.\n",
        "\n",
        "        INPUT:\n",
        "          x: ponto de dados sendo atualmente considerado\n",
        "          X: conjunto de dados\n",
        "          bandwidth: largura de banda para o kernel \"Gaussian\" (fator de suavização) ou raio para o kernel \"Flat\"\n",
        "        '''\n",
        "        # soma ponderada de todos os pontos, cujo peso é dado pela respectiva distância\n",
        "        # de cada ponto local em relação ao ponto x\n",
        "        dist = self._euclidean(x, X)\n",
        "        if self.kernel == 'Gaussian':\n",
        "          weight = self.gaussian_kernel(dist, bandwidth)\n",
        "        else:\n",
        "          if self.kernel == 'Flat':\n",
        "            weight = self.R_kernel(dist, bandwidth)\n",
        "          else:\n",
        "            raise ValueError('Kernel não suportado')\n",
        "        # soma ponderada de todos os pontos, cujo peso é dado pela respectiva distância\n",
        "        # de cada ponto em relação ao ponto x\n",
        "        return (weight[:, None]*X).sum(0) / weight.sum()\n",
        "        # weight[:, None] é uma forma comum de manipular a forma de um array no NumPy\n",
        "        # weight é inicialmente um array 1D com forma (3,), o que significa que ele tem 3 elementos.\n",
        "        # weight[:, None] muda sua forma para (3, 1), o que significa que agora ele tem 3 linhas e 1 coluna.\n",
        "\n",
        "    def fit (self, X, kernel = 'Gaussian'):\n",
        "        '''\n",
        "        Executa o algoritmo MeanShift.\n",
        "\n",
        "        INPUT:\n",
        "          X: conjunto de dados\n",
        "        '''\n",
        "        it = 0\n",
        "        self.kernel = kernel\n",
        "        for i in range(self.max_iter):\n",
        "            X_adapted = np.array([self._calc(x,X,self.bandwidth) for x in X])\n",
        "            if it % 1 == 0:\n",
        "                self._plot(X_adapted)\n",
        "            if it >= self.max_iter or abs(X-X_adapted).sum()/abs(X.sum()) < self.tol:\n",
        "                print (\"Convergência na iteração {}\".format(it))\n",
        "                self._plot(X_adapted)\n",
        "                return X_adapted\n",
        "            X = X_adapted\n",
        "            it += 1\n",
        "\n",
        "    def _plot (self, X):\n",
        "        '''\n",
        "        Plotagem simples\n",
        "        '''\n",
        "        dist = self._euclidean([0,0], X)\n",
        "        plt.figure()\n",
        "        plt.scatter(X[:,0], X[:, 1], c=dist, cmap='viridis')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "FmRdZBSImTba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo de Mean Shift com 5 pontos\n",
        "\n",
        "\n",
        "Abaixo é ilustrado o processo de MEan Shift passo a passo para o conjunto de pontos $\\mathbf{X} = \\{(1, 2), (2, 3), (3, 4), (6, 8), (7, 9)\\}$, considerando a largura de banda $h = 3$.\n",
        "\n",
        "1. __Inicialize os pontos:__\n",
        "$\\mathbf{X} = \\{(1, 2), (2, 3), (3, 4), (6, 8), (7, 9)\\}$\\;\n",
        "\n",
        "2. __Escolha uma largura de banda:__\n",
        "Suponha uma largura de banda $h$ (raio para a vizinhança), digamos $h = 3$.\n",
        "\n",
        "3. __Iterações para cada ponto:__\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1gN_Z50euKE3WPZCDwPFPnPS5NMDx3BD-' width=\"800\"></center>\n",
        "\n",
        "* Para o ponto $(1, 2)$:\n",
        " * Pontos dentro da largura de banda $h = 3$ são $(1, 2), (2, 3), (3, 4)$. Portanto, tem-se a média de:\n",
        " $$\\left( \\frac{1+2+3}{3}, \\frac{2+3+4}{3} \\right) = (2, 3)$$\n",
        "\n",
        "\n",
        "\n",
        " * Para o ponto $(2, 3)$:\n",
        " * Pontos dentro da largura de banda $h = 3$ são $(1, 2), (2, 3), (3, 4)$.Portanto, tem-se a média de:\n",
        " $$\\left( \\frac{1+2+3}{3}, \\frac{2+3+4}{3} \\right) = (2, 3)$$\n",
        "\n",
        "\n",
        "* Para o ponto $(3, 4)$:\n",
        " * Pontos dentro da largura de banda $h = 3$ são $(1, 2), (2, 3), (3, 4)$. Portanto, tem-se a média de:\n",
        " $$\\left( \\frac{1+2+3}{3}, \\frac{2+3+4}{3} \\right) = (2, 3)$$\n",
        "\n",
        "\n",
        "* Para o ponto $(6, 8)$:\n",
        " * Pontos dentro da largura de banda $h = 3$ são $(6, 8), (7, 9)$. Portanto, tem-se a média de:\n",
        " $$\\left( \\frac{6+7}{2}, \\frac{8+9}{2} \\right) = (6.5, 8.5)$$\n",
        "\n",
        "\n",
        "* Para o ponto $(7, 9)$:\n",
        " * Pontos dentro da largura de banda $h = 3$ são $(6, 8), $(7, 9)$. Portanto, tem-se a média de:\n",
        " $$\\left( \\frac{6+7}{2}, \\frac{8+9}{2} \\right) = (6.5, 8.5)$$\n",
        "\n",
        "\n",
        "4. __Atualize os pontos:__\n",
        "$$\\mathbf{X}_{\\text{novo}} = \\{(2, 3), (2, 3), (2, 3), (6.5, 8.5), (6.5, 8.5)\\}$$\n",
        "\n",
        "Repita as iterações de Mean Shift até que os pontos parem de mudar significativamente. No caso de nosso exemplo, os pontos já atingiram o equilíbrio:\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1Nksf-6M7PkuUjSE2CE8Ja8MjgZ-Xto93' width=\"200\"></center>\n",
        "\n",
        "Atribua clusters com base nos pontos convergidos\\;\n",
        "\n",
        "Clusters finais:\n",
        "* __Cluster 1__: $(1, 2), (2, 3), (3, 4)$\n",
        "* __Cluster 2__: $(6, 8), (7, 9)$\n",
        "\n",
        "O algoritmo, ao final, terá todos os pontos com coordenadas de seu centroide:\n",
        "```\n",
        "[[2.  3. ]\n",
        " [2.  3. ]\n",
        " [2.  3. ]\n",
        " [6.5 8.5]\n",
        " [6.5 8.5]]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQXcMC_dN2GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1, 2], [2, 3], [3, 4], [6, 8], [7, 9]])\n",
        "meanshift = MeanShift(max_iter=100, bandwidth=3, tol=1e-4)\n",
        "centroids = meanshift.fit(X, kernel = 'Flat');"
      ],
      "metadata": {
        "id": "0itc5ipAPcpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(centroids)"
      ],
      "metadata": {
        "id": "HgmQI6fp8Ocj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teste com Gaussiano:"
      ],
      "metadata": {
        "id": "ITwbyaKOLEeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1, 2], [2, 3], [3, 4], [6, 8], [7, 9]])\n",
        "meanshift = MeanShift(max_iter=100, bandwidth=1.5, tol=1e-4)\n",
        "centroids = meanshift.fit(X, kernel = 'Gaussian')\n",
        "print(centroids)"
      ],
      "metadata": {
        "id": "sB89t30dLBnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora teste um modelo maior:"
      ],
      "metadata": {
        "id": "IvFJQuISLJ9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "centers = 5\n",
        "X_train, true_labels_train = make_blobs(n_samples=2000, centers=centers, cluster_std=0.60, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "sns.scatterplot(x=X_train[:,0], #[X[0] for X in X_train],\n",
        "                y= X_train[:, 1], #[X[1] for X in X_train],\n",
        "                hue=true_labels_train,\n",
        "                palette=\"Set1\", alpha = 0.6,\n",
        "                legend=None\n",
        "                );"
      ],
      "metadata": {
        "id": "CZbfk9Qij9mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meanshift = MeanShift()\n",
        "meanshift.fit(X_train);"
      ],
      "metadata": {
        "id": "qQU_tyfAukWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA\n",
        "\n",
        "Pense em um livro grande...\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1sWQKalXLKWKXpkC7DFowivpvQpCgUehn' width=\"400\"></center>\n",
        "\n",
        "\n",
        "O PCA é extremamente útil ao trabalhar com conjuntos de dados que possuem muitas características. Embora ter mais dados seja sempre ótimo, às vezes eles têm tantas informações que teríamos um tempo de treinamento de modelo incrivelmente longo e a maldição da dimensionalidade começa a se tornar um problema.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=14CMKgvh1VkHwxUyhvONq1TBjXT_FwhgY' width=\"400\"></center>\n",
        "\n",
        "Intuitivamente, PCA é um método de transformação que converte uma matriz de dados com características possivelmente correlacionadas em um conjunto de variáveis linearmente não correlacionadas, chamadas componentes principais. Cada componente principal é uma combinação linear dos dados originais.\n",
        "\n"
      ],
      "metadata": {
        "id": "3QcpxpTz9LFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ideia por trás do PCA\n",
        "\n",
        "\n",
        "> *The central idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of a large number of interrelated variables, while retaining as much as possible of the variation present in the data set. This is achieved by transforming to a new set of variables,\n",
        "the principal components (PCs), which are uncorrelated, and which are ordered so that the first few retain most of the variation present in all of the original variables.* Principal Component Analysis, de  I.T. Jolliffe, Springer Science & Business Media, 5th edition, 2002.\n",
        "\n",
        "Redução de dimensionalidade significa projetar dados em um espaço de menor dimensão, o que facilita a análise e a visualização de dados. No entanto, a redução da dimensão requer uma troca entre precisão (altas dimensões) e interpretabilidade (baixas dimensões).\n",
        "\n",
        "Se encontrarmos uma maneira de eliminarmos dimensões redundantes ou com pouca ou nenhuma informação, tornaremos nossos dados mais *legíveis*. Os dados se tornam ilegíveis devido a dois possíveis conflitos: ruído e redundância.\n",
        "\n",
        "O ruído em qualquer conjunto de dados deve ser baixo ou – não importa a técnica de análise – nenhuma informação sobre um sistema pode ser extraída.\n",
        "\n",
        "Além disso, é essencial identificar variáveis fortemente dependentes porque elas contêm informações viesadas e redundantes, o que reduz o desempenho geral do modelo.\n",
        "\n",
        "Portanto, a ideia principal da análise de componentes principais (PCA) é encontrar padrões e correlações entre diferentes características do conjunto de dados de modo que este possa ser transformado em um conjunto de dados de dimensão significantemente menor sem perda de informação importante!"
      ],
      "metadata": {
        "id": "EsNVUJ2qg7S4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## O poder da variância\n",
        "\n",
        "O PCA pode entender qual parte de nossos dados é importante? Podemos quantificar matematicamente a quantidade de informação incorporada nos dados?\n",
        "\n",
        "Bem, a variação pode.\n",
        "__Quanto maior a variação, mais informações. Vice-versa.__"
      ],
      "metadata": {
        "id": "OqyFdW80dzwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Breve Recap"
      ],
      "metadata": {
        "id": "kR7FmJSTd0LW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBbJOtjLxf_y"
      },
      "source": [
        "Para uma determinada variável, a quantidade de informação nela é proporcional à sua variância. Simples de perceber, pois se todos os dados são constantes, sua variância é zero e a informação que aquele dado nos fornece também é nula. Defina, por exemplo, na ilustração abaixo, com imagens embaçadas de seus amigos, em qual dos dois casos a tabela fornece informações importantes. Você deve concordar que é aquela em que há maior variância na altura dos amigos...\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1OlPeYgiWQQZeRvUyvv7MEzBL0WIeoWKu' width=\"600\"></center>\n",
        "\n",
        "Se tivermos dados de muitas dimensões, podemos reduzir sua dimensionalidade projetando-os ao longo de direções específicas, de modo que a variância na direção escolhida seja maximizada, para preservar o máximo de informações contidas nos dados.\n",
        "\n",
        "É exatamente o papel do PCA. A análise de componentes principais é um método para encontrar direções ortogonais de variância máxima nos dados e projetar os dados nessas direções.\n",
        "\n",
        "Qual é essa direção de máxima variação? Para isso devemos recordar as definições de variância e covariância.\n",
        "\n",
        "Basicamente, variância mede a variação de uma única variável aleatória (como a altura de uma pessoa em uma população), enquanto covariância é uma medida de quanto duas variáveis aleatórias variam juntas (como a altura e peso de uma pessoa em uma população).\n",
        "\n",
        "\n",
        "A variância e a covariância são afetadas pela *disseminação* dos dados em torno da média. Mas o que a variação e a covariância significam?\n",
        "\n",
        "Além disso, qual a diferença entre covariância e correlação?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WQqXt7Izzio"
      },
      "source": [
        "### Variância\n",
        "\n",
        "\n",
        "A variância é simplesmente uma medida da dispersão em torno da média - se as amostras de dados forem muito diferentes umas da outra, a variância será alta e vice-versa. Dado um conjunto de dados $\\mathbf x$, estes apresentam um valor *médio* $\\bar x$ e uma *variação* sobre essa média $s_x^2$. Tecnicamente **a variância é a média do quadrado diferenças dos dados com relação à média**,\n",
        "\n",
        "$$s_x^2 =  {\\rm Var} (\\mathbf x) = {1 \\over m-1} \\sum_ {j = 1}^m (x_j- \\bar x)^2, $$\n",
        "\n",
        "Pense na variância como a \"propagação\" ou \"extensão\" dos dados, sobre algum eixo específico.\n",
        "\n",
        "A ressalva é que estamos dividindo por $ m-1 $ em vez de $ m $, então, não estamos calculando exatamente uma média. Para grandes conjuntos de dados, isso não faz diferença, mas para um pequeno número de pontos de dados, $m-1$ é introduzido como forma de reduzir o viés de amostra.\n",
        "\n",
        "Importante ressaltar que a variância calculada em python com o comando `np.var`, por default, refere-se à população, com a opção `ddof=0`, e, portanto, a fórmula acima de $s_x^2$ é divida por $m$ e não por $m-1$. Para calcular a variância de uma amostra, deve-se usar a opção `ddof=1`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exemplo\n",
        "\n",
        "Os dados abaixo referem-se ao peso e altura de 4 pessoas. Monte a matriz $\\mathbf X$ e calcule a matriz de covariância usando as fórmulas que aprendemos.\n",
        "\n",
        "\\begin{array}{c} \\hline\n",
        "Pessoa & Altura [cm] & Peso [kg] \\\\ \\hline\n",
        "1 & 149 & 48 \\\\\n",
        "2 & 155 & 52 \\\\\n",
        "3 & 163 & 57 \\\\\n",
        "4 & 177 & 68 \\\\ \\hline\n",
        "\\end{array}\n",
        "\n"
      ],
      "metadata": {
        "id": "DjpOZ8DzpxwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veka que, no exemplo, o número de características é $n=2$ (altura, peso) e o número de observações é $m=4$. Portanto, a matriz $\\mathbf X$ terá dimensão $m \\times n$, isto é, $4 \\times 2$."
      ],
      "metadata": {
        "id": "pMagQR87BJMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[149,48],[155,52],[163,57],[177,68]])\n",
        "print(X)\n",
        "print(X.shape)\n",
        "\n",
        "print(\"Média Altura =\",np.mean(X,axis=0)[0])\n",
        "print(\"Média Peso =\",np.mean(X,axis=0)[1])\n",
        "\n",
        "print(\"Variância Altura =\",np.var(X, axis = 0, ddof=1)[0])\n",
        "print(\"Variância Peso =\",np.var(X, axis = 0, ddof=1)[1])"
      ],
      "metadata": {
        "id": "XLE8lSkzrcKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKt6Det305UO"
      },
      "source": [
        "### Covariância e correlação\n",
        "\n",
        "No exemplo anterior, quando $x_1$ (altura) varia, $x_2$ (peso) praticamente varia junto. Então $x_2$ é *covariante* com $x_1$. A covariância indica o nível em que duas variáveis ​​variam juntas,\n",
        "$$\n",
        "{\\rm Cov} (\\mathbf{x}_1,\\mathbf{x}_2) = s_{12}= {1 \\over m-1} \\sum_{j = 1}^m (x_{j1} - \\bar {x}_1) (x_{j2} - \\bar {x}_2),\n",
        "$$\n",
        "onde $\\bar {x}_1$ e $\\bar {x}_2$ são as médias das componentes $\\mathbf{x}_1$ e $\\mathbf{x}_2$ dos dados, respectivamente. Ressalta-se novamente que dividimos por $m-1 $ em vez de $m$, para reduzir o  viés de pequenas amostras.\n",
        "\n",
        "Uma covariância positiva, como no exemplo acima, diz que as duas variáveis *andam juntas*, isto é, em média, quando $\\mathbf{x}_1$ cresce $\\mathbf{x}_2$ também cresce, e vice-versa. Se a covariância for negativa, então, quando uma variável está crescendo, a outra variável, para a mesma amostra, está diminuindo.\n",
        "\n",
        "Ainda, se dividirmos covariância pelos dois desvios padrão, obteremos uma medida sem dimensão denominada coeficiente de correlação:\n",
        "$$\n",
        "r_{12} = \\frac{{\\rm Cov} (\\mathbf{x}_1,\\mathbf{x}_2)}{{s_1} s_{2}},\n",
        "$$\n",
        "onde $s_1$ e $s_2$ são o desvio padrão (raíz quadrada da variância) da variável $\\mathbf{x}_1$ e da variável  $\\mathbf{x}_2$, respectivamente.\n",
        "\n",
        "O coeficiente de correlação $r_{12}$ é usado para medir *a força* dessa correlação e pode assumir valores entre $+1$ e $-1$. Valor $r=+1$ indica uma correlação positiva perfeita entre variáveis. Uma correlação negativa perfeita leva ao valor de $r=-1$. Valor $r=0$ é quando ambas as variáveis são completamente independentes uma da outra.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvq6YX4GBF_n"
      },
      "source": [
        "Generalizando, dada uma matriz dos dados de entrada $\\mathbf X_{m,n}$, onde cada linha é um conjunto de observações das variáveis,\n",
        "\\begin{equation}\n",
        " \\mathbf x_i =\n",
        "\\begin{pmatrix}\n",
        "x_{i1} & x_{i2} & \\cdots & x_{in}\n",
        "\\end{pmatrix}, \\quad i=1,\\cdots,m\n",
        "\\end{equation}\n",
        "e cada coluna representa várias medições da mesma variável.\n",
        "\\begin{equation}\n",
        "  \\hat{\\mathbf x}_i =\n",
        "\\begin{pmatrix}\n",
        "x_{1i} \\\\\n",
        " x_{2i}\\\\\n",
        "  \\cdots \\\\\n",
        "   x_{mi}\n",
        "\\end{pmatrix}, \\quad i=1,\\cdots,n\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Dessa forma, as componentes $s_{jk}$ $(j,k = 1,\\cdots,n)$ da matriz de covariância $\\mathbf S$ podem ser definidas como,\n",
        "\n",
        "$$\n",
        "s_{jk} = {\\rm Cov} (\\mathbf x_j,\\mathbf x_k) = {1 \\over m-1} \\sum_{i = 1}^m (x_{ij} - \\bar{x}_j) (x_{ik} - \\bar{x}_k), \\tag{1}\n",
        "$$\n",
        "\n",
        "Verifique que as entradas diagonais da matriz de covariância $\\mathbf S$ são as variâncias e as entradas fora da diagonal são as covariâncias. Como a ordem das variáveis não importa ao calcular a covariância, a matriz será *simétrica* e, portanto, *quadrada*.\n",
        "\n",
        "Outra notação bastante útil é,\n",
        "\n",
        "$$\n",
        "\\mathbf S = \\frac{1}{m-1} \\sum^{m}_{i=1}{\\left( {\\mathbf x}_i - \\mathbf{\\bar{x}} \\right)^T\\left({\\mathbf x}_i-\\mathbf{\\bar{x}}\\right)}, \\tag{2}\n",
        "$$\n",
        "\n",
        "onde $\\bar{\\mathbf x}^T = \\left[ \\bar{x}_1 \\quad \\bar{x}_2 \\quad  \\cdots \\quad \\bar{x}_n\\right]$, de modo que $\\bar{x}_j = {1 \\over m}\\sum_{i=1}^m x_{ij}$ é a média da variável $j$.\n",
        "\n",
        "Pode-se reescrever a matriz de covariância como,\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf S\n",
        "= {1 \\over {m-1}} \\left[\\mathbf X_c^T \\mathbf X_c\\right], \\tag{3}\n",
        "\\end{equation}\n",
        "para $X_c = \\mathbf X - \\mathbf 1_m  \\bar{\\mathbf x}^T$, onde $\\mathbf 1_m^T = \\begin{bmatrix}1 & 1 & \\cdots& 1\\end{bmatrix}$ tem dimensão $m \\times 1$.\n",
        "\n",
        "`Numpy` facilita muito o cálculo da covariância. Porém, ressalta-se que, a princípio, calculamos covariância em amostras, e não na população completa. Portanto, o default do cálculo da matriz de covariância é `np.cov(X.T, bias = False)`. Para obter a covariância da população (com base em $m$), você precisará definir o viés como `bias=True` no código.\n",
        "\n",
        "Ainda, `Numpy` quer variáveis (características) ao longo de linhas, em vez de colunas... Portanto, a matriz entra como transposta para cálculo da variância. Outra alternativa é usar o parâmetro `rowvar`. Se  `rowvar` for `True` (padrão), cada linha representa uma variável, com observações nas colunas. Caso contrário, a relação é transposta: cada coluna representa uma variável, enquanto as linhas contêm observações."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMAgDlQkv6HO"
      },
      "source": [
        "#### Continuação do Exemplo\n",
        "\n",
        "Calcule, para o exemplo anterior, calcule a matriz de covariância e de correlação.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrjIjUUcv6HT"
      },
      "source": [
        "#Reescrevendo o vetor X e recalculando as médias:\n",
        "X = np.array([[149,48],[155,52],[163,57],[177,68]])\n",
        "xbar = np.mean(X, axis = 0) #média das colunas de X\n",
        "m,n = X.shape\n",
        "\n",
        "#Gabarito:\n",
        "print('Matriz de covariância via Numpy:\\n')\n",
        "cov_x = np.cov(X.T)\n",
        "print(pd.DataFrame([[cov_x[0][0], cov_x[0][1]],[cov_x[1][0], cov_x[1][1]]], index =(\"x1\", \"x2\"), columns=['x1', 'x2']))\n",
        "#print(np.cov(X,rowvar = False),'\\n') # alternativa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A matriz de correlação é calculada através do comando,\n",
        "```\n",
        "cor = np.corrcoef(X.T)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "riUe50SO8Px_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Matriz de correlação via Numpy:\\n')\n",
        "cor = np.corrcoef(X.T)\n",
        "print(pd.DataFrame([[cor[0][0], cor[0][1]],[cor[1][0], cor[1][1]]], index =(\"x1\", \"x2\"), columns=['x1', 'x2']))"
      ],
      "metadata": {
        "id": "buMP2erd75nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, recalcule a matriz de covariância usando as fórmulas (1), (2) e (3), definidas acima."
      ],
      "metadata": {
        "id": "8jFtHHPw8WkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Usando a fórmula (1):\n",
        "print('Primeira fórmula:')\n",
        "S1 = np.zeros((n,n))\n",
        "\n",
        "for j in range(n):\n",
        "  for k in range(n):\n",
        "    aux = 0.\n",
        "    for i in range(m):\n",
        "      xij = X[i][j]\n",
        "      xik = X[i][k]\n",
        "      xbarj = xbar[j]\n",
        "      xbark = xbar[k]\n",
        "      aux += (xij-xbarj)*(xik-xbark)\n",
        "    S1[j][k] = aux/(m-1)\n",
        "print(S1)"
      ],
      "metadata": {
        "id": "Mj_A4a4txZn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Usando a fórmula (2):\n",
        "#\n",
        "print('\\n Segunda fórmula:')\n",
        "S2 = np.zeros((n,n))\n",
        "xbar = np.mean(X, axis = 0)\n",
        "\n",
        "for j in range(m):\n",
        "  S2 += np.outer((X[j,:]-xbar),(X[j,:]-xbar))\n",
        "S2 = S2/(m-1)\n",
        "print(S2)"
      ],
      "metadata": {
        "id": "FSegtIGExiKi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Usando a fórmula (3):\n",
        "#\n",
        "print('\\n Terceira fórmula:')\n",
        "S3 = np.zeros((n,n))\n",
        "xbar = np.mean(X, axis = 0).T\n",
        "Vet1 = np.ones((m,1))\n",
        "Xc = X - np.outer(Vet1,xbar)\n",
        "S3 = np.dot(Xc.T,Xc)/((m-1))\n",
        "print(S3)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3gw2ASA3xkpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Segundo exemplo\n",
        "Neste segundo exemplo, `m` valores são gerados de `x1` e `x2`, de modo que exista uma correlação entre esses dados,\n",
        "```\n",
        "x2 = x1 + 5.8\n",
        "```\n",
        "afetada por um ruído randômico.\n"
      ],
      "metadata": {
        "id": "qV35KWkdv6HT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0NP_wnTv6HT"
      },
      "source": [
        "m = 100\n",
        "np.random.seed(42)\n",
        "x1 = 10.*np.sqrt((np.random.normal(size=m))**2)\n",
        "x2 = x1 + 5.8*(np.random.normal(size=m))\n",
        "\n",
        "\n",
        "plt.scatter(x1, x2, marker =\"s\", s = 50,alpha=0.5)\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.show()\n",
        "print(\"Variância x1 =\",np.var(x1,ddof=1))\n",
        "print(\"Variância x2 =\",np.var(x2,ddof=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_s3z6s-v6HT"
      },
      "source": [
        "X = np.stack((x1,x2),axis=1)\n",
        "print(\"As dimensões de X são\",X.shape)\n",
        "cov = np.cov(X.T, bias = False)\n",
        "print(\"Matriz de covariância:\")\n",
        "pd.DataFrame([[cov[0][0], cov[0][1]],[cov[1][0], cov[1][1]]], index =(\"x1\", \"x2\"), columns=['x1', 'x2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnVsMz0p9m9k"
      },
      "source": [
        "A matriz de correlação é calculada através do comando,\n",
        "```\n",
        "cor = np.corrcoef(X.T)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCmkuQI09iLN"
      },
      "source": [
        "cov = np.cov(X.T)\n",
        "print('Matriz de covariância :')\n",
        "print(pd.DataFrame([[cov[0][0], cov[0][1]],[cov[1][0], cov[1][1]]], index =(\"x1\", \"x2\"), columns=['x1', 'x2']))\n",
        "\n",
        "print('\\n Matriz de correlação :')\n",
        "cor = np.corrcoef(X.T)\n",
        "print(pd.DataFrame([[cor[0][0], cor[0][1]],[cor[1][0], cor[1][1]]], index =(\"x1\", \"x2\"), columns=['x1', 'x2']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A matriz de correlação é obtida também se usarmos os valores de $\\mathbf X$ padronizados, ié, subtrairmos sua média e dividirmos pelo seu desvio padrão."
      ],
      "metadata": {
        "id": "K12C3NxEu10t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_std = StandardScaler().fit_transform(X)\n",
        "cov = np.cov(X_std.T)\n",
        "print('Matriz de covariância padronizada :')\n",
        "print(pd.DataFrame([[cov[0][0], cov[0][1]],[cov[1][0], cov[1][1]]], index =(\"x1\", \"x2\"), columns=['x1', 'x2']))"
      ],
      "metadata": {
        "id": "3TOGrb6hvXuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base de um vetor\n",
        "\n",
        "Matematicamente, para que um conjunto de vetores $\\mathbf b_i$ seja linearmente independente, em um espaço 𝑛-dimensional, a expressão\n",
        "\n",
        "$$\n",
        "c_1 \\mathbf b_1 + c_2 \\mathbf b_2 + \\cdots+ c_n \\mathbf b_n=0\n",
        "$$\n",
        "\n",
        "deve ser possível apenas se todos os fatores lineares $c_i$ forem 0.\n",
        "Em resumo, nenhum vetor pode ser expresso como uma combinação linear dos outros. Os vetores $\\mathbf b_i$, portanto, formam uma base de dimensão $n$.\n",
        "\n",
        "Por outro lado, cada amostra de dados é um vetor no espaço de dimensão $n$, onde $n$ é o número de características de uma amostra. Dessa forma, cada amostra é um vetor de um espaço vetorial de dimensão $n$, representado por uma base ortonormal.\n",
        "\n",
        "Qualquer vetor de medição nesse espaço, ié, qualquer $\\mathbf x^{(i)}$, é uma combinação linear desse conjunto de vetores básicos de comprimento unitário. Uma escolha simples e direta de uma base $\\mathbf B$ é a matriz de identidade $\\mathbf I$,\n",
        "$$\n",
        "\\mathbf B=\\left(\\begin{array}{c}\n",
        "\\mathbf b_1 \\\\\n",
        "\\mathbf b_2 \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf b_n\n",
        "\\end{array}\\right) =\n",
        "\\left(\\begin{array}{cccc}\n",
        "1 & 0 & \\cdots & 0 \\\\\n",
        "0 & 1 & \\cdots & 0  \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
        "0 & 0 & \\cdots & 1\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "onde cada linha $\\mathbf b_i$ é uma base com $n$ componentes."
      ],
      "metadata": {
        "id": "0lPOkBGFPjpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mudança de base\n",
        "\n",
        "Dado o vetor $\\mathbf X \\in \\mathbb{R}^{m \\times n}$ de dados de entrada,\n",
        "\\begin{equation}\n",
        "\\mathbf X_{m,n} =\n",
        "\\begin{pmatrix}\n",
        "x_{11} & x_{12} & \\cdots & x_{1n} \\\\\n",
        "x_{21} & x_{22} & \\cdots & x_{2n} \\\\\n",
        "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
        "x_{m1} & x_{m2} & \\cdots & x_{m,n}\n",
        "\\end{pmatrix} =\n",
        "\\begin{pmatrix}\n",
        "|  & |  &   & |  \\\\\n",
        "\\hat{\\mathbf x}_{1} & \\hat{\\mathbf x}_{2} & \\cdots & \\hat{\\mathbf x}_{n} \\\\\n",
        "|  & |  &   & |  \n",
        "\\end{pmatrix}=\n",
        "\\begin{pmatrix}\n",
        "-  & {\\mathbf x}_{1}   & -  \\\\\n",
        " - & {\\mathbf x}_{2}   & - \\\\\n",
        "   & \\vdots &    \\\\\n",
        " - & {\\mathbf x}_{m} & -\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Define-se que as linhas representam diferentes observações e cada coluna refere-se a uma característica ou variável. Por exemplo, se medirmos peso e altura de $3$ pessoas nossa matriz $\\mathbf X$ terá dimensão $(3,2)$, onde cada linha representa um dos 3 indivíduos e as duas colunas se referem às características peso e altura. Dessa forma, $x_{j1}$ seria o peso do $j-$ésimo indivíduo, $x_{k1}$ seria o peso do $k-$ésimo indivíduo e $x_{j2}$ seria a altura do $j-$ésimo indivíduo.\n",
        "\n",
        "Define-se um vetor $\\mathbf w_i$ de transformação\n",
        "\\begin{equation}\n",
        "\\mathbf w_{n,n} =\n",
        "\\begin{pmatrix}\n",
        "|  & |  &   & |  \\\\\n",
        "\\mathbf w_{1} & \\mathbf w_{2} & \\cdots & \\mathbf w_{n} \\\\\n",
        "|  & |  &   & |  \n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "de forma que:\n",
        "\\begin{equation}\n",
        "\\mathbf Z_{m,n} = \\mathbf X_{m,n} \\mathbf w_{n,n} =  \n",
        "\\begin{pmatrix}\n",
        "{\\mathbf x}_{1}\\mathbf w_{1}& {\\mathbf x}_{1} \\mathbf w_{2}  & \\cdots & {\\mathbf x}_{1} \\mathbf w_{n}\\\\\n",
        "{\\mathbf x}_{2}\\mathbf w_{1}& {\\mathbf x}_{2} \\mathbf w_{2}  & \\cdots & {\\mathbf x}_{2} \\mathbf w_{n}\\\\\n",
        "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
        "{\\mathbf x}_{m}\\mathbf w_{1}& {\\mathbf x}_{m} \\mathbf w_{2}  & \\cdots & {\\mathbf x}_{m} \\mathbf w_{n}\\\\\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Geometricamente, $\\mathbf w_i$ é uma rotação e um alongamento que transforma $\\mathbf X$ em $\\mathbf Z$. A nova matriz $\\mathbf Z$ pode ser definida como,\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf Z_{m,n} =\n",
        "\\begin{pmatrix}\n",
        "|  & |  &   & |  \\\\\n",
        "\\hat{\\mathbf z}_{1} & \\hat{\\mathbf z}_{2} & \\cdots & \\hat{\\mathbf z}_{n} \\\\\n",
        "|  & |  &   & |  \n",
        "\\end{pmatrix}=\n",
        "\\begin{pmatrix}\n",
        "-  & {\\mathbf z}_{1}   & -  \\\\\n",
        " - & {\\mathbf z}_{2}   & - \\\\\n",
        "   & \\vdots &    \\\\\n",
        " - & {\\mathbf z}_{m} & -\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "onde\n",
        "\\begin{equation}\n",
        "\\hat{\\mathbf z}_i =\n",
        "\\begin{pmatrix}\n",
        "{\\mathbf x}_{1}\\mathbf w_i \\\\\n",
        "{\\mathbf x}_{2}\\mathbf w_i \\\\\n",
        "\\vdots   \\\\\n",
        "{\\mathbf x}_{m}\\mathbf w_i\n",
        "\\end{pmatrix}, \\qquad i=1,\\cdots,n\n",
        "\\end{equation}\n",
        "é a transformação das observações na direção $\\mathbf w_i$, e\n",
        "\\begin{equation}\n",
        "{\\mathbf z}_i =\n",
        "\\begin{pmatrix}\n",
        "{\\mathbf x}_{i}\\mathbf w_{1} & {\\mathbf x}_{i}\\mathbf w_{2} & \\cdots  & {\\mathbf x}_{i}\\mathbf w_{n}\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "é a transformação da observação $i$.\n"
      ],
      "metadata": {
        "id": "TWzLLPn7YaXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exemplo\n",
        "\n",
        "\n",
        "\n",
        "O ponto  $\\mathbf x_b =[1 \\quad 1]$, definido nas bases $\\mathbf b_1 = [1 \\quad 0]^T$ e $\\mathbf b_2 = [0 \\quad 1]^T$ pode ser escrito como,\n",
        "$$\n",
        "\\mathbf x_b = x_{b_1} \\left(\\begin{array}{rr} 1\\\\ 0  \\end{array}\\right) + x_{b_2} \\left(\\begin{array}{rr} 0\\\\ 1  \\end{array}\\right) = 1 \\left(\\begin{array}{rr} 1\\\\ 0  \\end{array}\\right) + 1 \\left(\\begin{array}{rr} 0\\\\ 1  \\end{array}\\right) = \\left(\\begin{array}{rr} 1 & 1  \\end{array}\\right)\\left(\\begin{array}{rr}1 & 0\\\\ 0 & 1 \\end{array}\\right) = \\left(\\begin{array}{rr} 1 & 1  \\end{array}\\right).\n",
        "$$\n",
        "\n",
        "Para esse ponto $\\mathbf x_b =[1 \\quad 1]$, defina a matriz de transformação para os dois espaços mostrados na figura abaixo. No primeiro, $\\mathbf b_1$ e $\\mathbf b_2$ são duplicados de tamanho, e no segundo, são rotacionados $\\theta = 30^o$.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1PwEQnwiyFdFbtqbCb6ZiqigQqitFVa-H' width=\"800\"></center>\n",
        "\n",
        "Para o primeiro caso, a matriz de transformação de base é dada por:\n",
        "$$\n",
        "\\mathbf{W}_1 = \\left(\\begin{array}{rr}2 & 0\\\\ 0 & 2  \\end{array}\\right)\n",
        "$$\n",
        "isto é,\n",
        "$$\n",
        "\\mathbf b'_1 = \\left(\\begin{array}{rr} 2\\\\ 0  \\end{array}\\right) = \\left(\\begin{array}{rr} 2 & 0\\\\ 0 & 2  \\end{array}\\right)\\left(\\begin{array}{rr} 1\\\\ 0  \\end{array}\\right)\n",
        "$$\n",
        "ou\n",
        "$$\n",
        "\\mathbf b'_2 = \\left(\\begin{array}{rr} 0\\\\ 2  \\end{array}\\right) = \\left(\\begin{array}{rr} 2 & 0\\\\ 0 & 2  \\end{array}\\right)\\left(\\begin{array}{rr} 0\\\\ 1  \\end{array}\\right).\n",
        "$$\n",
        "\n",
        "Portanto, o vetor $\\mathbf x_b$ escrito na nova base, ié, $\\mathbf z$, fica:\n",
        "$$\n",
        "\\left(\\begin{array}{rr} 1 & 1  \\end{array}\\right)\\left(\\begin{array}{rr}1 & 0\\\\ 0 & 1 \\end{array}\\right) = \\mathbf z  \\mathbf{W}_1  = \\left(\\begin{array}{rr} z_{1} & z_{2}  \\end{array}\\right)  \\left(\\begin{array}{rr} 2 & 0 \\\\ 0  & 2 \\end{array}\\right)\n",
        "$$\n",
        "\n",
        "Se multiplicarmos ambos os lados da equação acima por $\\mathbf{W}_1^{-1}$, e, sabendo que$^*$,\n",
        "$$\n",
        "\\mathbf{W}_1^{-1} = \\frac{1}{4} \\left(\\begin{array}{rr}2 & 0\\\\ 0 & 2  \\end{array}\\right)\n",
        "$$\n",
        "tem-se:\n",
        "$$\n",
        "\\left(\\begin{array}{rr} z_{1} & z_{2}  \\end{array}\\right)  = \\left(\\begin{array}{rr} 1 & 1  \\end{array}\\right) \\frac{1}{4} \\left(\\begin{array}{rr}2 & 0\\\\ 0 & 2  \\end{array}\\right)  = \\left(\\begin{array}{rr} \\frac{1}{2} & \\frac{1}{2}  \\end{array}\\right).\n",
        "$$\n",
        "\n",
        "Para o segundo caso, lembre-se que a matriz de rotação bidimensional que gira os eixos no plano no sentido anti-horário através de um ângulo $\\theta$ em torno da origem é definida como,\n",
        "\n",
        "$$\n",
        "\\mathbf{R}_2 = \\left(\\begin{array}{rr}\\cos\\theta & -\\sin\\theta\\\\ \\sin\\theta & \\cos\\theta \\end{array}\\right).\n",
        "$$\n",
        "\n",
        "<small> *Caso você não lembre mais como se inverte uma matriz, use:\n",
        "```\n",
        "np.linalg.inv(np.array([[2,0],[0,2]]))\n",
        "```\n",
        "<small>\n"
      ],
      "metadata": {
        "id": "N9rvNEjdYEt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Primeiro Caso\n",
        "b1 = [1,0]\n",
        "b2 = [0,1]\n",
        "B = np.array([b1,b2])\n",
        "W1 = 2*B\n",
        "\n",
        "W1inv=np.linalg.inv(W1)\n",
        "X=np.array([[1,1]])\n",
        "Z1=np.dot(X,W1inv)\n",
        "print(' Vetor z:\\n',Z1)"
      ],
      "metadata": {
        "id": "_vkJflCSd0Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq9cFOPfdcgb"
      },
      "source": [
        "#@title Resposta do Segundo caso { display-mode: \"form\" }\n",
        "#Segundo caso\n",
        "theta = np.radians(30)\n",
        "c, s = np.cos(theta), np.sin(theta)\n",
        "W2 = np.array([[c,s], [-s, c]])\n",
        "W2inv=W2.T\n",
        "X=np.array([[1,1]])\n",
        "Z2=np.dot(X,W2inv)\n",
        "print(' Vetor z: \\n',Z2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como conferir a resposta?\n",
        "\n",
        "Bom, o vetor $\\mathbf x_b =[1 \\quad 1]$ quando escrito nas bases iniciais $\\mathbf b_1$ e $\\mathbf b_2$.\n",
        "$$\n",
        "\\left(\\begin{array}{rr} 1 & 1  \\end{array}\\right)\\left(\\begin{array}{rr}1 & 0\\\\ 0 & 1 \\end{array}\\right) = \\mathbf z \\mathbf{W}$$\n",
        "\n",
        "Então, substitua os valores de $\\mathbf w_i$ e $\\mathbf z$ para cada transformação e veja se recupera o vetor $\\mathbf x_b$ nas bases originais $\\mathbf b_1$ e $\\mathbf b_2$."
      ],
      "metadata": {
        "id": "TQqkyvWoeUIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Conferindo as respostas:\n",
        "print('xb = [ {:f}  {:f}  ]'.format(Z1[0][0]*W1[0][0]+Z1[0][1]*W1[1][0],Z1[0][0]*W1[0][1]+Z1[0][1]*W1[1][1]))\n",
        "print('xb = [ {:f}  {:f}  ]'.format(Z2[0][0]*W2[0][0]+Z2[0][1]*W2[1][0],Z2[0][0]*W2[0][1]+Z2[0][1]*W2[1][1]))"
      ],
      "metadata": {
        "id": "pvZrfMAQektB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Onde estamos\n",
        "\n",
        "Definimos a análise de componentes principais (PCA) como uma técnica estatística fundamental usada para reduzir a dimensionalidade de um conjunto de dados, enquanto preserva o máximo de variação possível. *Maior variação possível* está conectada a conceitos-chave que estudamos: variância e covariância:\n",
        "\n",
        "* __Variância__ A variância $s_{ii}$ mede a dispersão da característica $i$ em torno da média do conjunt de dados. Em outras palavras, indica o quanto a característica *se espalha*. No PCA, a variância é crucial porque a técnica busca identificar direções (componentes principais) ao longo das quais a variabilidade dos dados é máxima. Ao maximizar a variância, a PCA garante que os componentes principais capturam a maior parte da informação contida nos dados originais. Componentes com maior variância são considerados mais importantes porque explicam mais da estrutura dos dados.\n",
        "\n",
        "* __Covariância__ A covariância $s_{ij}$ mede a relação entre duas características $i$ e $j$. Especificamente, ela indica se as características tendem a aumentar e diminuir juntas (covariância positiva) ou se uma tende a aumentar quando a outra diminui (covariância negativa). A covariância é importante no PCA porque revela a medida em que duas características mudam juntas. Se as características estão fortemente correlacionadas, o PCA pode combiná-las em um componente principal, reduzindo assim a dimensionalidade.\n",
        "\n",
        "* __Matriz de Covariância__ No PCA, a matriz de covariância dos dados é calculada para entender as relações entre todas as características. A matriz de covariância é usada para identificar as direções de maior variância e as direções em que os dados estão mais correlacionados. As direções dos componentes principais são, na verdade, os autovetores da matriz de covariância, e os valores que indicam a quantidade de variância explicada em cada direção são os autovalores. Mas isso já é spoiler...\n",
        "\n",
        "$$\n",
        "\\mathbf S = \\begin{pmatrix}\n",
        "\\color{palevioletred}{s_{11}} & \\color{skyblue}{s_{12}} & \\cdots & \\color{skyblue}{s_{1n}} \\\\\n",
        "\\color{skyblue}{s_{21}} & \\color{palevioletred}{s_{22}} & \\cdots & \\color{skyblue}{s_{2n}} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\color{skyblue}{s_{n1}} & \\color{skyblue}{s_{n2}} & \\cdots & \\color{palevioletred}{s_{nn}}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "E porque recordamos mudança de base?\n",
        "\n",
        "Bom, a  mudança de base é um conceito matemático que se refere à transformação dos dados para um novo sistema de coordenadas. No PCA, os dados são projetados em um novo conjunto de eixos que são as direções dos componentes principais. Esses novos eixos são ortogonais entre si e correspondem aos autovetores da matriz de covariância (outro spoiler). A mudança de base facilita a interpretação dos dados, pois as primeiros componentes principais capturam a maior parte da variabilidade dos dados, permitindo a redução da dimensionalidade sem perder informação significativa. Ao projetar os dados nas primeiras poucas componentes principais, é possível visualizar e analisar dados complexos de maneira mais simples e intuitiva.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1W6RDvrbIsplbi5x0Fqtt_F7HdzJ2I4kS' width=\"800\"></center>"
      ],
      "metadata": {
        "id": "JsB2vQXzpmhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?export=view&id=18X6gd0Iop0LN8rQUKpwGmtJtiuoFONFH' width=\"400\"></center>\n"
      ],
      "metadata": {
        "id": "e9DVdhb7zsJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma característica importante de autovalores e autovetores de uma matriz simétrica, que aprendemos ao estudarmos SVD: os autovalores e autovetores representam as direções e dimensões de máxima variância da transformação.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=153VU5KvJfBDjbLqM7WENSKGrkRH9CMvq' width=\"600\"></center>\n"
      ],
      "metadata": {
        "id": "_WVRpl-gL0kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA e os autovalores e autovetores\n",
        "\n",
        "Para fazer a análise de componentes principais, precisamos encontrar os *componentes*, e isso requer a busca de *autovetores* para a matriz de covariância do conjunto de dados."
      ],
      "metadata": {
        "id": "IiGwAWpf1G06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autovalores e autovetores\n",
        "\n",
        "Em inglês, autovalores e autovetores chamam-se, respectivamente, *eigenvalues* e *eigenvectors*. Mas, a palavra original é alemã, *eigen*, e significa *próprio, peculiar*. Mas, isso não explica muito. É uma curiosidade.\n",
        "\n",
        "Os autovetores de uma matriz de covariância são chamados direções principais, pois correspondem às direções da variância máxima. As projeções dos dados nas direções principais são conhecidas como componentes principais.\n",
        "Daí o nome Análise de Componentes Principais.\n",
        "\n",
        "Quanto maior o autovalor,  maior a quantidade de variação capturada por aquela componente principal. A variância capturada ao longo de cada CP pode ser calculada pela variância da projeção dos dados na direção principal.\n",
        "\n",
        "#### Definição de autovalor e autovetor\n",
        "\n",
        "Dado um operador linear $ {\\bf A} $ com dimensões $ n \\times n $, existe um conjunto de $n$ vetores $ \\bf {v}_i $, cada um com a dimensão $n$, de modo que a multiplicação de qualquer um desses vetores por $ {\\bf A } $ resulta em um vetor paralelo a $ \\bf {v} _i $, com um comprimento multiplicado por uma constante $ \\lambda_i $,\n",
        "\n",
        "$$ {\\bf A} \\bf {v} _i = \\lambda_i \\bf {v}_i $$\n",
        "\n",
        "onde $ \\lambda_i $ são os *autovalores* e os vetores $ \\bf {v}_i $ são os *autovetores*.\n",
        "\n",
        "**Importante: autovalores e autovetores são uma característica da matriz ${\\bf A}$.** O número de autovalores e autovetores é igual ao rank da matriz.\n",
        "\n",
        "Use a biblioteca `numpy` para calcular autovalores e autovetores da matriz de covariância que estamos usando como exemplo.\n",
        "```\n",
        "autovalores, autovetores = np.linalg.eig(cov)\n",
        "```\n",
        "Veja que os autovetores são as colunas da matriz `autovetores`, e referem-se aos autovalores listados no vetor `autovalores`."
      ],
      "metadata": {
        "id": "sh1BW9lokFh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autovalores, autovetores = np.linalg.eig(cov)\n",
        "print('Autovalores:\\n',autovalores)\n",
        "print('Autovetores:\\n',autovetores)\n",
        "print('Autovalor 1: ',autovalores[0],'Autovetor 1:', autovetores[:,0])\n",
        "print('Autovalor 2: ',autovalores[1],'Autovetor 2:', autovetores[:,1])"
      ],
      "metadata": {
        "id": "n0uXiCjTkZyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os autovalores no PCA informam quanta variância pode ser explicada por seu autovetor associado. Daí o nome *variância explicada*. Portanto, o maior autovalor indica que a maior variância nos dados foi observada na direção de seu autovetor. Consequentemente, se você juntar todos os autovetores, poderá explicar toda a variação na amostra de dados.\n",
        "\n",
        "Em vez de usar o valor absoluto da *variância explicada*, conforme indicado pelo autovalor, você também pode obter números relativos primeiro somando todos os autovalores e depois dividindo cada autovalor $\\lambda_i$ por esta soma.\n",
        "\n",
        "\n",
        "Se ordenarmos os autovalores $\\lambda_i$ e as direções principais $\\mathbf v_i$ de forma decrescente em relação ao autovalor $\\lambda_i$, isso quer dizer que as primeiras componentes principais serão aquelas mais importantes com respeito à *quantidade de variação capturada*.\n",
        "\n",
        "A classe abaixo foi adaptada do [link](https://github.com/Marcussena/ML-and-Ai-from-scratch/blob/main/PCA/pca.py)."
      ],
      "metadata": {
        "id": "jhFZ65dNQuBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PCA:\n",
        "    # https://github.com/Marcussena/ML-and-Ai-from-scratch/blob/main/PCA/pca.py\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.components = None\n",
        "        self.mean = None\n",
        "        self.explained_variance = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        #1: Padronização dos dados (subtraia a média)\n",
        "        self.mean = np.mean(X, axis=0)\n",
        "        Xc = X - self.mean\n",
        "\n",
        "        #2: Cálculo da matriz de covariância\n",
        "        cov_matrix = np.cov(Xc.T)\n",
        "\n",
        "        #3: Cálculo dos autovalores e autovetores\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "        #4: Ordenação dos autovalores e autovetores correspondentes\n",
        "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "        eigenvalues = eigenvalues[sorted_indices]\n",
        "        eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "        # 5: Seleção das top n_components\n",
        "        self.components = eigenvectors[:, :self.n_components]\n",
        "\n",
        "        # Cálculo da variância explicada\n",
        "        total_variance = np.sum(eigenvalues)\n",
        "        self.explained_variance = eigenvalues[:self.n_components] / total_variance\n",
        "\n",
        "    def transform(self, X):\n",
        "        # 6: Projeção dos dados nas componentes selecionadas\n",
        "        Xc = X - self.mean\n",
        "        return np.dot(Xc, self.components)\n",
        "\n",
        "    def plot_explained_variance(self):\n",
        "        # Criação de rótulos para cada componente principal\n",
        "        labels = [f'PCA{i+1}' for i in range(self.n_components)]\n",
        "\n",
        "        # Criação de um gráfico de barras para variação explicada\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.bar(range(1, self.n_components + 1), self.explained_variance, alpha=0.7, align='center', color='blue', tick_label=labels)\n",
        "        plt.xlabel('Componente Principal')\n",
        "        plt.ylabel('Razão de Variância Explicada')\n",
        "        plt.title('Variância explicada por componentes principais')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "c84l6O15VkoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "low_dim_data = np.random.randn(100, 4) # sinal de baixa dimensão\n",
        "projection_matrix = np.random.randn(4, 10) # matriz de projeção aleatória para projetar em dimensões superiores\n",
        "high_dim_data = np.dot(low_dim_data, projection_matrix) # Projeção dos dados de baixa dimensão para dimensões superiores\n",
        "noise = np.random.normal(loc=0, scale=0.5, size=(100, 10)) #  Ruído\n",
        "data_with_noise = high_dim_data + noise # Addição de ruído aos dados de alta dimensão\n",
        "\n",
        "X = data_with_noise\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "IkmSj1LCVnL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA\n",
        "pca = PCA(n_components=10)\n",
        "pca.fit(X)\n",
        "X_transformed = pca.transform(X)\n",
        "\n",
        "print(\"Variância explicada:\\n\", pca.explained_variance)\n",
        "pca.plot_explained_variance()"
      ],
      "metadata": {
        "id": "ySPJ4GNJlFIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA\n",
        "pca = PCA(n_components=3)\n",
        "pca.fit(X)\n",
        "X_transformed = pca.transform(X)\n",
        "\n",
        "print(\"Variância explicada:\\n\", pca.explained_variance)\n",
        "pca.plot_explained_variance"
      ],
      "metadata": {
        "id": "xHYDuA3omlZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?export=view&id=15futtT5ZCa2OKSjnn6jl91ADIW_BdFfi' width=\"600\"></center>\n",
        "\n",
        "O PCA, portanto, nada mais é que a decomposição em autovalores da matriz de covariância. Mas decomposição em autovalores pode ser bem ineficiente e, na prática, o PCA é calculado usando Decomposição em Valores Singulares (SVD, do inglês *Singular Value Decomposition*)."
      ],
      "metadata": {
        "id": "ZUyG8Om-Vq15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA e SVD\n",
        "\n",
        "Toda matriz simétrica quadrada $\\mathbf A $ pode ser diagonalizada através de seus autovalores e autovetores,\n",
        "\n",
        "$$\n",
        "\\mathbf A = \\mathbf V \\mathbf \\Lambda \\mathbf V^T = \\sum_{i = 1}^n \\lambda_i \\mathbf v_i \\mathbf v_i^T\n",
        "$$\n",
        "sendo que $\\mathbf v_i \\perp \\mathbf v_j, i\\neq j$ e $\\| \\mathbf v_i \\| = \\mathbf v_i^T \\mathbf v_i = 1$\n",
        "\n",
        "A decomposição pode ser explicitada como:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf A_{n \\times n} =\n",
        "\\begin{pmatrix}\n",
        "|  & |  &   & |  \\\\\n",
        "\\mathbf v_{1} & \\mathbf v_{2} & \\cdots & \\mathbf v_{n} \\\\\n",
        "|  & |  &   & |  \n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "\\lambda_1 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\lambda_2 & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots &\\vdots \\\\\n",
        "0 & 0 & \\cdots & \\lambda_n\n",
        "\\end{pmatrix} \\begin{pmatrix}\n",
        "-  & \\mathbf v_{1}^T  & -  \\\\\n",
        "- & \\mathbf v_{2}^T & - \\\\\n",
        "  & \\vdots   &    \\\\\n",
        "- & \\mathbf v_{n}^T & -\n",
        "\\end{pmatrix}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "__QhnLQJQB1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[4, 1],\n",
        "              [1, 3]])\n",
        "lam, v = np.linalg.eig(A)\n",
        "print(\"autovalores=\", np.round(lam, 4))\n",
        "print(\"autovetores=\", np.round(v, 4))\n",
        "Diag=np.array([[lam[0], 0],[0, lam[1]]])\n",
        "print(v @ Diag @ v.T)"
      ],
      "metadata": {
        "id": "ReOw3lzqQIrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = array([[ 5, 18, 17],\n",
        "           [18, 11,  3],\n",
        "           [17, 3,  8]])\n",
        "print('A matriz A:')\n",
        "print(A)\n",
        "# SVD\n",
        "V, Sigma, _ = np.linalg.svd(A)\n",
        "print('Autovalores:')\n",
        "print(Sigma)\n",
        "print('Autovetores:')\n",
        "print(V)"
      ],
      "metadata": {
        "id": "ZjnLuT9qTvaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A matriz $\\mathbf{A}$ pode ser decomposta em $n$ matrizes $\\mathbf{v}_i\\mathbf{v}_i^T$ $n \\times n$ ponderadas pelos autovalores $\\lambda_i$:\n",
        "$$\n",
        "\\mathbf{A} = \\sum_{i=1}^n \\lambda_i \\mathbf{v}_i\\mathbf{v}_i^T\n",
        "$$\n",
        "ou\n",
        "$$\n",
        "\\mathbf{A} = \\lambda_1 \\mathbf{v}_1\\mathbf{v}_1^T+\\lambda_2 \\mathbf{v}_2\\mathbf{v}_2^T+\\cdots + \\lambda_n \\mathbf{v}_1\\mathbf{v}_n^T\n",
        "$$\n",
        "\n",
        "Como $\\mathbf{u}_i$ são vetores unitários, podemos até ignorar termos $\\lambda_i \\mathbf{v}_i\\mathbf{v}_i^T$ com $\\lambda_i$ pequeno.\n"
      ],
      "metadata": {
        "id": "XdQH6AWiVDIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código abaixo reconstrói a matriz $\\mathbf A$ com o número de componentes $\\mathbf v_i \\mathbf v_i^T$ necessárias para atingir um limite mínimo de variância explicada definido pela variável `variance_threshold`, cujo padrão é $80\\%$."
      ],
      "metadata": {
        "id": "axcyedo2HAtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def svd_reconstruction_with_variance(matrix, variance_threshold=0.80):\n",
        "    if np.all(matrix-matrix.T==0) == False:\n",
        "        raise ValueError(\"Matriz deve ser simétrica\")\n",
        "\n",
        "    # Cálculo do SVD\n",
        "    V, Lambda, VT = np.linalg.svd(matrix)\n",
        "\n",
        "    # Cálculo da variância total\n",
        "    total_variance = np.sum(Lambda)\n",
        "\n",
        "    # Número de valores singulares necessários para alcançar o limite mínimo de variância dos dados\n",
        "    variance_sum = 0\n",
        "    num_singular_values = 0\n",
        "    variances = []\n",
        "    for i in range(len(Lambda)):\n",
        "        variance_sum += Lambda[i]\n",
        "        variances.append(variance_sum / total_variance)\n",
        "        if variance_sum / total_variance >= variance_threshold:\n",
        "            num_singular_values = i + 1\n",
        "            break\n",
        "\n",
        "    # Uso apenas dos principais valores singulares definido em 'num_singular_values' para reconstruir a matriz\n",
        "    V_reduced = V[:, :num_singular_values]\n",
        "    Lambda_reduced = np.diag(Lambda[:num_singular_values])\n",
        "    VT_reduced = VT[:num_singular_values, :]\n",
        "\n",
        "    # Reconstruct the matrix\n",
        "    reconstructed_matrix = np.dot(V_reduced, np.dot(Lambda_reduced, VT_reduced))\n",
        "\n",
        "    return reconstructed_matrix, num_singular_values, variances"
      ],
      "metadata": {
        "id": "kVQZ1BzwYcD1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[4, 1],\n",
        "              [1, 3]])\n",
        "variance_threshold = 0.6\n",
        "Ar, num_singular_values, variances = svd_reconstruction_with_variance(A, variance_threshold)\n",
        "\n",
        "print(\"Matriz original:\")\n",
        "print(A)\n",
        "print(\"\\nMatriz reconstruída com {:.0f}% de variância:\".format(variance_threshold*100))\n",
        "print(Ar)\n",
        "print(\"\\nNúmero de valores singulares usados:\")\n",
        "print(num_singular_values)\n",
        "\n",
        "print(\"\\nPorcentagem acumulada da variância total de cada valor singular utilizado:\")\n",
        "print(variances)"
      ],
      "metadata": {
        "id": "ZdJMy0mxYxaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10\n",
        "np.random.seed(42)\n",
        "A = np.random.random(size=(N,N))\n",
        "A_symm = (A + A.T)/2\n",
        "print(A_symm)"
      ],
      "metadata": {
        "id": "-2JoDmttCPIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var_threshold = 0.6\n",
        "A_symm_r, num_singular_values, variances = svd_reconstruction_with_variance(A_symm, var_threshold)\n",
        "\n",
        "print(\"Matriz original:\")\n",
        "print(A_symm)\n",
        "print(\"\\nMatriz reconstruída com {:.0f}% de variância:\".format(var_threshold*100))\n",
        "print(A_symm_r)\n",
        "print(\"\\nNúmero de valores singulares usados:\")\n",
        "print(num_singular_values)\n",
        "\n",
        "print(\"\\nPorcentagem acumulada da variância total de cada valor singular utilizado:\")\n",
        "print(variances)"
      ],
      "metadata": {
        "id": "RjRaSHfyC0bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E quando amatriz não é simétrica?\n",
        "\n",
        "Dada uma matriz $\\mathbf A \\in \\mathbb{R}^{m \\times n}$, podemos calcular a decomposição SVD como:\n",
        "\n",
        "$$\n",
        "\\mathbf A = \\mathbf U \\boldsymbol \\Sigma \\mathbf V^T\n",
        "$$\n",
        "\n",
        "As colunas das matrizes $\\mathbf U$ e $\\mathbf V$ são formadas pelos autovetores de $\\mathbf A \\mathbf A^T$ e $\\mathbf A^T \\mathbf A$, respectivamente.\n",
        "\n",
        "A matriz diagonal $\\boldsymbol \\Sigma$ é composta dos autovalores $\\sigma_i$ da matriz $\\mathbf A \\mathbf A^T$ ou $\\mathbf A^T \\mathbf A$. O valor $\\sigma_i$ é conhecido como valor singular.\n",
        "\n",
        "Importante relembrar que os autovalores $\\lambda_i$ de $\\mathbf A$ se relacionam com $\\sigma_i$ de modo que:\n",
        "\n",
        "$$\\sigma_i = \\sqrt{\\lambda_i}$$"
      ],
      "metadata": {
        "id": "rnam0YnIV59a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.random.rand(10, 2)\n",
        "print(A)"
      ],
      "metadata": {
        "id": "bvrc1m1xWJ7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variance_threshold=0.80\n",
        "A_reduced, num_singular_values, variances = svd_reconstruction_with_variance(A,variance_threshold)\n",
        "\n",
        "print(\"Matriz original:\")\n",
        "print(A)\n",
        "print(\"\\nMatriz reconstruída com {:0.0f}% de variância:\".format(variance_threshold*100))\n",
        "print(A_reduced)\n",
        "print(\"\\nNúmero de valores singulares usados:\")\n",
        "print(num_singular_values)\n",
        "\n",
        "print(\"\\nPorcentagem acumulada da variância total de cada valor singular utilizado:\")\n",
        "print(variances)"
      ],
      "metadata": {
        "id": "-wo1lz2FCrYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svd_reconstruction_with_variance(matrix, variance_threshold=0.80):\n",
        "    # Cálculo do SVD\n",
        "    U, Sigma, VT = np.linalg.svd(matrix, full_matrices=False)\n",
        "\n",
        "    # Cálculo da variância total (soma dos valores singulares ao quadrado)\n",
        "    total_variance = np.sum(Sigma**2)\n",
        "\n",
        "    # Número de valores singulares necessários para alcançar\n",
        "    # o limite mínimo de variância dos dados\n",
        "    variance_sum = 0\n",
        "    num_singular_values = 0\n",
        "    variances = []\n",
        "    for i in range(len(Sigma)):\n",
        "        variance_sum += Sigma[i]**2\n",
        "        variances.append(variance_sum / total_variance)\n",
        "        if variance_sum / total_variance >= variance_threshold:\n",
        "            num_singular_values = i + 1\n",
        "            break\n",
        "\n",
        "    # Uso apenas dos principais valores singulares definido em\n",
        "    # 'num_singular_values' para reconstruir a matriz\n",
        "    U_reduced = U[:, :num_singular_values]\n",
        "    Sigma_reduced = np.diag(Sigma[:num_singular_values])\n",
        "    VT_reduced = VT[:num_singular_values, :]\n",
        "\n",
        "    # Reconstrução da Matriz\n",
        "    reconstructed_matrix = np.dot(U_reduced, np.dot(Sigma_reduced, VT_reduced))\n",
        "\n",
        "    return reconstructed_matrix, num_singular_values, variances"
      ],
      "metadata": {
        "id": "Ft2UlBmYbnQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variance_threshold=0.80\n",
        "A_reduced, num_singular_values, variances = svd_reconstruction_with_variance(A,variance_threshold)\n",
        "\n",
        "print(\"Matriz original:\")\n",
        "print(A)\n",
        "print(\"\\nMatriz reconstruída com {:0.0f}% de variância:\".format(variance_threshold*100))\n",
        "print(A_reduced)\n",
        "print(\"\\nNúmero de valores singulares usados:\")\n",
        "print(num_singular_values)\n",
        "\n",
        "print(\"\\nPorcentagem acumulada da variância total de cada valor singular utilizado:\")\n",
        "print(variances)"
      ],
      "metadata": {
        "id": "YurK_p3-b2Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotar dados originais/dados reduzidos\n",
        "plt.scatter(A[:, 0], A[:, num_singular_values], alpha = 0.6, label = 'Original')\n",
        "plt.scatter(A_reduced[:, 0], A_reduced[:, num_singular_values], c='crimson', label = 'Dim. reduzida')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q4m940tcWwn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voltando ao nosso problema, a matriz de covariância é simétrica e pode ser, portanto, decomposta em autovalores\n",
        "$$\n",
        "\\mathbf S = \\frac{1}{m-1}\\mathbf X_c^T \\mathbf X_c = \\mathbf V \\mathbf \\Lambda \\mathbf V^T\n",
        "$$\n",
        "ou\n",
        "$$\n",
        "\\mathbf S =\\sum_{i = 1}^m \\lambda_i \\mathbf v_i \\mathbf v_i^T\n",
        "$$\n",
        "\n",
        "Os autovetores $\\mathbf v_i$ são chamados de eixos principais ou direções principais dos dados.\n",
        "\n",
        "Portanto, a matriz $\\mathbf S$ pode ser decomposta em $n$ matrizes $\\mathbf v_i \\mathbf v_i^T$ $n\\times n$, ponderadas de $\\lambda_i$, onde $\\mathbf v_{i}$ são os $n$ autovetores de $\\mathbf S$ e $\\lambda_i$ os respectivos autovalores distribuídos na matriz diagonal $\\mathbf \\Lambda$.\n"
      ],
      "metadata": {
        "id": "-Zleuudmngzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veja que, se transformamos o vetor de dados da seguinte forma:\n",
        "$$\n",
        "\\mathbf Z=\\mathbf X_c\\mathbf V\n",
        "$$\n",
        "basta multiplicar o conjunto de dados, após centralizado, pela matriz de autovetores encontrada na decomposição da matriz de covariância.\n",
        "\n",
        "Essa transformação $\\mathbf Z=\\mathbf X_c\\mathbf V$ leva a uma matriz de covariância,\n",
        "\\begin{equation}\n",
        "\\mathbf S_Z\n",
        "= {1 \\over {m-1}}\\mathbf Z^T \\mathbf Z\n",
        "\\end{equation}\n",
        "\n",
        "Portanto,\n",
        "\\begin{equation}\\begin{array}{cl}\n",
        "\\mathbf S_z & = {1 \\over {m-1}}\\left(\\mathbf X_c\\mathbf V\\right)^T \\left(\\mathbf X_c \\mathbf V\\right) \\\\\n",
        "& = {1 \\over {m-1}}\\mathbf V^T \\mathbf X_c^T \\mathbf X_c \\mathbf V \\\\\n",
        "&   = {1 \\over {m-1}}\\mathbf V^T \\left(\\mathbf X_c^T \\mathbf X_c\\right) \\mathbf V \\\\\n",
        "& = \\mathbf V^T \\mathbf S \\mathbf V \\\\\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "onde $\\mathbf S=\\frac{1}{m-1}\\mathbf X_c^T \\mathbf X_c$ que, conforme aprendemos, pode ser decomposta em $\\mathbf S=\\mathbf V \\mathbf \\Lambda \\mathbf V^T$,\n",
        "\\begin{equation}\\begin{array}{cl}\n",
        "\\mathbf S_z & = \\mathbf V^T\\left(\\mathbf V\\mathbf \\Lambda \\mathbf V^T \\right) \\mathbf V \\\\\n",
        "& = \\left(\\mathbf V^T\\mathbf V\\right)\\mathbf \\Lambda \\left(\\mathbf V^T \\mathbf V \\right)\\\\\n",
        "& = \\mathbf \\Lambda \\\\\n",
        "\\end{array}\n",
        "\\end{equation}\n",
        "sabendo-se que o vetor $\\mathbf V^T=\\mathbf V^{-1}$.Portanto,\n",
        "$$\n",
        "\\mathbf S_z =\\mathbf \\Lambda \\tag{1}\n",
        "$$\n",
        "\n",
        "Dessa forma,  $\\mathbf S_z$ é diagonal, e esse era outro objetivo do PCA!!! Além da máxima variância, lembre-se que estamos procurando uma transformação com a covariância mínima possível, que é zero. Então, o ideal é que $\\mathbf S_z$ seja uma matriz diagonal.  Que foi exatamente o que encontramos. O i-ésimo valor diagonal de $\\mathbf S_z$ é a variância de $\\mathbf X$ ao longo de $\\mathbf v_i$.\n"
      ],
      "metadata": {
        "id": "2jHJ9zZ1YMux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se agora aplicarmos a decomposição de valor singular (SVD) à matriz de dados centralizada $\\mathbf X_c$, obteremos uma decomposição,\n",
        "$$\n",
        "\\mathbf X_c=\\mathbf U\\mathbf \\Sigma\\mathbf V^T\n",
        "$$\n",
        "onde $\\mathbf \\Sigma$ é a matriz diagonal que possui os autovalores $\\sigma_i$ de $\\mathbf X_c$. Os autovetores de $\\mathbf X_c\\mathbf X_c^𝑇$ formam as colunas da matriz $\\mathbf U$, e os autovetores de $\\mathbf X_c^T\\mathbf X_c$ formam as colunas da matriz $\\mathbf V$ (vejam que essa matriz $\\mathbf V$ já foi definida anteriormente na decomposição em autovalores) .\n",
        "\n",
        "Tentando construir a matriz de covariância a partir desta decomposição tem-se\n",
        "$$\n",
        "\\mathbf S = \\frac{1}{m-1}\\mathbf X_c^T\\mathbf X_c\n",
        "=\\frac{1}{m-1}(\\mathbf U\\mathbf \\Sigma\\mathbf V^T)^T(\\mathbf U\\mathbf \\Sigma\\mathbf V^T)\n",
        "= \\frac{1}{m-1}(\\mathbf V\\mathbf \\Sigma\\mathbf U^T)(\\mathbf U\\mathbf \\Sigma\\mathbf V^T)\n",
        "$$\n",
        "e como $\\mathbf U^T \\mathbf U=\\mathbf I$,\n",
        "$$\n",
        "\\mathbf S =\\frac{1}{m-1}\\mathbf X_c^T\\mathbf X_c=\\frac{1}{m-1}\\mathbf V\\mathbf \\Sigma^2 \\mathbf V^T \\mathbf =\\mathbf V \\mathbf \\Lambda \\mathbf V^T\n",
        "$$\n",
        "e a correspondência é facilmente vista. Por exemplo, as raízes quadradas dos autovalores de $\\mathbf X_c^T\\mathbf X_c$ são os valores singulares de $\\mathbf X_c$,\n",
        "$$\n",
        "\\mathbf \\Lambda = \\frac{1}{m-1}\\mathbf \\Sigma^2\n",
        "$$\n",
        "\n",
        "significando que os vetores singulares à direita $\\mathbf V$ são direções principais e que os valores singulares estão relacionados aos autovalores da matriz de covariância via\n",
        "$$\\lambda_i = \\frac{\\sigma_i^2}{m−1}. \\tag{2}$$\n",
        "\n",
        "Dessa forma, é possível, sem criar a matriz de covariância, achar as componentes principais."
      ],
      "metadata": {
        "id": "jXeDLdbj_fcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo\n",
        "\n",
        "Para a matriz do conjunto de dados $\\mathbf X$\n",
        "\\begin{equation}\n",
        "\\mathbf X = \\begin{bmatrix}\n",
        "3 & 4 & 3 & 1\\\\\n",
        "1 & 3 & 2 & 6\\\\\n",
        "2 & 4 & 1 & 5\\\\\n",
        "3 & 3 & 5 & 2\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "de modo que,\n",
        "\n",
        "$$\n",
        "\\mathbf S = \\frac{1}{n-1}\\mathbf X_c^T\\mathbf X_c=\\mathbf V\\mathbf \\Lambda \\mathbf V^T\n",
        "$$\n",
        "\n",
        "Plote a variância relacionada às componentes principais."
      ],
      "metadata": {
        "id": "SdGdomrDnAVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.matrix([[3, 4, 3, 1],[1,3,2,6],[2,4,1,5],[3,3,5,2]])\n",
        "m,n = X.shape\n",
        "print(m,n)"
      ],
      "metadata": {
        "id": "iq85_Q3R5KYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cov = np.cov(X.T)\n",
        "print(f'Matriz de Covariância:\\n {cov}')"
      ],
      "metadata": {
        "id": "tT8sSXY85NRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calcula a média de cada coluna\n",
        "xbar = np.mean(X, axis = 0).T\n",
        "#gera um vetor de 1s\n",
        "Vet1 = np.ones((m,1))\n",
        "#subtrai de cada coluna de A sua respectiva média\n",
        "Xc = X - np.outer(Vet1,xbar)\n",
        "print(f'Matriz X centralizada:\\n {Xc}')"
      ],
      "metadata": {
        "id": "3cHQV9vl5TS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V, lambdai, _ = np.linalg.svd(cov)\n",
        "Lambda = np.diag(lambdai)\n",
        "print(f'Matriz Diagonal com autovalores da matriz de Covariância:\\n {Lambda}')"
      ],
      "metadata": {
        "id": "czdMvu6a6T0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Variância através da matriz X centralizada Xc:\\n {np.dot(Xc.T,Xc)/(m-1)}')\n",
        "print(f'Variância através de autovetores:\\n {np.dot(np.dot(V,Lambda),V.T)}')"
      ],
      "metadata": {
        "id": "n6eiRK4m6976"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp = V\n",
        "Z = np.dot(Xc,cp)\n",
        "print(f'Projeção de Xc nas direções principais (componentes principais):\\n{Z}')"
      ],
      "metadata": {
        "id": "1z8H735J7jXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análise da matriz de covariância\n",
        "\n",
        "Os autovetores $\\mathbf v_i$ são chamados de eixos principais ou direções principais dos dados. As projeções dos dados nos eixos principais são chamadas de *componentes principais*, também conhecidas como *PC scores*; estas podem ser vistas como as novas variáveis transformadas.\n",
        "\n",
        "A j-ésima componente principal é dada pela j-ésima coluna de $\\mathbf X_c \\mathbf V $. As coordenadas do i-ésimo ponto de dados no novo espaço PC são dadas pela i-ésima linha de $\\mathbf X_c \\mathbf V $."
      ],
      "metadata": {
        "id": "SgJLwN9OcpEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "U, sigmai, Vt = np.linalg.svd(Xc)\n",
        "Sigma = np.diag(sigmai)\n",
        "print(f'Matriz Diagonal com autovalores de Xc:\\n {Sigma}')"
      ],
      "metadata": {
        "id": "9WaA_3yhBMlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceba que os autovalores $\\sigma_i$ são resultado da decomposição em valor singular do vetor $\\mathbf{X}_c$, enquanto que os autovalores $\\lambda_i$ referem-se à decomposição em autovalores da matriz de covariância. Eles estão relacionados da seguinte forma:\n",
        "$$\\lambda_i = \\frac{\\sigma_i^2}{m−1}.$$\n",
        "\n",
        "São os autovalores da matriz de covariância $\\lambda_i$ que mostram as variâncias das respectivas PCs.\n",
        "\n",
        "Compare o primeiro autovalor $\\lambda_1=8.23690504$ com:"
      ],
      "metadata": {
        "id": "r_LFSxleCuSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "4.97098734**2/(m-1)"
      ],
      "metadata": {
        "id": "3xn_Rt2EC2EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Variância através de autovetores e autovalores de Xc:\\n {np.dot(np.dot(Vt.T,Sigma**2),Vt)/(m-1)}')"
      ],
      "metadata": {
        "id": "by4_ZH9oBcRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# conforme a teoria, Z pode ser calculado também como,\n",
        "print(f'Pode ser também calculada como U*Sigma:\\n{np.dot(U,Sigma)}')"
      ],
      "metadata": {
        "id": "r70V2clpBsS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_list = np.array(np.arange(1,m+1))\n",
        "x_list = range(math.floor(min(x_list)), math.ceil(max(x_list))+1) # só para o gráfico mostrar inteiros no eixo x\n",
        "\n",
        "plt.scatter(x_list, lambdai, marker =\"s\", s = 50,alpha=0.5)\n",
        "plt.xlabel(\"Componente principal\")\n",
        "plt.ylabel(\"Autovalor \"+r\"$\\lambda_i$\")\n",
        "plt.xticks(x_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vb1ZNetq8NkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo bidimensional novamente\n",
        "\n",
        "Voltemos ao exemplo bidimensional,"
      ],
      "metadata": {
        "id": "e0SHHU80tSJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Voltemos aos nossos dados aleatórios iniciais\n",
        "m = 100\n",
        "n = 2\n",
        "np.random.seed(42)\n",
        "x1 = 10.*np.sqrt((np.random.normal(size=m))**2)\n",
        "x2 = x1 + 5.8*(np.random.normal(size=m))\n",
        "X = np.stack((x1,x2),axis=1)"
      ],
      "metadata": {
        "id": "an9D46fUtTMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calcula a média de cada coluna\n",
        "xbar = np.mean(X, axis = 0).T\n",
        "#gera um vetor de 1s\n",
        "Vet1 = np.ones((m,1))\n",
        "#subtrai de cada coluna de A sua respectiva média\n",
        "Xc = X - np.outer(Vet1,xbar)\n",
        "print(f'Primeiros 10 valores da Matriz A centralizada:\\n {Xc[:10]}')"
      ],
      "metadata": {
        "id": "jmfH9Fa-OPYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x1, x2, marker =\"s\", s = 50,alpha=0.5)\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.show()\n",
        "print(\"Variância x1 =\",np.var(x1,ddof=1))\n",
        "print(\"Variância x2 =\",np.var(x2,ddof=1))"
      ],
      "metadata": {
        "id": "iAKSf3fVFDP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "U, sigmai, VT = np.linalg.svd(Xc)\n",
        "Sigma =  np.diag(sigmai)\n",
        "print(f'Matriz Diagonal com autovalores de A:\\n {Sigma}')"
      ],
      "metadata": {
        "id": "wfTPFFmWO3M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xaxis_title=\"CP-1\"\n",
        "yaxis_title=\"CP-2\"\n",
        "plt.scatter(Z[:,0], Z[:,1], marker =\"s\", s = 50,alpha=0.5)\n",
        "plt.xlabel(xaxis_title)\n",
        "plt.ylabel(yaxis_title)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lujI_IrQjFt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXZnGtQ_rKIU"
      },
      "source": [
        "Depois que as direções principais são calculadas, podemos projetar nossos dados como,\n",
        "\n",
        "$$ \\mathbf Z_{(m \\times n)} = \\mathbf X_{c{(m \\times n)}} \\mathbf V _ {(n \\times n)}, $$\n",
        "\n",
        "onde $ \\mathbf V$ é a matriz das direções principais e  as colunas de $\\mathbf Z$ definem as componentes principais.  Lembre-se: A j-ésima componente principal é dada pela j-ésima coluna de $\\mathbf X_c \\mathbf V $. As coordenadas do i-ésimo ponto de dados no novo espaço PC são dadas pela i-ésima linha de $\\mathbf X_c \\mathbf V $.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cp = VT.T\n",
        "Z = np.dot(Xc,cp)\n",
        "print(f'Dimensão da matriz de projeção de Xc nas direções principais (componentes principais):\\n{Z.shape}')"
      ],
      "metadata": {
        "id": "o9R4b58wOCvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxzoFPiRxgAR"
      },
      "source": [
        "## Redução de dimensionalidade\n",
        "\n",
        "Vamos reduzir a dimensão para \"$k$\". Observe na figura que basta selecionar os $k$ primeiros autovetores, ié, as $k$ primeiras colunas de $\\mathbf V$.\n",
        "\n",
        "A projeção $\\mathbf z_i = \\mathbf V^T\\mathbf x_i$  pode ser escrita como a operação de matrizes:\n",
        "$$\n",
        "\\mathbf Z_{(m,k)} = \\mathbf X_{{(m,n)}}\\mathbf V_{(n,k)}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGGhRocbFdkl"
      },
      "source": [
        "Portanto,\n",
        "\\begin{equation}\n",
        "\\mathbf V_{n,n} =\n",
        "\\begin{pmatrix}\n",
        "|  & |  &   & |  \\\\\n",
        "\\mathbf v_{1} & \\mathbf v_{2} & \\cdots & \\mathbf v_{k}\\\\\n",
        "|  & |  &   & |\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "de forma que:\n",
        "\\begin{equation}\n",
        "\\mathbf Z_{m,k} = \\mathbf X_{m,n} \\mathbf V_{n,k} =  \n",
        "\\begin{pmatrix}\n",
        "{\\mathbf x}_{1}\\mathbf v_{1}& {\\mathbf x}_{1} \\mathbf v_{2}  & \\cdots & {\\mathbf x}_{1} \\mathbf v_{k}\\\\\n",
        "{\\mathbf x}_{2}\\mathbf v_{1}& {\\mathbf x}_{2} \\mathbf v_{2}  & \\cdots & {\\mathbf x}_{2} \\mathbf v_{k}\\\\\n",
        "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
        "{\\mathbf x}_{m}\\mathbf v_{1}& {\\mathbf x}_{m} \\mathbf v_{2}  & \\cdots & {\\mathbf x}_{m} \\mathbf v_{kn}\\\\\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "A nova matriz $\\mathbf Z$ pode ser definida como,\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf Z_{m,k} =\n",
        "\\begin{pmatrix}\n",
        "|  & |  &   & |  \\\\\n",
        "\\hat{\\mathbf z}_{1} & \\hat{\\mathbf z}_{2} & \\cdots & \\hat{\\mathbf z}_{k} \\\\\n",
        "|  & |  &   & |  \n",
        "\\end{pmatrix}=\n",
        "\\begin{pmatrix}\n",
        "-  & {\\mathbf z}_{1}   & -  \\\\\n",
        " - & {\\mathbf z}_{2}   & - \\\\\n",
        "   & \\vdots &    \\\\\n",
        " - & {\\mathbf z}_{m} & -\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "onde\n",
        "\\begin{equation}\n",
        "\\hat{\\mathbf z}_i =\n",
        "\\begin{pmatrix}\n",
        "{\\mathbf x}_{1}\\mathbf v_{i} \\\\\n",
        "{\\mathbf x}_{2}\\mathbf v_{i} \\\\\n",
        "\\vdots   \\\\\n",
        "{\\mathbf x}_{m}\\mathbf v_{i}\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "é a transformação das observações na direção $\\mathbf v_i$, e\n",
        "\\begin{equation}\n",
        "{\\mathbf z}_i =\n",
        "\\begin{pmatrix}\n",
        "{\\mathbf x}_{i}\\mathbf v_{1} & {\\mathbf x}_{i}\\mathbf v_{2} & \\cdots  & {\\mathbf x}_{i}\\mathbf v_{k}\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "é a transformação da observação $i$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5LSKg8LxgAS"
      },
      "source": [
        "k = 1\n",
        "Vk = cp[:,0:k]\n",
        "Zk = np.dot(Xc,Vk)\n",
        "print(\"Variância após a transformação\",np.var(Zk, ddof = 1, axis=0))\n",
        "print(\"Fração de variância capturada após a transformação: \",np.var(Zk,ddof = 1,axis=0)/np.sum(np.var(X_std, ddof =1, axis=0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgXSy9tSxgAX"
      },
      "source": [
        "Agora, vamos calcular a projeção inversa. Observe que as projeções podem ser escritas como $\\mathbf z_i = \\mathbf V^T \\mathbf x_i$, portanto, a projeção inversa (re-projeção) pode ser escrita como\n",
        "$$ \\mathbf x^r_i = \\mathbf {V}^{-1} \\mathbf z_i = \\mathbf {V}^{T} \\mathbf z_i $$\n",
        "\n",
        "Em forma de matrizes, temos\n",
        "$$\\mathbf X^r = \\mathbf Z \\mathbf V^T=\\mathbf X \\mathbf V \\mathbf V^T$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xr = np.dot(Zk,Vk.T)\n",
        "print(\"Variância após a re-transformação\",np.var(Xr, ddof = 1, axis=0))\n",
        "# Plotar os dados\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# Plotar os dados X_std\n",
        "plt.scatter(X_std[:, 0], X_std[:, 1], c='lightskyblue', marker='s', s=36, alpha=0.5, label='Dados X')\n",
        "\n",
        "# Plotar os dados Xr\n",
        "plt.scatter(Xr[:, 0], Xr[:, 1], c='cadetblue', s=64, alpha=0.8, label='Dados Xr')\n",
        "\n",
        "for x, xr in zip(X_std, Xr):\n",
        "    plt.plot([x[0], xr[0]], [x[1], xr[1]], 'DarkTurquoise', linestyle='dotted', linewidth=2)\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.title('Visualização dos Dados Originais e Reconstruídos')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8He2pf9EGG4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUfcoX_8m-ym"
      },
      "source": [
        "Para analisar os dados dentro de sua devida ordem de grandeza, deve-se transformar os dados normalizados **multiplicando pelo vetor de desvios padrão e adicionando o vetor da média**, ambos calculados anteriormente.\n",
        "Calcule e plote os dados projetoados, agora não mais normalizados."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xm = np.mean(X,axis=0)\n",
        "Xs = np.std(X,axis=0)\n",
        "\n",
        "Xr = np.dot(Zk,Vk.T)*Xs+Xm\n",
        "\n",
        "# Plotar os dados\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# Plotar os dados X\n",
        "plt.scatter(X[:, 0], X[:, 1], c='lightskyblue', marker='s', s=36, alpha=0.5, label='Dados X')\n",
        "\n",
        "# Plotar os dados Xr\n",
        "plt.scatter(Xr[:, 0], Xr[:, 1], c='cadetblue', s=64, alpha=0.8, label='Dados Xr')\n",
        "\n",
        "for x, xr in zip(X, Xr):\n",
        "    plt.plot([x[0], xr[0]], [x[1], xr[1]], 'DarkTurquoise', linestyle='dotted', linewidth=2)\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.title('Visualização dos Dados Originais e Reconstruídos')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o-PB-EcMIW2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classe PCA"
      ],
      "metadata": {
        "id": "2rAIID5gktjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PCA:\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.components = None\n",
        "        self.mean = None\n",
        "        self.explained_variance = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        #1: Padronização dos dados (subtração da média)\n",
        "        self.mean = np.mean(X, axis=0)\n",
        "        Xc = X - self.mean\n",
        "\n",
        "        #2: Cálculo das componentes principais\n",
        "        U, sigmai, VT = np.linalg.svd(Xc)\n",
        "        self.components = VT.T[:, :self.n_components]\n",
        "\n",
        "        #3: Cálculo da variância explicada\n",
        "        lambdai=sigmai**2/(len(U)-1)\n",
        "        total_variance = np.sum(lambdai)\n",
        "        self.explained_variance = lambdai[:self.n_components] / total_variance\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Projeção dos dados nas componentes selecionadas\n",
        "        Xc = X - self.mean\n",
        "        Zk = np.dot(Xc, self.components)\n",
        "        Xk = np.dot(Zk, self.components.T)\n",
        "        return Zk,Xk\n",
        "\n",
        "    def inverse_transform(self, Zk):\n",
        "        # Reconstrução dos dados\n",
        "        return np.dot(Xc, self.components)\n",
        "\n",
        "    def get(self):\n",
        "        return self.components, self.mean, self.explained_variance\n",
        "\n",
        "    def plot_explained_variance(self):\n",
        "        # Criação de rótulos para cada componente principal\n",
        "        labels = [f'PCA{i+1}' for i in range(self.n_components)]\n",
        "\n",
        "        # Criação de um gráfico de barras para variação explicada\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.bar(range(1, self.n_components + 1), self.explained_variance, alpha=0.7, align='center', color='blue', tick_label=labels)\n",
        "        plt.xlabel('Componente Principal')\n",
        "        plt.ylabel('Razão de Variância Explicada')\n",
        "        plt.title('Variância explicada por componentes principais')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "BLdOqATQkQ6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = 100\n",
        "n = 2\n",
        "np.random.seed(42)\n",
        "x1 = 10.*np.sqrt((np.random.normal(size=m))**2)\n",
        "x2 = x1 + 5.8*(np.random.normal(size=m))\n",
        "\n",
        "X = np.stack((x1,x2),axis=1)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "ULszwcZUnBZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], marker =\"s\", s = 50,alpha=0.5)\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-7AiiZw97aZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "\n",
        "cps, meanX, explained_variance = pca.get()\n",
        "print(\"Variância explicada:\\n\", explained_variance)\n",
        "pca.plot_explained_variance()"
      ],
      "metadata": {
        "id": "9hBZJYj4m1QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=1)\n",
        "pca.fit(X)\n",
        "cps, meanX, explained_variance = pca.get()\n",
        "print(\"Variância explicada:\\n\", explained_variance)"
      ],
      "metadata": {
        "id": "68tUVo93yFOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xc = X - meanX\n",
        "Zk, Xk = pca.transform(X)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "# Plotar os dados Xc\n",
        "plt.scatter(Xc[:, 0], Xc[:, 1], c='lightskyblue', marker='s', s=36, alpha=0.5, label='Dados X')\n",
        "\n",
        "# Plotar os dados Xk\n",
        "plt.scatter(Xk[:, 0], Xk[:, 1], c='cadetblue', s=64, alpha=0.8, label='Dados Xk')\n",
        "\n",
        "for x, xk in zip(Xc, Xk):\n",
        "    plt.plot([x[0], xk[0]], [x[1], xk[1]], 'DarkTurquoise', linestyle='dotted', linewidth=2)\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.title('Visualização dos Dados Originais e Reconstruídos')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vhLF_asFp6sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um exemplo 3D..."
      ],
      "metadata": {
        "id": "GWULHfX9AeU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = 100\n",
        "n = 3\n",
        "np.random.seed(42)\n",
        "x1 = 10.*np.sqrt((np.random.normal(size=m))**2)\n",
        "x2 = x1 + 5.8*(np.random.normal(size=m))\n",
        "x3 = x1 + 10.*np.sqrt((np.random.normal(size=m))**2)\n",
        "X = np.stack((x1,x2,x3),axis=1)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "xpA6VG7z85vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "\n",
        "ax = plt.axes(projection ='3d')\n",
        "ax.scatter(X[:,0], X[:,1], X[:,2], 'green')\n",
        "ax.set_title('Dados tridimensionais')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dE2scah29BjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=1)\n",
        "pca.fit(X)\n",
        "cps, meanX, explained_variance = pca.get()\n",
        "print(\"Variância explicada:\\n\", explained_variance)"
      ],
      "metadata": {
        "id": "MCVchN8DAl_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xc = X - meanX\n",
        "Zk, Xk = pca.transform(X)"
      ],
      "metadata": {
        "id": "3Z3PndhBA6EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "\n",
        "ax = plt.axes(projection ='3d')\n",
        "ax.scatter(Xc[:,0], Xc[:,1], Xc[:,2], label='Dados Xc')\n",
        "plt.scatter(Xk[:, 0], Xk[:, 1], c='cadetblue', s=30, alpha=0.8, label='Dados Xk')\n",
        "ax.set_title('Dados tridimensionais')\n",
        "ax.view_init(elev=90., azim=180)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-LqLfAeKAslJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "cps, meanX, explained_variance = pca.get()\n",
        "print(\"Variância explicada:\\n\", explained_variance)\n",
        "pca.plot_explained_variance()"
      ],
      "metadata": {
        "id": "qmp06JBrBtNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xc = X - meanX\n",
        "Zk, Xk = pca.transform(X)"
      ],
      "metadata": {
        "id": "PY8dxdE4B2qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8, 8))\n",
        "\n",
        "ax = plt.axes(projection ='3d')\n",
        "ax.scatter(Xc[:,0], Xc[:,1], Xc[:,2], c= 'navy', label='Dados Xc')\n",
        "ax.plot_trisurf(Xk[:,0], Xk[:,1], Xk[:,2],  alpha=0.8, linewidth=0, edgecolor='none')\n",
        "ax.set_title('Dados tridimensionais')\n",
        "ax.view_init(elev=30., azim=40)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o0obMbo3B55L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seus estudos\n",
        "\n",
        "1. Explique as direções principais (em azul) das imagens Parte I e construa as componentes principais da Parte II.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1pgmLj0XGXOegBmAd6YJfmZn0DuAE77Yi' width=\"800\"></center>\n",
        "\n",
        "\n",
        "2. __What influences love at first sight?__ A partir de dados disponível no Kaggle e compilado por professores da Columbia Business School, quando tentavam encontrar uma resposta para a pergunta O que influencia o amor à primeira vista? , aplique PCA e tire conclusões relevantes sobre o tema.\n",
        "Os dados foram coletados de participantes em eventos experimentais de speed dating de 2002 a 2004.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1e_uFYG37Xjz5JJsq3tC3kLoWenrg6jo6' width=\"400\"></center>"
      ],
      "metadata": {
        "id": "Gm9_v2W_MiUg"
      }
    }
  ]
}