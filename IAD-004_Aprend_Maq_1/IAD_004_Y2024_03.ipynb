{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <font face=\"Verdana\" size=6 color='#6495ED'> IAD-004 APRENDIZAGEM DE M√ÅQUINA 1\n",
        "<font face=\"Verdana\" size=3 color='#40E0D0'> Professores Larissa Driemeier e Thiago Martins\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1J3dF7v9apzpj27oOsrT8aEagtNIYwq7J' width=\"600\"></center>\n",
        "\n",
        "Este notebook introdut√≥rio √© sobre problemas de Regress√£o log√≠stica, baseado na aula [IAD-004](https://alunoweb.net/moodle/pluginfile.php/142785/mod_resource/content/2/ML1_A03_Y2024.pdf), ano 2024."
      ],
      "metadata": {
        "id": "r0osRN_Y3F7O"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBR3rOF3SCq1"
      },
      "source": [
        "import operator\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU1dBbSje8ff"
      },
      "source": [
        "# Regress√£o Log√≠stica\n",
        "\n",
        "Define-se \"raz√£o de chance\" de um evento (*odds* ou *odds ratio* em Ingl√™s) bin√°rio (ou seja, que pode ou n√£o acontecer) como a raz√£o entre a probabilidade do evento acontecer e a probabilidade do evento n√£o acontecer.\n",
        "Assim, se $p$ √© a probabilidade do evento acontecer,\n",
        "\n",
        "\\begin{equation}\n",
        "OR(p) = \\frac{p}{1-p}\n",
        "\\end{equation}\n",
        "\n",
        "O modelo log√≠stico pressup√µe que o *logaritmo da raz√£o de chance* de um evento bin√°rio √© resultado de uma combina√ß√£o linear de vari√°veis explicativas ou caracter√≠sticas (independentes). Assim, a hip√≥tese do modelo logistico √©:\n",
        "\n",
        "\\begin{equation}\n",
        "\\ln\\left( \\frac{p}{1-p}\\right) = \\boldsymbol{\\omega}^T \\boldsymbol{X} \\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "onde $\\boldsymbol{X} =\\left\\{x_1, \\ldots, x_n\\right\\}$ √© um vetor de caracter√≠sticas e $\\boldsymbol{\\omega} =\\left\\{\\omega_1, \\ldots, \\omega_n\\right\\}$ √© vetor com *pesos* a elas atribu√≠do.\n",
        "\n",
        "Da equa√ß√£o (1), tira-se:\n",
        "\n",
        "\\begin{equation}\n",
        "p = \\sigma(\\boldsymbol{\\omega}^T \\boldsymbol{X}) = \\frac{1}{1+e^{-\\boldsymbol{\\omega}^T \\boldsymbol{X}}} \\tag{2}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "A fun√ß√£o $h$ definida em (2) √© conhecida como *fun√ß√£o log√≠stica* ou sigm√≥ide.\n",
        "Nota-se que $1-h(x) = h(-x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigm√≥ide\n",
        "\n",
        "Uma das fun√ß√µes mais populares √© a sigmoide, uma fun√ß√£o poderosa principalmente para os problemas de classifica√ß√£o. Basicamente, a fun√ß√£o sigm√≥ide retorna um valor entre $1$ e $0$, bastante √∫til para problemas de classifica√ß√£o bin√°ria.\n",
        "\n",
        "Mas como podemos interpretar um valor retornado por uma fun√ß√£o sigm√≥ide?\n",
        "\n",
        "Suponha que voc√™ treinou uma Rede Neural para classificar imagens de C√£es e Gatos, problema cl√°ssico, onde *c√£o* √© 1 e *gato* √© 0. Basicamente, quando seu modelo retorna valores $p \\ge 0.5$ significa que a imagem √© de um c√£o, e $ p < 0.5$ significa que a imagem √© de um gato.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=12lh2rLGm2r_D-DwkQB4ChBGAQP0deT26' width=\"400\"></center>"
      ],
      "metadata": {
        "id": "kb7Mln-pPWsO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyg9CD0vfPp9"
      },
      "source": [
        "### Exemplo 01\n",
        "\n",
        "Suponha que a probabilidade de um cliente adquirir um produto por mala direta √©,\n",
        "$$\n",
        "p(ùëíùë£ùëíùëõùë°ùëú)=\\frac{1}{1+e^{‚àí(-1.143+0.452 x_1+0.029 x_2 ‚àí 0.242x_3 )}}\n",
        "$$\n",
        "onde $x_1$ √© o sexo (1 para feminino e 0 para masculino), $x_2$ √© a idade e $x_3$ √© o estado civil (1 para solteiro e 0 para casado).\n",
        "\n",
        "Uma pessoa do sexo feminino, com 40 anos de idade e casada, ir√° adquirir o produto?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAsuAs3ve_uF"
      },
      "source": [
        "def sigmoid(z):\n",
        "    # Activation function used to map any real value between 0 and 1\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.array([-1.143,0.452, 0.029, -0.242 ])\n",
        "x =([1., 1., 40., 0.])\n",
        "z = np.dot (w,x)\n",
        "print('Probabilidade de compra = {:.4f}'.format(sigmoid(z)))"
      ],
      "metadata": {
        "id": "IfWTp2fu5Mn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 02\n",
        "\n",
        "Veremos a influ√™ncia de $\\omega_0$ e $\\omega_1$ na fun√ß√£o sigmoide."
      ],
      "metadata": {
        "id": "qFFJquNy9W1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random1=[]\n",
        "random2=[]\n",
        "random3=[]\n",
        "xlist = []\n",
        "w1=[10, 1,0.1]\n",
        "for i in range(100):\n",
        " x = np.random.uniform(-5,5)\n",
        " xlist.append(x)\n",
        " logreg1 = 1/(1+np.exp(-(w1[0]*x)))\n",
        " logreg2 = 1/(1+np.exp(-(w1[1]*x)))\n",
        " logreg3 = 1/(1+np.exp(-(w1[2]*x)))\n",
        " random1.append(logreg1)\n",
        " random2.append(logreg2)\n",
        " random3.append(logreg3)\n",
        "plt.scatter(xlist, random1, marker='*',s=40, c='skyblue',alpha=0.3,label=r'$\\omega_1 = %3.1f$'%(w1[0]))\n",
        "plt.scatter(xlist, random2, c='steelblue',alpha=0.3,label=r'$\\omega_1 = %3.1f$'%(w1[1]))\n",
        "plt.scatter(xlist, random3, c='navy',marker='d', alpha=0.3,label=r'$\\omega_1 = %3.1f$'%(w1[2]))\n",
        "plt.axhline(y=0.5, label='h(x)=0.5')\n",
        "plt.ylabel(r'$h(x)=\\frac{1}{1+e^{-\\omega_1 \\, x}}$', fontsize=16)\n",
        "plt.xlabel(r'$x$',fontsize=16)\n",
        "plt.legend(fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TveQLq798Etr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random1=[]\n",
        "random2=[]\n",
        "random3=[]\n",
        "xlist = []\n",
        "w0 = [-10, 0, 10]\n",
        "w1 = [5., 5., 5.]\n",
        "for i in range(100):\n",
        " x = np.random.uniform(-5,5)\n",
        " xlist.append(x)\n",
        " logreg1 = 1/(1+np.exp(-(w0[0]+w1[0]*x)))\n",
        " logreg2 = 1/(1+np.exp(-(w0[1]+w1[1]*x)))\n",
        " logreg3 = 1/(1+np.exp(-(w0[2]+w1[2]*x)))\n",
        " random1.append(logreg1)\n",
        " random2.append(logreg2)\n",
        " random3.append(logreg3)\n",
        "plt.scatter(xlist, random1, marker='*',s=40, c='skyblue',alpha=0.5,label=r'$\\omega_0 = %3.1f$'%(w0[0]))\n",
        "plt.scatter(xlist, random2, c='steelblue',alpha=0.3,label=r'$\\omega_0 = %3.1f$'%(w0[1]))\n",
        "plt.scatter(xlist, random3, c='navy',marker='d', alpha=0.3,label=r'$\\omega_0 = %3.1f$'%(w0[2]))\n",
        "plt.axhline(y=0.5, label='h(x)=0.5')\n",
        "plt.ylabel(r'$h(x)=\\frac{1}{1+e^{-\\omega_1 \\, x}}$', fontsize=16)\n",
        "plt.xlabel(r'$x$',fontsize=16)\n",
        "plt.legend(fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1C9L4k3P8hpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz0mUPMm4vKh"
      },
      "source": [
        "#Regress√£o Log√≠stica bin√°ria\n",
        "\n",
        "Dado um conjunto de dados de entrada  represento pela matriz $\\textbf{X}$ de dimens√£o $m\\times n$,\n",
        "$$\n",
        "\\mathbf{X} = \\begin{pmatrix}\n",
        "x_{11} & x_{12} & \\cdots & x_{1n} \\\\\n",
        "x_{21} & x_{22} & \\cdots & x_{2n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{m1} & x_{m2} & \\cdots & x_{mn}\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "\\mathbf{x}^{(1)} \\\\\n",
        "\\mathbf{x}^{(2)} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{x}^{(m)}\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_n \\\\\n",
        "x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_n \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x^{(m)}_1 & x^{(m)}_2 & \\cdots & x^{(m)}_n\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "$\\mathbf{y}$ o vetor de valores de dados observados\n",
        "$$\n",
        "\\mathbf{y} = \\begin{pmatrix}\n",
        "y^{(1)} \\\\\n",
        "y^{(2)} \\\\\n",
        "\\vdots \\\\\n",
        "y^{(m)}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "e $h_{\\omega}(\\mathbf x^{(i)})$ o modelo log√≠stico, $\\boldsymbol{\\omega}$ cont√©m os valores dos par√¢metros atuais.\n",
        "\n",
        "\n",
        "### Fun√ß√£o erro: Entropia Cruzada\n",
        "Em vez do erro quadr√°tico m√©dio, usamos a perda de entropia cruzada,\n",
        "\\begin{aligned}\n",
        "J(\\boldsymbol{\\omega}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{m} \\sum_i \\left[- y^{(i)} \\ln (h_{\\omega}(\\mathbf{x}^{(i)})) - \\left(1 - y^{(i)}\\right) \\ln \\left[1 - h_{\\omega}(\\mathbf{x}^{(i)}]\\right] \\right]\n",
        "\\end{aligned}\n",
        "\n",
        "Voc√™ pode observar que, como de costume, calculamos a perda m√©dia de cada ponto em nosso conjunto de dados. A express√£o interna no somat√≥rio acima representa a perda em um ponto de dados $(\\mathbf{x}^{(i)}, y^{(i)})$,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "L(\\boldsymbol{\\omega}, \\textbf{x}^{(i)}, y^{(i)}) = - y^{(i)} \\ln \\left[h_{\\omega}(\\textbf{x}^{(i)})\\right] - (1 - y^{(i)}) \\ln \\left[1 - h_{\\omega}(\\textbf{x}^{(i)}) \\right]\n",
        "\\end{aligned}\\tag{1}\n",
        "$$\n",
        "\n",
        "Dado que, na regress√£o log√≠stica cada $y^{(i)}$ assume os valores $0$ ou $1$ percebe-se que se $y^{(i)}=0$, o primeiro termo da equa√ß√£o (1) √© zero. Se $y^{(i)}=1$, o segundo termo da equa√ß√£o (1) √© zero. Assim, para cada ponto em nosso conjunto de dados, apenas um termo da perda de entropia cruzada contribui para a perda geral.\n",
        "\n",
        "Suponha $y^{(i)}=0$ e a previs√£o do modelo log√≠stico seja $h_{\\omega}(\\textbf{x}^{(i)}) = 0$ ‚Äî i√©, o modelo previu corretamente a resposta. A perda para este ponto ser√°:\n",
        "\\begin{split}\n",
        "\\begin{aligned}\n",
        "L(\\boldsymbol{\\omega}, \\textbf{x}^{(i)}, y^{(i)})\n",
        "&= - y^{(i)} \\ln \\left[h_{\\omega}(\\textbf{x}^{(i)})\\right] - (1 - y^{(i)}) \\ln \\left[1 - h_{\\omega}(\\textbf{x}^{(i)}) \\right] \\\\\n",
        "&= - 0 - (1 - 0) \\ln (1 - 0 ) \\\\\n",
        "&= - \\ln (1) \\\\\n",
        "&= 0\n",
        "\\end{aligned}\n",
        "\\end{split}\n",
        "\n",
        "Como esperado, a perda de uma previs√£o correta √© $0$. Pode-se verificar tamb√©m que quanto mais longe a probabilidade prevista estiver do valor verdadeiro, maior ser√° a perda.\n",
        "\n",
        "Minimizar a perda geral de entropia cruzada requer que o modelo $h_{\\omega}(\\textbf{x}^{(i)})$ fa√ßa as previs√µes mais precisas que puder. Convenientemente, essa fun√ß√£o de perda √© convexa, tornando a descida do gradiente uma escolha natural para otimiza√ß√£o.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dados os valores da sigmoide de `z`, definidos pela vari√°vel `sig`, onde $z = \\boldsymbol{\\omega}^T \\mathbf{x}$ varia entre $-5$ e $5$,  os gr√°ficos azul e laranja abaixo mostram, respectivamente, a primeira e segunda parcelas da equa√ß√£o (1)."
      ],
      "metadata": {
        "id": "jPmiRj9K057R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.arange(-5., 5., 0.2)\n",
        "plt.xticks(np.arange(0, 1.1, step=0.1))\n",
        "plt.yticks(np.arange(0, 0.7, step=0.5))\n",
        "sig = sigmoid(z)\n",
        "plt.plot(sig,-np.log(sig),label = r'$y_i=1$')\n",
        "plt.plot(sig,-np.log(1-sig), color = 'orange',label = r'$y_i=0$')\n",
        "plt.xlabel(r'$h_{\\omega}(x^{(i)})$')\n",
        "plt.ylabel(r'Fun√ß√£o Perda')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vp0xW7OM5usL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradiente da fun√ß√£o de perda por entropia cruzada\n",
        "\n",
        "Para executar o gradiente descendente na perda de entropia cruzada de um modelo, devemos calcular o gradiente da fun√ß√£o de perda. Primeiro, calculamos a derivada da fun√ß√£o sigm√≥ide, uma vez que a usaremos em nosso c√°lculo de gradiente.\n",
        "\n",
        "\\begin{split}\n",
        "\\begin{aligned}\n",
        "\\sigma(z) &= \\frac{1}{1 + e^{-z}} \\\\\n",
        "\\sigma'(z) &= \\frac{e^{-z}}{(1 + e^{-z})^2} \\\\\n",
        "\\sigma'(z) &= \\frac{1}{1 + e^{-z}} \\cdot \\left[1 - \\frac{1}{1 + e^{-z}} \\right] \\\\\n",
        "\\sigma'(z) &= \\sigma(z) \\left[1 - \\sigma(z)\\right]\n",
        "\\end{aligned}\n",
        "\\end{split}\n",
        "\n",
        "A derivada da fun√ß√£o sigm√≥ide pode ser convenientemente expressa em termos da pr√≥pria fun√ß√£o sigm√≥ide.\n",
        "\n",
        "Define-se $\\sigma^{(i)} = h_{\\omega}(\\textbf{x}^{(i)}) = \\sigma({\\textbf{x}^{(i)}}^T \\boldsymbol{\\omega})$. Portanto,\n",
        "\n",
        "\\begin{split}\n",
        "\\begin{aligned}\n",
        "\\nabla_{\\omega} \\sigma^{(i)}\n",
        "&= \\nabla_{\\omega} \\sigma(\\textbf{x}^{(i)} \\cdot \\boldsymbol{\\omega}) \\\\\n",
        "&= \\sigma(\\textbf{x}^{(i)} \\cdot \\boldsymbol{\\omega}) \\left[(1 - \\sigma(\\textbf{x}^{(i)} \\cdot \\boldsymbol{\\omega})\\right]  \\nabla_{\\omega} (\\textbf{x}^{(i)} \\cdot \\boldsymbol{\\omega}) \\\\\n",
        "&= \\sigma^{(i)} (1 - \\sigma^{(i)}) \\textbf{x}^{(i)}\n",
        "\\end{aligned}\n",
        "\\end{split}\n",
        "\n",
        "Agora, derivamos o gradiente da perda de entropia cruzada em rela√ß√£o aos par√¢metros do modelo $\\boldsymbol\\omega$.\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "\\begin{aligned}\n",
        "L(\\boldsymbol{\\omega}, \\textbf{X}, \\textbf{y})\n",
        "&= \\frac{1}{m} \\sum_i \\left[- y^{(i)} \\ln [h_{\\omega}(\\textbf{x}^{(i)})] - (1 - y^{(i)}) \\ln [1 - h_{\\omega}(\\textbf{x}^{(i)})] \\right] \\\\\n",
        "&= \\frac{1}{m} \\sum_i \\left[- y^{(i)} \\ln \\sigma^{(i)} - (1 - y^{(i)}) \\ln (1 - \\sigma^{(i)}) \\right] \\\\\n",
        "\\nabla_{\\omega} L(\\boldsymbol{\\omega}, \\textbf{X}, \\textbf{y})\n",
        "&= \\frac{1}{m} \\sum_i \\left[\n",
        "    - \\frac{y^{(i)}}{\\sigma^{(i)}} \\nabla_{\\omega} \\sigma^{(i)}\n",
        "    + \\frac{1 - y^{(i)}}{1 - \\sigma^{(i)}} \\nabla_{\\omega} \\sigma^{(i)} \\right] \\\\\n",
        "&= - \\frac{1}{m} \\sum_i \\left[\n",
        "    \\frac{y^{(i)}}{\\sigma^{(i)}} - \\frac{1 - y^{(i)}}{1 - \\sigma^{(i)}}\n",
        "\\right] \\nabla_{\\omega} \\sigma^{(i)} \\\\\n",
        "&= - \\frac{1}{m} \\sum_i \\left[\n",
        "    \\frac{y^{(i)}}{\\sigma^{(i)}} - \\frac{1 - y^{(i)}}{1 - \\sigma^{(i)}}\n",
        "\\right] \\sigma^{(i)} (1 - \\sigma^{(i)}) \\textbf{x}^{(i)} \\\\\n",
        "&= - \\frac{1}{m} \\sum_i \\left(\n",
        "    y^{(i)} - \\sigma^{(i)}\n",
        "\\right) \\textbf{x}^{(i)} \\\\\n",
        "\\end{aligned}\n",
        "\\end{split}\\tag{2}\n",
        "$$\n",
        "\n",
        "Uma express√£o surpreendentemente simples nos permite ajustar um modelo log√≠stico para a perda de entropia cruzada usando gradiente descendente:\n",
        "$$\n",
        "\\hat{\\boldsymbol{\\omega}} = \\displaystyle\\arg \\min_{\\substack{\\boldsymbol{\\omega}}}  L(\\boldsymbol{\\omega}, \\textbf{X}, \\textbf{y})\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "_7r0lMPb5r9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradiente descendente em lote\n",
        "\n",
        "A f√≥rmula geral de atualiza√ß√£o para a descida do gradiente √© dada por:\n",
        "$$\n",
        "\\boldsymbol{\\omega}^{(t+1)} = \\boldsymbol{\\omega}^{(t)} - \\alpha \\nabla_{\\omega} L(\\boldsymbol{\\omega}^{(t)}, \\textbf{X}, \\textbf{y})\\tag{3}\n",
        "$$\n",
        "onde $\\alpha$ √© o hiperpar√¢metro taxa de aprendizado.\n",
        "\n",
        "Ao inserir a eq. (2) √† f√≥rmula de atualiza√ß√£o (3), tem-se o algoritmo de gradiente descendente espec√≠fico para regress√£o log√≠stica,\n",
        "$$\n",
        "\\begin{split}\n",
        "\\begin{align}\n",
        "\\boldsymbol{\\omega}^{(t+1)} &= \\boldsymbol{\\omega}^{(t)} - \\alpha \\left[- \\frac{1}{m} \\sum\\limits_{i=1}^{m} \\left(y^{(i)} - \\sigma^{(i)}\\right) \\textbf{x}^{(i)} \\right] \\\\\n",
        "&= \\boldsymbol{\\omega}^{(t)} + \\alpha \\left[\\frac{1}{m} \\sum\\limits_{i=1}^{m} \\left(y^{(i)} - \\sigma^{(i)}\\right) \\textbf{x}^{(i)} \\right]\n",
        "\\end{align}\n",
        "\\end{split}\n",
        "$$"
      ],
      "metadata": {
        "id": "uY0AXusl74EK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eEYgu204vKq"
      },
      "source": [
        "### Exemplo: Ma√ß√£ ou cereja?\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1ehoGZZHsj35Gg4bd-yMTDGpC5j_ym86d' width=\"300\"></center>\n",
        "\n",
        "O exemplo refere-se √† distin√ß√£o entre pequenas ma√ß√£s e cerejas utilizando a medi√ß√£o do di√¢metro da fruta. Os tamanhos medidos foram:\n",
        "\\begin{equation}\n",
        "[1.8, 2.6, 3.2, 4.2, 4.4, 4.8, 5.2, 6.2 , 6.9, 8.6]\n",
        "\\end{equation}\n",
        "\n",
        "E a resposta (1 - ma√ß√£, 0 - cereja) √©,\n",
        "\\begin{equation}\n",
        "[0, 0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "__Aten√ß√£o__ Esta an√°lise foi feita para ilustrar a resposta e o significado da entropia cruzada. N√£o se preocupem em entender a bilbioteca, ela ser√° assunto da pr√≥xima aula."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ2n4DB8jERt"
      },
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "#Estabilidade\n",
        "x = np.array([1.8, 2.6, 3.2, 4.2, 4.4, 4.8, 5.2, 6.2 , 6.9, 8.6])\n",
        "y = np.array([0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
        "\n",
        "#Essa parte ir√£o aprender na pr√≥xima aula\n",
        "logr = LogisticRegression(solver='lbfgs')\n",
        "logr.fit(x.reshape(-1, 1), y)\n",
        "\n",
        "y_pred = logr.predict_proba(x.reshape(-1, 1))[:, 1].ravel()\n",
        "loss = log_loss(y, y_pred)\n",
        "\n",
        "print('x = {}'.format(x))\n",
        "print('y = {}'.format(y))\n",
        "print('p(y) = {}'.format(np.round(y_pred, 2)))\n",
        "print('Entropia cruzada = {:.4f}'.format(loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "210_qBtgjKZx"
      },
      "source": [
        "print('p = {}'.format(np.round(y_pred, 2)))\n",
        "gen = -(y*np.log(y_pred)+(1.0-y)*np.log(1-y_pred))\n",
        "print('log_prob = {}'.format(np.round(gen, 3)))\n",
        "print('Entropia cruzada = {:.4f}'.format(1/len(y)*np.sum(gen)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8p0YBiRgkb1"
      },
      "source": [
        "## M√©tricas\n",
        "\n",
        "A regra √©:\n",
        "__there is no \"one size fits all\" policy!__\n",
        "\n",
        "### Matriz de confus√£o\n",
        "\n",
        "*Matriz de confus√£o* √© uma medida de desempenho para o problema de classifica√ß√£o de aprendizado de m√°quina em que a sa√≠da pode ser duas ou mais classes. √â uma tabela com 4 combina√ß√µes diferentes de valores previstos e reais.\n",
        "\n",
        "\n",
        "|$\\downarrow$Prediction/Target $\\rightarrow$      | Positivo (1) |Negativo (0)|\n",
        "|:-----|:----|:------|\n",
        "|**Positivo (1)** |VP            |FP |\n",
        "|**Negativo (0)** |FN            |VN |\n",
        "\n",
        "Onde VP = verdadeiro positivo, FP = falso positivo, FN = falso negativo e VN = verdadeiro negativo. Veja que:\n",
        "* **Falsos negativos** e **falsos positivos** s√£o exemplos classificados **incorretamente**.\n",
        "* **Verdadeiros negativos** e **verdadeiros positivos** s√£o exemplos classificados **corretamente**.\n",
        "\n",
        "A partir dos erros e acertos mostrados na tabela pode-se definir o que √© *acur√°cia* (accuracy, em ingl√™s), *precis√£o* (*precision*, em ingl√™s) e *revoca√ß√£o* ou *sensibilidade* (*recall*, ou *sensitivity* em ingl√™s)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acur√°cia\n",
        "\n",
        "Acur√°cia indica uma performance geral do modelo. Dentre todas as classifica√ß√µes, quantas o modelo classificou corretamente. Sua f√≥rmula √©,\n",
        "$$\n",
        "A = \\frac{VP+VN}{VP+VN+FP+FN}\n",
        "$$\n",
        "\n",
        "A acur√°cia √© uma boa indica√ß√£o geral do desempenho do modelo. Por√©m, pode haver situa√ß√µes em que ela √© enganosa. A principal desvantagem da precis√£o √© que ela __mascara a quest√£o do desequil√≠brio de classes__. Por exemplo, se os dados contiverem apenas 10% de inst√¢ncias positivas, um classificador que sempre atribua o r√≥tulo negativo alcan√ßaria 90% de precis√£o, uma vez que preveria corretamente 90% das vezes.\n",
        "\n",
        "Em suma, embora a sua simplicidade seja apelativa, a raz√£o mais significativa pela qual a acur√°cia *n√£o √© uma boa medida para dados desequilibrados* √© que n√£o considera as nuances da classifica√ß√£o. Simplesmente fornece uma vis√£o limitada do verdadeiro desempenho do modelo.\n",
        "\n",
        "√â por isso que m√©tricas complementares ‚Äî como precis√£o e recall ‚Äî s√£o t√£o ben√©ficas no contexto do monitoramento de modelos."
      ],
      "metadata": {
        "id": "0Ogf7aWcMeBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Precis√£o\n",
        "\n",
        "Precis√£o √© a fra√ß√£o dos resultados, entre os positivos detectados, que de fato s√£o positivos.\n",
        "\n",
        "A precis√£o √© calculada de acordo com a seguinte equa√ß√£o:\n",
        "$$\n",
        "P = \\frac{VP}{VP+FP}\n",
        "$$\n",
        "\n",
        "Se um modelo classificou um total de 100 amostras como sendo de classe positiva, e 70 delas realmente pertenciam √† classe positiva do conjunto de dados (e 30 eram amostras de classe negativa previstas incorretamente como ‚Äúpositivas‚Äù pelo classificador), ent√£o a precis√£o √© de 70%. Se nosso modelo tem uma precis√£o de 0,7 significa que, quando prev√™ a classe 1, est√° correto em 70% do tempo."
      ],
      "metadata": {
        "id": "foH8sq6DMZLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revoca√ß√£o ou sensibilidade\n",
        "\n",
        "Revoca√ß√£o √© a fra√ß√£o de positivos detectados dentre todos os positivos.\n",
        "\n",
        "$$\n",
        "R = \\frac{VP}{VP+FN}\n",
        "$$\n",
        "\n",
        "Ou seja, se o conjunto de teste de um conjunto de dados consiste em 100 amostras em sua classe positiva, quantas delas foram identificadas? Se 60 das amostras positivas foram identificadas corretamente, ent√£o o recall √© de 60%.\n"
      ],
      "metadata": {
        "id": "MYRtwW1qMVf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precis√£o vs revoca√ß√£o\n",
        "\n",
        "Precis√£o √© a rela√ß√£o entre a fra√ß√£o de previstos como sendo $y = 1$, que de fato pertencem √† classe $y = 1$, em rela√ß√£o ao n√∫mero total de previstos com pertencendo √† classe $y = 1$.\n",
        "\n",
        "Revoca√ß√£o √© a rela√ß√£o entre o n√∫mero total de previstos como pertencendo √† classe $y = 1$, em rela√ß√£o ao n√∫mero total de elementos que realmente pertencem a essa classe.\n",
        "\n",
        "O desejado em um problema de classifica√ß√£o bin√°ria √© ter ambos precis√£o e revoca√ß√£o altas e iguais a 1, mas isso nem sempre √© poss√≠vel. Porque?\n",
        "\n",
        "**Porque existe um compromisso entre precis√£o e revoca√ß√£o.**\n",
        "\n",
        "Como visto, a sa√≠da de uma regress√£o log√≠stica em um problema de classifica√ß√£o bin√°ria √© uma probabilidade, i√©, um valor real entre 0 e 1 para cada caso analisado, que representa a probabilidade do caso pertencer a uma das classes.\n",
        "\n",
        "Tamb√©m como j√° vimos, dado $0<\\hat{y}<1$, devemos decidir em qual classe esse caso pertence, ou seja:\n",
        "* Casos s√£o previstos como sendo da classe $y = 1$, se $\\hat{y}\\ge limiar$;\n",
        "*Casos s√£o previstos como sendo da classe $y = 0$, se $\\hat{y} < limiar$\n",
        "\n",
        "Dependendo do valor do limar utilizado teremos resultados diferentes para a precis√£o e a revoca√ß√£o. Portanto, existe um compromisso entre precis√£o e revoca√ß√£o que depende do que queremos e em fun√ß√£o disso podemos escolher o valor do limiar.\n",
        "\n",
        "Por exemplo, se optamos por $limiar = 0.7$ teremos uma precis√£o maior, mas tamb√©m teremos uma revoca√ß√£o menor. Por outro lado, um $limiar = 0.3$ teremos maior seguran√ßa na previs√£o, ou seja, teremos uma revoca√ß√£o alta, mas\n",
        "uma precis√£o baixa.\n",
        "\n",
        "__Concluindo:__\n",
        "* Quanto maior o limiar, maior a precis√£o e menor a revoca√ß√£o;\n",
        "* Quanto menor o limiar, maior a revoca√ß√£o e menor a precis√£o."
      ],
      "metadata": {
        "id": "YjAWdh9RMvzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pontua√ß√£o $F1$ (*$F1$ score*)\n",
        "\n",
        "Uma m√©trica melhor, que combina a precis√£o com a revoca√ß√£o √© a pontua√ß√£o $F1$.  √â a m√©dia harm√¥nica entre precis√£o e revoca√ß√£o.\n",
        "\n",
        "A pontua√ß√£o $F1$ √© definida por:\n",
        "$$\n",
        "F1 = \\frac{2PR}{P+R}\n",
        "$$\n",
        "\n",
        "Observa-se que:\n",
        "* para a pontua√ß√£o $F1$ ser alta, tanto a precis√£o quanto a revoca√ß√£o devem ser altas;\n",
        "* $F1 =1$ somente se $P$ e $R$ forem ambos iguais a $1$.\n",
        "* se $P$ ou $R$ for igual a $0$, ent√£o, $F1$ √© igual a $0$.\n",
        "* pontua√ß√£o $F1$ √© uma forma de comparar precis√£o e revoca√ß√£o.\n",
        "\n",
        "Desse modo, a Pontua√ß√£o $F1$ √© a melhor m√©trica para problemas de classifica√ß√£o onde o n√∫mero de exemplos de uma classe √© desbalanceado.\n",
        "\n",
        "O Keras do TensorFlow n√£o possui a m√©trica pontua√ß√£o $F1$, mas ela pode ser facilmente calculada  tendo a precis√£o e a revoca√ß√£o."
      ],
      "metadata": {
        "id": "bgs5_JCjMzuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo: Ma√ß√£ ou cereja?\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1ehoGZZHsj35Gg4bd-yMTDGpC5j_ym86d' width=\"300\"></center>\n",
        "\n",
        "Supondo que a classidica√ß√£o real esteja definida em `y_Actual` e `y_Predicted` seja a previs√£o do modelo.\n",
        "\n",
        "Mude os valores e verifique como as m√©tricas mudam.\n"
      ],
      "metadata": {
        "id": "c_1N45Y4NcjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'y_Actual':    [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0],\n",
        "        'y_Predicted': [1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0]\n",
        "       }\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
        "df"
      ],
      "metadata": {
        "id": "b7P4JNdS-zPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LcTWHrBeeqy"
      },
      "source": [
        "confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Target'], colnames=['Predicted'])\n",
        "sns.heatmap(confusion_matrix, cmap=\"YlGnBu\" , annot=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCWoDe8-e0m7"
      },
      "source": [
        "print('Acur√°cia : {:0.3f}'.format(metrics.accuracy_score(df['y_Actual'], df['y_Predicted'])))\n",
        "P = metrics.precision_score(df['y_Actual'], df['y_Predicted'])\n",
        "print('Precis√£o : {:0.3f}'.format(P))\n",
        "R = metrics.recall_score(df['y_Actual'], df['y_Predicted'])\n",
        "print('Revoca√ß√£o: {:0.3f}'.format(R))\n",
        "F1 = 2*R*P/(R+P)\n",
        "print('F1       : {:0.3f}'.format(F1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Curva ROC\n",
        "\n",
        "A Curva Caracter√≠stica de Opera√ß√£o do Receptor (Curva COR), ou, do ingl√™s,  *Receiver Operating Characteristic (ROC)* √© uma curva de probabilidade.\n",
        "\n",
        "Ela √© criada tra√ßando a taxa de verdadeiros positivos (revoca√ß√£o) em  fun√ß√£o da taxa de falsos positivos para diferentes limites de classifica√ß√£o. Ou seja, n√∫mero de vezes que o classificador acertou a predi√ß√£o contra o n√∫mero de vezes que o classificador errou a predi√ß√£o.\n",
        "\n",
        "A taxa de falsos positivos √© dada por,\n",
        "$$\n",
        "FPR = \\frac{FP}{FP+VN}\n",
        "$$\n",
        "\n",
        "A taxa de falsos positivos (FPR) tamb√©m √© conhecida como probabilidade de alarme falso (fall-out or probability of false alarm) e pode ser calculada como o complementar da taxa de verdadeiros negativos (VNR), i√©, $(1 ‚Äî VNR)$. VNR tamb√©m √© conhecida como *especificidade* (*specificity* e ingl√™s).\n",
        "\n",
        "Para simplificar a curva ROC, foi criada a AUC (*Area Under the Curve*). A AUC resume a curva ROC num √∫nico valor, calculando a *√°rea sob a curva*.\n",
        "\n",
        "Quanto maior o AUC, melhor o modelo est√° em prever 0s como 0s e 1s como 1s. A pontua√ß√£o $AUC = 1$ representa o classificador perfeito e $AUC = 0.5$ representa um classificador sem valor.\n",
        "\n",
        "Um modelo excelente tem AUC pr√≥ximo de $1$, o que significa que tem uma boa medida de distin√ß√£o das classes. Um modelo pobre tem AUC pr√≥ximo de $0$, o que significa que tem a pior medida de separabilidade. Na verdade, significa que est√° retribuindo o resultado. Ele est√° prevendo 0s como 1s e 1s como 0s. E quando AUC √© 0.5, significa que o modelo n√£o tem valor nenhum, i√©, n√£o tem capacidade de separa√ß√£o de classes melhor que a aleatoriedade."
      ],
      "metadata": {
        "id": "6bARBDjc-Jeh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo: Ma√ß√£ ou cereja?\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1ehoGZZHsj35Gg4bd-yMTDGpC5j_ym86d' width=\"300\"></center>\n",
        "\n",
        "Usando o exemplo de ma√ß√£s e cerejas, vamos supor que a resposta de nosso modelo tenha sido conforme apresentado na figura abaixo.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1tnPz7PoyD412P7sV0QKAJ3AC-0RaeJTb' width=\"500\"></center>\n"
      ],
      "metadata": {
        "id": "dygC3MIVMRjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'y_Actual':    [0,    0,   1,   0,    0,  1,   1,  1],\n",
        "        'prob_Predicted': [0.01, 0.04, 0.05, 0.2, 0.9, 0.95, 0.97, 0.99 ]\n",
        "        }\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_Actual','prob_Predicted'])"
      ],
      "metadata": {
        "id": "lQjdazxEO4W2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTq5XRP1gqKV"
      },
      "source": [
        "fpr, tpr, thresholds = metrics.roc_curve(df['y_Actual'], df['prob_Predicted'])\n",
        "auc = metrics.roc_auc_score(df['y_Actual'], df['prob_Predicted'])\n",
        "\n",
        "plt.plot(fpr, tpr, label='Regress√£o log√≠stica (area = {:0.3f})'.format(auc))\n",
        "plt.plot([0, 1], [0, 1],'r--', label='FPR = R')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Taxa de Falsos Positivos')\n",
        "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('Log_ROC')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2Gr0ibijujt"
      },
      "source": [
        "A linha pontilhada vermelha representa a curva ROC de um classificador puramente aleat√≥rio; um bom classificador fica o mais longe poss√≠vel dessa linha (em dire√ß√£o ao canto superior esquerdo)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regress√£o log√≠stica multiclasse"
      ],
      "metadata": {
        "id": "PQKwze90qiDP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSvF4woo0InX"
      },
      "source": [
        "## Regress√£o log√≠stica multinomial\n",
        "\n",
        "A regress√£o Softmax (ou regress√£o log√≠stica multinomial) √© uma generaliza√ß√£o da regress√£o log√≠stica para o caso em que queremos lidar com v√°rias classes. Na regress√£o log√≠stica assumimos que os r√≥tulos eram bin√°rios: $y^{(i)} \\in \\{0,1\\}$. A regress√£o Softmax nos permite lidar com $y^{(i)} \\in \\{1,\\ldots,K\\}$ onde $K$ √© o n√∫mero de classes.\n",
        "\n",
        "Nesse caso, faz-se uma modifica√ß√£o da regress√£o log√≠stica usando a *fun√ß√£o softmax* em vez da *fun√ß√£o sigm√≥ide*, na fun√ß√£o de perda de entropia cruzada.\n",
        "\n",
        "### Softmax\n",
        "\n",
        "Dado uma entrada de teste $\\mathbf{x}$, queremos que nossa hip√≥tese estime a probabilidade de que $P(y=k | \\mathbf{x})$ para cada valor de $k = 1, \\cdots, K$. Ou seja, queremos estimar a probabilidade do r√≥tulo de classe assumir cada um dos $K$ diferentes valores poss√≠veis. Assim, nossa hip√≥tese produzir√° um vetor K-dimensional (cujos elementos somam 1) dando-nos nossas $K$ probabilidades estimadas. Concretamente, nossa hip√≥tese $h_{\\omega}(\\mathbf{x})$ assume a forma:\n",
        "\n",
        "\\begin{align}\n",
        "h_\\omega(\\mathbf{x}) =\n",
        "\\begin{bmatrix}\n",
        "P(y = 1 | \\mathbf{x}; \\boldsymbol\\omega) \\\\\n",
        "P(y = 2 | \\mathbf{x}; \\boldsymbol\\omega) \\\\\n",
        "\\vdots \\\\\n",
        "P(y = K | \\mathbf{x}; \\boldsymbol\\omega)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\boldsymbol\\omega^{(j)\\top} \\mathbf{x}) }}\n",
        "\\begin{bmatrix}\n",
        "\\exp(\\boldsymbol\\omega^{(1)\\top} \\mathbf{x} ) \\\\\n",
        "\\exp(\\boldsymbol\\omega^{(2)\\top} \\mathbf{x} ) \\\\\n",
        "\\vdots \\\\\n",
        "\\exp(\\boldsymbol\\omega^{(K)\\top} \\mathbf{x} ) \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Aqui $\\boldsymbol\\omega^{(1)}, \\boldsymbol\\omega^{(2)}, \\ldots, \\boldsymbol\\omega^{(K)} \\in \\Re^{n}$ s√£o os par√¢metros do nosso modelo. Observe que o termo $\\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\boldsymbol\\omega^{(j)\\top} \\mathbf{x}) } }$ normaliza a distribui√ß√£o, de modo que soma um.\n",
        "\n",
        "Por conveni√™ncia, tamb√©m escreveremos $\\boldsymbol\\omega$ para denotar todos os par√¢metros do nosso modelo. Quando voc√™ implementa a regress√£o softmax, geralmente √© conveniente representar $\\boldsymbol\\omega$ como uma matriz n por K obtida pela concatena√ß√£o de $\\boldsymbol\\omega^{(1)}, \\boldsymbol\\omega^{(2)}, \\ldots, \\boldsymbol\\omega^{(K)} $ em colunas, de modo que\n",
        "\n",
        "$$\n",
        "\\boldsymbol\\omega = \\left[\\begin{array}{cccc}| & | & | & | \\\\\n",
        "\\boldsymbol\\omega^{(1)} & \\boldsymbol\\omega^{(2)} & \\cdots & \\boldsymbol\\omega^{(K)}\\\\\n",
        "| & | & | & |\n",
        "\\end{array}\\right].\n",
        "$$\n",
        "\n",
        "### Fun√ß√£o de Custo\n",
        "\n",
        "A fun√ß√£o de custo utilizada para a regress√£o softmax √© dada por,\n",
        "\\begin{align}\n",
        "J(\\boldsymbol\\omega) = - \\left[ \\sum_{i=1}^{m} \\sum_{k=1}^{K}  1\\left\\{y^{(i)} = k\\right\\} \\log \\frac{\\exp(\\boldsymbol\\omega^{(k)\\top} \\mathbf{x}^{(i)})}{\\sum_{j=1}^K \\exp(\\boldsymbol\\omega^{(j)\\top} \\mathbf{x}^{(i)})}\\right]\n",
        "\\end{align}\n",
        "onde $1\\{\\cdot\\}$ √© a *fun√ß√£o indicadora*, de modo que $1\\{\\hbox{uma afirma√ß√£o verdadeira}\\}=1$ e  $1\\{\\hbox{uma afirma√ß√£o falsa}\\}=0$. Por exemplo, $1\\{\\hbox{2 + 2 = 4}\\}$ √© avaliado como 1; enquanto $1\\{\\hbox{1 + 1 = 5}\\}$ √© avaliado como 0. Nossa fun√ß√£o de custo ser√°:\n",
        "\n",
        "\\begin{align}\n",
        "J(\\boldsymbol\\omega) = - \\left[ \\sum_{i=1}^{m} \\sum_{k=1}^{K}  1\\left\\{y^{(i)} = k\\right\\} \\log \\frac{\\exp(\\boldsymbol\\omega^{(k)\\top} \\mathbf{x}^{(i)})}{\\sum_{j=1}^K \\exp(\\boldsymbol\\omega^{(j)\\top} \\mathbf{x}^{(i)})}\\right]\n",
        "\\end{align}\n",
        "\n",
        "O gradiente utilizado no treinamento de par√¢metros √© dado por,\n",
        "\\begin{align}\n",
        "\\nabla_{\\boldsymbol\\omega^{(k)}} J(\\boldsymbol\\omega) = - \\sum_{i=1}^{m}{ \\left[ \\mathbf{x}^{(i)} \\left( 1\\{ y^{(i)} = k\\}  - P(y^{(i)} = k | \\mathbf{x}^{(i)}; \\boldsymbol\\omega) \\right) \\right]  }\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regress√£o log√≠stica com classificadores bin√°rios m√∫ltiplos\n",
        "\n",
        "Existem dois m√©todos comuns para realizar a classifica√ß√£o multiclasse usando o algoritmo de regress√£o log√≠stica de classifica√ß√£o bin√°ria: um-vs-todos (one-vs-all) e um-vs-um (one-vs-one).\n",
        "\n",
        "Em *um-vs-todos*, treinamos $K$ classificadores bin√°rios separados para cada classe e executamos todos esses classificadores em qualquer novo exemplo $\\mathbf{x}^{(i)}$ que desejamos prever e selecionamos a classe com a pontua√ß√£o m√°xima,\n",
        "\\begin{equation}\n",
        "\\hat{y}=\\arg \\max_{k \\in {1,2,\\cdots,K}} h_{\\omega}^{(k)}\\left(\\boldsymbol{x}\\right)\n",
        "\\end{equation}\n",
        "onde $‚Ñé_ùúî^{(k)}$  √© um classificador bin√°rio projetado para reconhecer objetos da classe $k$ entre todos os objetos das outras classes.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1htbozVBe0_qfizJQdtIr_bk3qELxpNqB' width=\"800\"></center>\n",
        "\n",
        "\n",
        "Em *um-vs-um*, treinamos $\\begin{pmatrix}\n",
        "K \\\\ 2\\end{pmatrix} = \\frac{K (K-1)}{2}$ combina√ß√µes, i√©, uma para cada par poss√≠vel de classes, e escolhemos a classe com probabilidade m√°xima quando prevemos para um novo exemplo,\n",
        "\\begin{equation}\n",
        "\\hat{y}=\\arg \\max_{k \\in {1,2,\\cdots,K}} \\sum_{j=1}^K h_{\\omega}^{(kj)}\\left(\\boldsymbol{x}_{y=k,j}\\right)\n",
        "\\end{equation}\n",
        "onde $‚Ñé_ùúî^{(k)}$  √© um classificador bin√°rio projetado para reconhecer objetos da classe $k$ entre todos os objetos das outras classes.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1R8ZLwR-WvaGzv7bFbNNkXx0qJ6n8Zu1K' width=\"800\"></center>\n"
      ],
      "metadata": {
        "id": "UQIqXqZ0n7Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pros and Cons\n",
        "\n",
        "Em geral, essa escolha depende de como seus dados se relacionam com as classes. Se seus dados podem pertencer *exclusivamente* a uma classe, o classificador multinomial como softmax √© a escolha certa. O banco de dados de d√≠gitos MNIST √© um bom exemplo disso, pois um d√≠gito √© 0,1,2,.... ou 9. Um d√≠gito pode ser 0 e 8 ao mesmo tempo.\n",
        "\n",
        "Quando seus dados podem pertencer a mais de uma classe com diferentes graus de aproxima√ß√£o a cada classe, √© melhor treinar um classificador bin√°rio para cada classe. Por exemplo, se voc√™ estiver classificando a m√∫sica em g√™neros, uma m√∫sica pode ser principalmente *pop*, mas ter um pouco de *rock light*. Nesses casos, v√°rios classificadores bin√°rios funcionar√£o melhor.\n",
        "\n",
        "Agora, se voc√™ quiser usar v√°rios classificadores bin√°rios, voc√™ tem a op√ß√£o OvO e OvA. Nesse caso, OvO costuma funcionar um pouco melhor do que OvA, mas, obviamente, √© seriamente limitado pelo n√∫mero de classes que voc√™ tem... Se voc√™ tiver apenas algumas classes, ent√£o n√£o h√° problema em usar OvO, mas para muitas classes OvA √© a escolha certa."
      ],
      "metadata": {
        "id": "_hipv3Vdn1j-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpLJlqdf6nbV"
      },
      "source": [
        "## Exemplo\n",
        "\n",
        "Este exerc√≠cio √© apenas para ilustra√ß√£o. Voc√™s aprender√£o a usar as bibliotecas na aula do Prof. Thiago.\n",
        "\n",
        "O conjunto de dados de d√≠gitos $0-9$ j√° est√° incorporado √† biblioteca scikit-learn. Os dados de entrada `data` e de sa√≠da `target` podem ser carregados com a fun√ß√£o:\n",
        "\n",
        "`load_digits()`\n",
        "\n",
        "No problema de classifica√ß√£o multivari√°veis exemplificado aqui iremos treinar o modelo para distinguir entre dez classes distintas, i√©, n√∫meros de 0 a 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqXBObCp6nNW"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "# Print to show there are 1797 images (8 by 8 images for a dimensionality of 64)\n",
        "print('Forma dos dados de entrada', digits.data.shape)\n",
        "# Print to show there are 1797 labels (integers from 0‚Äì9)\n",
        "print('Forma dos dados de sa√≠da  ', digits.target.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p02loqr_7_Lc"
      },
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "for index, (image, label) in enumerate(zip(digits.data[0:5], digits.target[0:5])):\n",
        " plt.subplot(1, 5, index + 1)\n",
        " plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
        " plt.title('Treinamento: %i\\n' % label, fontsize = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKSEbxqj8N3h"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8clKEF9t8Svi"
      },
      "source": [
        "logreg2 = LogisticRegression(max_iter = 10000)\n",
        "logreg2.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJNGrLeO9Sfe"
      },
      "source": [
        "ex = 4\n",
        "print('Para a entrada {:2d} o primeira entrada do conjunto de teste:\\n'.format(ex))\n",
        "print('A previs√£o do modelo √©:',logreg2.predict(x_test[ex].reshape(1,-1))[0])\n",
        "print('O valor correto √©     :', y_test[ex])\n",
        "plt.figure(figsize=(20,4))\n",
        "plt.imshow(np.reshape(x_test[ex], (8,8)), cmap=plt.cm.gray)\n",
        "plt.title('Treinamento: %i\\n' % y_test[ex], fontsize = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUHTV-Nv_15A"
      },
      "source": [
        "predictions = logreg2.predict(x_test)\n",
        "confusion_matrix = pd.crosstab(y_test, predictions, rownames=['Target'], colnames=['Predicted'])\n",
        "sn.heatmap(confusion_matrix, cmap=\"YlGnBu\" , annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHK0IuKn_uKn"
      },
      "source": [
        "print('Acur√°cia : {:0.3f}'.format(metrics.accuracy_score(y_test, predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noqmVMx-9HUx"
      },
      "source": [
        "print(logreg2.predict(x_test[0:10]))\n",
        "print(y_test[0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8qTUoDvgCTu"
      },
      "source": [
        "### Vantagens e desvantagens da regress√£o log√≠stica\n",
        "\n",
        "Devido √† sua natureza eficiente e direta, n√£o requer alto poder de computa√ß√£o, f√°cil de implementar, facilmente interpret√°vel, amplamente utilizado por analistas de dados e cientistas. Al√©m disso, n√£o requer dimensionamento de recursos. A regress√£o log√≠stica fornece uma pontua√ß√£o de probabilidade para observa√ß√µes.\n",
        "\n",
        "Por√©m, a regress√£o log√≠stica n√£o √© capaz de lidar com um grande n√∫mero de caracter√≠sticas/vari√°veis categ√≥ricas. √â vulner√°vel a overfitting. Para resolver o problema n√£o lineares, a regress√£o log√≠stica requer uma transforma√ß√£o de recursos, conforme visto em regress√£o polinomial. A regress√£o log√≠stica n√£o ter√° um bom desempenho com vari√°veis independentes que n√£o est√£o correlacionadas com a vari√°vel de destino e s√£o muito semelhantes ou correlacionadas entre si."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression from Scratch\n",
        "\n",
        "Vamos nos basear no algoritmo apresentado no [link](https://developer.ibm.com/articles/implementing-logistic-regression-from-scratch-in-python/) para entender toda a l√≥gica de regress√£o log√≠stica."
      ],
      "metadata": {
        "id": "yWM9_icutjWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicialmente, iremos gerar um problema aleat√≥rio de classifica√ß√£o bin√°ria, com 4 caracter√≠sticas, e dividir os dados em teste e valida√ß√£o, usando a biblioteca `sklearn`. Particularmente usaremos as fun√ß√µes   `make_classification` e `train_test_split`."
      ],
      "metadata": {
        "id": "rrJ6yQSduGVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X,y = make_classification(n_samples = 200, n_features = 3,n_classes=2,n_redundant = 0)\n",
        "X_tr,X_te,y_tr,y_te = train_test_split(X,y,test_size=0.1)"
      ],
      "metadata": {
        "id": "JJJuI2CatxJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12, 12))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(X[:,0], X[:,1], X[:,2], c = y)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O9uYafBA24b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Padroniza√ß√£o__\n",
        "\n",
        "A padroniza√ß√£o √© o processo de redimensionar os dados em torno da m√©dia com um desvio padr√£o unit√°rio. Isso significa que estamos efetivamente tornando a m√©dia do atributo 0 com a distribui√ß√£o resultante tendo um desvio padr√£o igual a 1. √â uma boa pr√°tica padronizar os dados antes de aliment√°-los ao algoritmo.\n",
        "\n",
        "Matematicamente a padroniza√ß√£o √© dada por,\n",
        "$$\n",
        "z=\\frac{x-\\mu}{\\sigma}\n",
        "$$"
      ],
      "metadata": {
        "id": "I8BilWoZufKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(X_tr):\n",
        "    for i in range(np.shape(X_tr)[1]):\n",
        "        X_tr[:,i] = (X_tr[:,i] - np.mean(X_tr[:,i]))/np.std(X_tr[:,i])"
      ],
      "metadata": {
        "id": "uzANaeSiu8_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A fun√ß√£o F1-score √© definida abaixo."
      ],
      "metadata": {
        "id": "2ong-H-hv-Av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def F1_score(y,y_hat):\n",
        "    tp,tn,fp,fn = 0,0,0,0\n",
        "    for i in range(len(y)):\n",
        "        if y[i] == 1 and y_hat[i] == 1:\n",
        "            tp += 1\n",
        "        elif y[i] == 1 and y_hat[i] == 0:\n",
        "            fn += 1\n",
        "        elif y[i] == 0 and y_hat[i] == 1:\n",
        "            fp += 1\n",
        "        elif y[i] == 0 and y_hat[i] == 0:\n",
        "            tn += 1\n",
        "    precision = tp/(tp+fp)\n",
        "    recall = tp/(tp+fn)\n",
        "    f1_score = 2*precision*recall/(precision+recall)\n",
        "    return (f1_score,precision,recall)"
      ],
      "metadata": {
        "id": "L0h0dVxZwkx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classe `MyLogReg` definida abaixo calcula a regress√£o log√≠stica multinomial."
      ],
      "metadata": {
        "id": "mI6lRdp51CqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLogReg:\n",
        "  ####calcula a fun√ß√£o sigmoide\n",
        "    def sigmoid(self,z):\n",
        "        sig = 1/(1+np.exp(-z))\n",
        "        return sig\n",
        "\n",
        "  ###inicializa os pesos em zero e\n",
        "  ### acrescenta uma coluna nos dados de entrada X (vies)\n",
        "    def initialize(self,X):\n",
        "        weights = np.zeros((np.shape(X)[1]+1,1))\n",
        "        X = np.c_[np.ones((np.shape(X)[0],1)),X]#coloca uma primeira coluna de 1s em X\n",
        "        return weights,X\n",
        "  ### fun√ß√£od e treinamento\n",
        "    def fit(self,X,y,alpha=0.001,iter=400):\n",
        "        weights,X = self.initialize(X)\n",
        "    ## calcula o custo\n",
        "        def cost(theta):\n",
        "            z = np.dot(X,theta)\n",
        "            cost0 = y.T.dot(np.log(self.sigmoid(z)))\n",
        "            cost1 = (1-y).T.dot(np.log(1-self.sigmoid(z)))\n",
        "            cost = -((cost1 + cost0))/len(y)\n",
        "            return cost\n",
        "        cost_list = np.zeros(iter)\n",
        "    ## loop de itera√ß√µes atualizando os pesos\n",
        "        for i in range(iter):\n",
        "            weights = weights - alpha*np.dot(X.T,self.sigmoid(np.dot(X,weights))-np.reshape(y,(len(y),1)))\n",
        "            cost_list[i] = cost(weights)\n",
        "        self.weights = weights\n",
        "        return cost_list\n",
        "  ## depois de treinados os pesos, faz nova previs√£o\n",
        "    def predict(self,X):\n",
        "        z = np.dot(self.initialize(X)[1],self.weights)\n",
        "        lis = []\n",
        "        for i in self.sigmoid(z):\n",
        "            if i>0.5:\n",
        "                lis.append(1)\n",
        "            else:\n",
        "                lis.append(0)\n",
        "        return lis"
      ],
      "metadata": {
        "id": "Ac31vcNQw1IP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standardize(X_tr)\n",
        "standardize(X_te)\n",
        "obj1 = MyLogReg()\n",
        "model = obj1.fit(X_tr,y_tr)\n",
        "y_pred = obj1.predict(X_te)\n",
        "y_train = obj1.predict(X_tr)\n",
        "#Let's see the f1-score for training and testing data\n",
        "f1_score_tr = F1_score(y_tr,y_train)[0]\n",
        "f1_score_te = F1_score(y_te,y_pred)[0]\n",
        "print('F1 treino:',f1_score_tr)\n",
        "print('F1 teste :',f1_score_te)"
      ],
      "metadata": {
        "id": "JSdR2mgbxgmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos plotar a curva de aprendizado?"
      ],
      "metadata": {
        "id": "QHzt3O5sAbg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i_iter = np.arange(0,len(np.array(model)))\n",
        "plt.scatter(i_iter,np.array(model))\n",
        "plt.title('Curva de aprendizado')\n",
        "plt.xlabel('Iteracao')\n",
        "plt.ylabel(r'$J(\\omega)$')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nhzo4H2b_97K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conferindo com a resposta da fun√ß√£o `LogisticRegression`da biblioteca `sklearn`."
      ],
      "metadata": {
        "id": "oO9-X5sg4k7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "model_sk = LogisticRegression().fit(X_tr,y_tr)\n",
        "y_pred = model_sk.predict(X_te)\n",
        "print(f1_score(y_te,y_pred))"
      ],
      "metadata": {
        "id": "aHNFE_pEx1lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quest√µes para voc√™ pensar...\n",
        "1. Com base no c√≥digo anterior, modifique o valor limite (veja que atualmente est√° 0.5) e verifique as modifica√ß√µes na resposta, em termos de matriz de confus√£o.\n",
        "2. Mostre a curva ROC.\n",
        "3. Com base no c√≥digo anterior, tente implementar a op√ß√£o de multiclasses `OvR` e compare novamente com os resultados da biblioteca `sklearn`, conforme script abaixo:\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "# Dados\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
        "# Modelo\n",
        "model = LogisticRegression()\n",
        "# define the ovr strategy\n",
        "ovr = OneVsRestClassifier(model)\n",
        "# Fit\n",
        "ovr.fit(X, y)\n",
        "# Previs√£o do modelo\n",
        "y_pred = ovr.predict(X)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "5OaXR_TF49fy"
      }
    }
  ]
}