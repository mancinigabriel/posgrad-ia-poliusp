{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKM9mrWQHmep"
      },
      "source": [
        "# <font face=\"Verdana\" size=6 color='#6495ED'> IAD-004 AULA 01: REGRESS√ÉO\n",
        "<font face=\"Verdana\" size=3 color='#40E0D0'> Professores Larissa Driemeier e Thiago Martins\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1J3dF7v9apzpj27oOsrT8aEagtNIYwq7J' width=\"600\"></center>\n",
        "\n",
        "Este notebook introdut√≥rio √© sobre problemas de Regress√£o, baseado na primeira aula [IAD-004](https://alunoweb.net/moodle/pluginfile.php/141625/mod_resource/content/2/ML1_A01_Y2024.pdf), ano 2024."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt_qqh1_Aceu"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP8y854RBU0v"
      },
      "source": [
        "A regress√£o linear consiste em tentar explicar o comportamento de uma vari√°vel, dita *dependente*, a partir de uma ou mais vari√°veis, ditas *independentes*, com um modelo linear.\n",
        "\n",
        "Seja $\\mathbf y = \\left\\{y^{(1)}, y^{(2)}, \\ldots, y^{(m)}\\right\\}$ uma amostra do conjunto de vari√°veis dependentes e $\\mathbf X = \\left\\{\\mathbf x^{(1)}, \\mathbf x^{(2)}, \\ldots, \\mathbf x^{(m)}\\right\\}$ as correspondentes vari√°veis independentes da amostra.\n",
        "\n",
        "O modelo linear para o comportamento destas vari√°veis √© dado pela equa√ß√£o:\n",
        "\\begin{equation}\n",
        " y^{(i)} =  w_0 +  w_1 x_1^{(i)} + w_2 x_2^{(i)}+\\cdots + w_n x_n^{(i)} + \\epsilon^{(i)}\n",
        "\\end{equation}\n",
        "onde $ w_0, \\cdots, w_n$ s√£o *par√¢metros* do modelo e $\\epsilon^{(i)$ √© o erro.\n",
        "\n",
        "Na forma matricial tem-se,\n",
        "\\begin{equation}\n",
        "\\mathbf y = \\begin{bmatrix}y_1\\\\\n",
        "y_2\\\\\n",
        "\\vdots\\\\\n",
        "y_m\\end{bmatrix}\n",
        "\\end{equation}\n",
        "um vetor de dimens√£o $m$, onde $m$ √© o n√∫mero de amostras do conjunto de dados, e\n",
        "\\begin{equation}\n",
        "\\mathbf X = \\begin{bmatrix}x_{1,1} & x_{1,2} & \\ldots & x_{1,n} & 1\\\\\n",
        "x_{2,1} & x_{2,2} & \\ldots & x_{2,n} & 1\\\\\n",
        " \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
        "x_{m,1} & x_{m,2} & \\ldots & x_{m,n} & 1\\end{bmatrix}=\\begin{bmatrix}\\mathbf x^{(1)}\\\\\n",
        "\\mathbf x^{(2)}\\\\\n",
        " \\vdots \\\\\n",
        "\\mathbf x^{(m)}\\end{bmatrix}\n",
        "\\end{equation}\n",
        "uma matriz $m \\times (n+1)$ cujas primeiras $n$ colunas s√£o compostas por uma amostra de cada uma das $n$ vari√°veis independetes e a sua *√∫ltima* coluna √© composta da constante 1.\n",
        "Define-se tamb√©m,\n",
        "\\begin{equation}\n",
        "\\mathbf w = \\begin{bmatrix}w_0\\\\\n",
        "w_1\\\\\n",
        "w_2\\\\\n",
        "\\vdots\\\\\n",
        "w_n\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "√© o vetor de $(n+1)$ componentes dos coeficientes de cada uma das vari√°veis independentes. Note que nesta nota√ß√£o o coeficiente $w_0$ √© o *primeiro* coeficiente (h√° nota√ß√µes distintas nas quais ele √© o √∫ltimo).\n",
        "\n",
        "O modelo linear para o comportamento destas vari√°veis √© dado pela equa√ß√£o:\n",
        "\\begin{equation}\n",
        "\\mathbf y =  \\mathbf X \\mathbf w + \\mathbf e\n",
        "\\end{equation}\n",
        "onde o erro rand√¥mico $\\mathbf e$ do conjunto de dados √© a diferen√ßa entre os valores observados e os valores verdadeiros, n√£o observ√°veis.\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=1mRM2uGuHlB46FuiRz6iaeV9O71P9IbIv)\n",
        "\n",
        "Importante comentar que o erro rand√¥mico n√£o √© observ√°vel, pois sua defini√ß√£o depende do conhecimento de $ \\mathbf X \\mathbf w$. Como n√£o conhecemos o erro, fazemos apenas suposi√ß√µes a seu respeito.\n",
        "\n",
        "No problema de regress√£o, estimamos os par√¢metros reais $\\mathbf w$, de forma que :\n",
        "\\begin{equation}\n",
        "\\hat{\\mathbf y} =  \\mathbf X \\hat{\\mathbf w} +  \\newcommand{\\beps}{\\boldsymbol \\epsilon} \\beps\n",
        "\\end{equation}\n",
        "onde $\\hat{\\mathbf y}$ √© uma aproxima√ß√£o das observa√ß√µes $\\mathbf y$. Define-se ainda o res√≠duo $\\epsilon^{(i)}$ como a diferen√ßa entre o valor observado $ y^{(i)}$ e estimado $\\hat y^{(i)}$,\n",
        "\\begin{equation}\n",
        "\\epsilon^{(i)} = y^{(i)}-\\hat y^{(i)} = y^{(i)}-\\left( \\hat w_0 + \\hat w_1 x_1^{(i)} + w_2 x_2^{(i)}+\\cdots +\\hat w_n x_n^{(i)} \\right)\n",
        "\\end{equation}\n",
        "de forma que $\\beps$ √© o vetor de *res√≠duos* de dimens√£o $n+1$. Os res√≠duos podem ser considerados somente estimativas dos erros. No entanto, s√≥ temos acesso aos res√≠duos, ent√£o √© com isso que trabalhamos.\n",
        "\n",
        "Daqui para frente, por brevidade, e cientes de que sempre estamos buscando um conjunto de par√¢metros que  aproxime a fun√ß√£o real, $\\hat w_i = w_i$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oht4Dlpb5zri"
      },
      "source": [
        "## As suposi√ß√µes de Gauss-Markov\n",
        "\n",
        "Ao usar estimadores n√£o viesados para os modelos de regress√£o, i√©, $ùê∏\\left(\\epsilon^{(ùëñ)}\\right)=0$, garantimos que pelo menos em m√©dia, estimamos o par√¢metro verdadeiro.\n",
        "\n",
        "Ao comparar diferentes estimadores n√£o viesados, √©, ainda, interessante saber qual deles tem a maior precis√£o poss√≠vel.\n",
        "\n",
        "O teorema de Gauss Markov nos diz que se um certo conjunto de suposi√ß√µes for atendido, a estimativa de m√≠nimos quadrados ordin√°rios para coeficientes de regress√£o fornece a mais baixa vari√¢ncia de amostragem dentro da classe dos estimadores lineares n√£o enviesados (BLUE, do ingl√™s Best Linear Unbiased Estimate) poss√≠vel.\n",
        "\n",
        "1. **Linearidade:** $\\newcommand{\\my}{\\mathbf y}\\my=\\newcommand{mX}{\\mathbf{Xw}}\\mX \\newcommand{\\mw}{\\mathbf w} + \\beps$, os par√¢metros que estimamos usando o m√©todo OLS devem ser lineares.\n",
        "2. **Aleatoriedade:** nossos dados devem ter sido amostrados aleatoriamente na popula√ß√£o, por um mecanismo n√£o relacionado a $\\beps$.\n",
        "5. **Exogeneidade:** como dito no item anterior, os regressores $x^{(i)}$ n√£o s√£o correlacionados com o termo de res√≠duo $cov\\left(x^{(i)},\\epsilon^{(i)}\\right)=0, i\\ne j$.\n",
        "3. **Res√≠duos com m√©dia nula:** Essa suposi√ß√£o afirma que a *m√©dia dos res√≠duos* √© $0$ para qualquer valor de $\\mX$, i√©, $ùê∏(\\epsilon^{(i)}|\\mX)=0$. Colocado de outra forma, nenhuma observa√ß√£o das vari√°veis independentes fornece qualquer informa√ß√£o sobre o valor esperado do res√≠duo. A suposi√ß√£o implica que $E(\\my) = \\mX$. Isso √© importante, pois essencialmente diz que acertamos a fun√ß√£o m√©dia.\n",
        "4. **Res√≠duos com covari√¢ncia nula:** Cada termo de res√≠duo √© independentemente distribu√≠do e n√£o correlacionado $cov\\left(\\epsilon^{(ùëñ)},\\epsilon^{(ùëó)}|\\mX\\right)=0, i\\ne j$. A suposi√ß√£o de nenhuma autocorrela√ß√£o quer dizer que: saber algo sobre o res√≠duo para uma observa√ß√£o n√£o nos diz nada sobre o res√≠duo para qualquer outra observa√ß√£o.\n",
        "6. **Homocedasticidade:** a vari√¢ncia de $\\epsilon^{(i)}$ √© constante para qualquer $i$, i√©, $var\\left(\\epsilon^{(ùëñ)}|\\mX\\right)=\\sigma_{\\epsilon}^2,\\forall i$.\n",
        "\n",
        "Ainda, para garantirmos a m√°xima verossimilhan√ßa, assume-se que o res√≠duo tem distribui√ß√£o normal com m√©dia nula e vari√¢ncia $\\sigma_{\\epsilon}^2$,\n",
        "$$\n",
        "\\beps \\approx N\\left[ 0,\\sigma_{\\epsilon}^2 \\mathbf I \\right]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Mais matem√°tica sobre o Teorema de Gauss Markov pode ser encontrada no [link](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4L5gvnax19s"
      },
      "source": [
        "\n",
        "\n",
        "## 1. Regress√£o Linear Simples\n",
        "\n",
        "A regress√£o linear simples trata de apenas uma vari√°vel independente.\n",
        "\n",
        "Seja $\\mathbf y = \\left\\{y^{(1)}, y^{(2)}, \\ldots, y^{(m)}\\right\\}$ uma amostra do conjunto de vari√°veis independentes e $\\mathbf x = \\left\\{x^{(1)}, x^{(2)}, \\ldots, x^{(m)}\\right\\}$\n",
        "\n",
        "O modelo linear simples para o comportamento destas vari√°veis √© dado pela equa√ß√£o:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat y^{(i)} = w_0 + w_1 x^{(i)} + \\epsilon^{(i)}\n",
        "\\end{equation}\n",
        "\n",
        "A hip√≥tese do modelo linear √© a de que os res√≠duos s√£o vari√°veis aleat√≥rias *independentes* distribu√≠das de acordo com uma distribui√ß√£o *Gaussiana* de valor esperado *nulo*,\n",
        "$$\n",
        "\\epsilon^{(i)} \\approx N\\left[ 0,\\sigma_{\\epsilon}^2 \\right]\n",
        "$$\n",
        "\n",
        "Isso significa essencialmente que nossos dados t√™m uma rela√ß√£o linear que √© corrompida pelo ru√≠do gaussiano aleat√≥rio que tem m√©dia zero e varia√ß√£o constante.\n",
        "\n",
        "Isso tem a implica√ß√£o de que $y^{(i)}$  √© uma vari√°vel aleat√≥ria gaussiana e podemos calcular sua expectativa e varia√ß√£o:\n",
        "$$\n",
        "E[y^{(i)}] = E[\\mathbf x^{(i)T} \\mathbf w + \\epsilon^{(i)}] = \\mathbf x^{(i)T} \\mathbf w\n",
        "$$\n",
        "\n",
        "$$\n",
        "Var[y^{(i)}] = Var[\\mathbf x^{(i)T} \\mathbf w + \\epsilon^{(i)}] = \\sigma^2\n",
        "$$\n",
        "onde $\\mathbf x^{(i)}=\\left[x^{(i)} \\quad 1\\right]^T$ e $\\mathbf w=\\left[w_1 \\quad w_0\\right]^T$.\n",
        "\n",
        "Isso equivale a supor que as vari√°veis independentes s√£o resultado da reta $y = w_0 + w_1 x $ sobreposta a um ru√≠do Gaussiano.\n",
        "\n",
        "Adicionando-se a hip√≥tese de que os ru√≠dos Gaussianos s√£o todos com a mesma covari√¢ncia, os par√¢metros da reta de *m√°xima verissimilhan√ßa* s√£o dados pela minimiza√ß√£o da fun√ß√£o custo dada pelo somat√≥rio quadr√°tico dos res√≠duos, i√©:\n",
        "\n",
        "\\begin{equation}\n",
        "\\underset{w_0, w_1}{\\mbox{arg min}} \\sum_i \\left[\\epsilon^{(i)}\\right]^2= \\sum_i \\left(y^{(i)} - w_0 - w_1 x^{(i)}\\right)^2\n",
        "\\end{equation}\n",
        "\n",
        "Ent√£o:\n",
        "\\begin{align}\n",
        "\\frac{\\partial EQT}{\\partial w_0} &= 2  \\sum_i \\left(y^{(i)} - w_0 - w_1 x^{(i)}\\right)(-1)=0\\\\\n",
        "\\frac{\\partial EQT}{\\partial w_1} &= 2  \\sum_i \\left(y^{(i)} - w_0 - w_1 x^{(i)}\\right)(-x^{(i)})=0\n",
        "\\end{align}\n",
        "\n",
        "Portanto, ap√≥s longa, e trivial, manipula√ß√£o alg√©brica,\n",
        "\\begin{align}\n",
        "w_1 &= \\frac{s_{xy}}{s_{xx}}\\\\\n",
        "w_0 &= \\bar{y} - w_1 \\bar{x}\n",
        "\\end{align}\n",
        "definindo-se,\n",
        "\\begin{align}\n",
        "\\bar{x} &= \\frac{1}{n}\\sum_i x^{(i)} \\\\\n",
        "\\bar{y} &= \\frac{1}{n}\\sum_i y^{(i)} \\\\\n",
        "s_{xx} &= \\sum_i (x^{(i)} - \\bar{x})^2 \\\\\n",
        "s_{yy} &= \\sum_i (y^{(i)} - \\bar{y})^2 \\\\\n",
        "s_{xy} &= \\sum_i (x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y})\n",
        "\\end{align}\n",
        "\n",
        "E finalmente, a soma total dos quadrados dos res√≠duos √© dada por:\n",
        "\n",
        "\\begin{equation}\n",
        "R^2 = \\sum_i \\left[\\epsilon^{(i)}\\right]^2=s_{yy}\\left(1-\\frac{s_{xy}^2}{s_{xx} s_{yy}}\\right)\n",
        "\\end{equation}\n",
        "\n",
        "O valor\n",
        "\n",
        "\\begin{equation}\n",
        "r = \\frac{s_{xy}}{\\sqrt{s_{xx} s_{yy}}}\n",
        "\\end{equation}\n",
        "\n",
        "√© chamado de *coeficiente de correla√ß√£o de Pearson*. Este √© um valor que varia de $-1$ a $1$ e mede o qu√£o bem a vari√°vel dependente pode ser explicada por um modelo linear da vari√°vel dependente.\n",
        "\n",
        "Valores mais pr√≥ximos de zero significam um modelo linear menos explicativo.\n",
        "\n",
        "Valores mais pr√≥ximos de $1$ ou $-1$ significam um modelo linear mais explicativo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 01\n",
        "\n",
        "Dados de idade e press√£o,\n",
        "```\n",
        "idade = x = [52,59,67,73,64,74,54,61,65,46,72]\n",
        "pressao = y = [132,143,153,162,154,168,137,149,159,128,166]\n",
        "```\n",
        "\n",
        "Determine:\n",
        "1. Os valores de $\\bar{x}$, $\\bar{y}$;\n",
        "2. Os valores de $s_{xx}, s_{yy}, s_{xy}$;\n",
        "3. Os valores de  $w_0$ e $w_1$ no modelo $\\hat y^{(i)} = w_0 + w_1 x^{(i)} + \\epsilon^{(i)}$ de m√°xima verissimilhan√ßa;\n",
        "4. Repita a plotagem *scatter* do enunciado sobreposta √† reta $\\hat y=w_0 + w_1 x$;\n",
        "5. Calcule o res√≠duo quadr√°tico total $\\sum_i \\left[\\epsilon^{(i)}\\right]^2$;\n",
        "6. Calcule o coeficiente de correla√ß√£o de Pearson e compare o valor $s_{yy}(1-r^2)$ com o obtido no item anterior."
      ],
      "metadata": {
        "id": "_t6Ql3iLZ-Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idade = [52,59,67,73,64,74,54,61,65,46,72]\n",
        "pressao = [132,143,153,162,154,168,137,149,159,128,166]"
      ],
      "metadata": {
        "id": "LIw03lm3YMBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_bar = np.mean(idade)\n",
        "y_bar = np.mean(pressao)\n",
        "print('A m√©dia de x √©: {:6.2f}'.format(x_bar))\n",
        "print('A m√©dia de y √©: {:6.2f}'.format(y_bar))\n"
      ],
      "metadata": {
        "id": "HHpBJsC0aGOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dx = (idade - x_bar)\n",
        "dy = (pressao - y_bar)\n",
        "sxx = np.sum(dx**2)\n",
        "syy = np.sum(dy**2)\n",
        "sxy = np.sum(dx*dy)\n",
        "print('sxx: '+str(sxx))\n",
        "print('syy: '+str(syy))\n",
        "print('sxy: '+str(sxy))"
      ],
      "metadata": {
        "id": "2NN8Ge71aWBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1_teo=sxy/sxx\n",
        "w0_teo= y_bar-x_bar*w1_teo\n",
        "print('w0: '+str(w0_teo),'w1: '+str(w1_teo))"
      ],
      "metadata": {
        "id": "241axWDzbSBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=idade, y=pressao,alpha=0.75)\n",
        "plt.plot(idade,w0_teo+np.asarray(idade)*w1_teo,color='crimson')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X7i2lRvMb9dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w=[]\n",
        "w0,w1= 0.0,0.5\n",
        "w.append([w0,w1])\n",
        "\n",
        "alpha = 0.0005\n",
        "eps = 1e-4\n",
        "max_iter = 500000\n",
        "\n",
        "m = len(idade)\n",
        "grad_w0 = 0.0\n",
        "grad_w1 = 0.0\n",
        "for i,j in zip(idade,pressao):\n",
        "    grad_w0 += (w0 + w1*i - j)/m\n",
        "    grad_w1 += (w0 + w1*i - j)*i/m\n",
        "grad = [grad_w0,grad_w1]\n",
        "\n",
        "count = 0\n",
        "while np.linalg.norm(grad)>eps and count<max_iter:\n",
        "  grad_w0 = 0.0\n",
        "  grad_w1 = 0.0\n",
        "  for i,j in zip(idade,pressao):\n",
        "    grad_w0 += (w0 + w1*i - j)/m\n",
        "    grad_w1 += (w0 + w1*i - j)*i/m\n",
        "  w0 = w0-alpha*grad_w0\n",
        "  w1 = w1-alpha*grad_w1\n",
        "  w.append([w0,w1])\n",
        "  grad = [grad_w0,grad_w1]\n",
        "  count += 1\n",
        "\n",
        "print('w0: '+str(w0))\n",
        "print('w1: '+str(w1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCdydRLWllSZ",
        "outputId": "0d6b41cc-e1a8-4a7d-fe1c-4d1d1dd31e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w0: 58.19648423796317\n",
            "w1: 1.4712261243031135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=idade, y=pressao,alpha=0.75)\n",
        "\n",
        "plt.plot(idade,w0+np.asarray(idade)*w1,color='crimson', label = \"gradiente\")\n",
        "plt.plot(idade,w0_teo+np.asarray(idade)*w1_teo,color='mediumvioletred', label = \"teoria\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qXu4U9T7rCFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps = syy*(1.0-sxy**2/(sxx*syy))\n",
        "print('eps: {:2.4f}'.format(eps))\n",
        "r= sxy/(np.sqrt(sxx*syy))\n",
        "print('r: {:2.4f}  syy(1-r2):  {:6.4f}  '.format(r,(syy*(1-r**2))))"
      ],
      "metadata": {
        "id": "08u8eyM3Eds9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defini√ß√£o de $R^2$\n",
        "\n",
        "$ùëÖ^2$  (R-quadrado) √© uma medida estat√≠stica de qu√£o pr√≥ximos os dados est√£o da linha de regress√£o ajustada. Tamb√©m √© conhecido como coeficiente de determina√ß√£o.\n",
        "\n",
        "Pode-se calcular a partir de sua defini√ß√£o:\n",
        "$$\n",
        "\\begin{align} R^2&=1-\\frac{\\text{SSR}}{\\text{SST}},\\\\ &=1-\\frac{\\sum({y_i}-\\hat{y_i})^2}{\\sum(y_i-\\bar{y})^2} = \\frac{s_{xy}^2}{s_{xx}s_{yy}} \\end{align}\n",
        "$$\n",
        " onde SSR (sum squared regression) √© a soma do quadrado dos erros da regress√£o, enquanto SST (total sum of squares)  √© a soma do quadrado dos erros, quando a regress√£o coincide com a linha m√©dia ${\\bar y}$"
      ],
      "metadata": {
        "id": "HQelXiw0RWx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "coef = r2_score(np.array(pressao), w0+w1*np.array(idade))\n",
        "print(coef)"
      ],
      "metadata": {
        "id": "riC9bT9LQjSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSE = (np.array(pressao)-w0-w1*np.array(idade))**2\n",
        "SSy = np.sum((np.array(pressao)-np.mean(pressao))**2)\n",
        "print(1-np.sum(SSE)/SSy)"
      ],
      "metadata": {
        "id": "p9bgF7XENDpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sxy*sxy/(sxx*syy))"
      ],
      "metadata": {
        "id": "5s6tRKorOhmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exerc√≠cio 02\n",
        "\n",
        "Os dados abaixo referem-se aos anos de experi√™ncia e sal√°rio de funcion√°rios de uma empresa.\n",
        "\n",
        "Determine a curva de regress√£o e analise a correla√ß√£o entre a curva te√≥rica e os dados disponibilizados."
      ],
      "metadata": {
        "id": "RIcYJKAxF6j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array([\n",
        "[1.1,39343.00],\n",
        "[1.3,46205.00],\n",
        "[1.5,37731.00],\n",
        "[2.0,43525.00],\n",
        "[2.2,39891.00],\n",
        "[2.9,56642.00],\n",
        "[3.0,60150.00],\n",
        "[3.2,54445.00],\n",
        "[3.2,64445.00],\n",
        "[3.7,57189.00],\n",
        "[3.9,63218.00],\n",
        "[4.0,55794.00],\n",
        "[4.0,56957.00],\n",
        "[4.1,57081.00],\n",
        "[4.5,61111.00],\n",
        "[4.9,67938.00],\n",
        "[5.1,66029.00],\n",
        "[5.3,83088.00],\n",
        "[5.9,81363.00],\n",
        "[6.0,93940.00],\n",
        "[6.8,91738.00],\n",
        "[7.1,98273.00],\n",
        "[7.9,101302.00],\n",
        "[8.2,113812.00],\n",
        "[8.7,109431.00],\n",
        "[9.0,105582.00],\n",
        "[9.5,116969.00],\n",
        "[9.6,112635.00],\n",
        "[10.3,122391.00],[10.5,121872.00]])"
      ],
      "metadata": {
        "id": "0irA25ww5Ys1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w=[]\n",
        "w0,w1= 0.0,0.5\n",
        "w.append([w0,w1])\n",
        "\n",
        "alpha = 0.0005\n",
        "eps = 1e-4\n",
        "max_iter = 500000\n",
        "\n",
        "\n",
        "m = len(idade)\n",
        "grad_w0 = 0.0\n",
        "grad_w1 = 0.0\n",
        "for i,j in zip(idade,pressao):\n",
        "    grad_w0 += (w0 + w1*i - j)/m\n",
        "    grad_w1 += (w0 + w1*i - j)*i/m\n",
        "grad = [grad_w0,grad_w1]\n",
        "\n",
        "x=data[:,0]\n",
        "y=data[:,1]\n",
        "print(x,y)\n",
        "\n",
        "count = 0\n",
        "while np.linalg.norm(grad)>eps and count<max_iter:\n",
        "  grad_w0 = 0.0\n",
        "  grad_w1 = 0.0\n",
        "  for i,j in zip(x,y):\n",
        "    grad_w0 += (w0 + w1*i - j)/m\n",
        "    grad_w1 += (w0 + w1*i - j)*i/m\n",
        "  w0 = w0-alpha*grad_w0\n",
        "  w1 = w1-alpha*grad_w1\n",
        "  w.append([w0,w1])\n",
        "  grad = [grad_w0,grad_w1]\n",
        "  count += 1\n",
        "\n",
        "print('w0: '+str(w0))\n",
        "print('w1: '+str(w1))"
      ],
      "metadata": {
        "id": "U9rWIJMV7Oki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=data[:,0], y=data[:,1],alpha=0.75, label = \"Dados\")\n",
        "media = [np.mean(data[:,1])]*len(data)\n",
        "plt.plot(data[:,0], media, color='crimson', label = \"M√©dia\")\n",
        "plt.plot(data[:,0],w0+np.asarray(data[:,0])*w1,color='forestgreen', label = \"Regress√£o\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BKbViORs6Piw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_regr = w0+np.asarray(data[:,0])*w1\n",
        "\n",
        "x_bar = np.mean(data[:,0])\n",
        "y_bar = np.mean(data[:,1])\n",
        "\n",
        "dx = (data[:,0] - x_bar)\n",
        "dy = (data[:,1] - y_bar)\n",
        "dy_regr = (data[:,1] - data_regr)\n",
        "\n",
        "\n",
        "sxx = np.sum(dx**2)\n",
        "syy = np.sum(dy**2)\n",
        "sxy = np.sum(dx*dy)\n",
        "s_regr = np.sum(dy_regr**2)\n",
        "r2= (sxy/(np.sqrt(sxx*syy)))**2\n",
        "r2_regr = (syy - s_regr) / syy\n",
        "\n",
        "\n",
        "print('R2 √©: {:6.2f}'.format(r2))\n",
        "print('R2 √©: {:6.2f}'.format(r2_regr))"
      ],
      "metadata": {
        "id": "PUOr8a8rIPeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1Gv9pKVM9kT"
      },
      "source": [
        "## 2. Regress√£o multilinear\n",
        "\n",
        "A regress√£o multi-linear trata da rela√ß√£o de uma vari√°vel dependente com *m√∫ltiplas* vari√°veis independentes.\n",
        "\n",
        "\\begin{equation}\n",
        "y^{(i)} = w_1 x_1^{(i)} + w_2 x_2^{(i)} + \\dots + w_n x_n^{(i)} +w_0+\\epsilon^{(i)}\n",
        "\\end{equation}\n",
        "\n",
        "A fun√ß√£o perda ou custo √© definida pelo somat√≥rio da fun√ß√£o erro de cada amostra,\n",
        "$$\n",
        "J(\\mathbf{X,w})=\\frac{1}{m} \\sum_{i=1}^n \\textbf{L}\\left(y^{(i)}, h_\\mathbf w(\\mathbf x^{(i)})\\right)\n",
        "$$\n",
        "\n",
        "Utilizando-se a fun√ß√£o erro quadr√°tica tem-se que,\n",
        "$$\n",
        "\\textbf{L}\\left(y^{(i)}, h_\\mathbf w(\\mathbf x^{(i)})\\right) = \\| \\mathbf{y} - \\mathbf{X w} \\|^2_2\n",
        "$$\n",
        "\n",
        "Seguindo a mesma hip√≥tese de que os res√≠duos s√£o vari√°veis aleat√≥rias *independentes* distribu√≠das de acordo com uma distribui√ß√£o *Gaussiana* de valor esperado *nulo*, o vetor $\\mathbf w$ de *m√°xima verossimilhan√ßa* √© dado pela minimiza√ß√£o da fun√ß√£o custo, i√©,:\n",
        "\\begin{equation}\n",
        "\\underset{\\mathbf w}{\\mbox{arg min}} \\parallel \\mathbf \\beps \\parallel^2= \\parallel \\mathbf{y} - \\mathbf{X w} \\parallel^2_2\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0G8LUzAQryt"
      },
      "source": [
        "Suponha que estamos interessados em minimizar, para $\\newcommand{\\bhat}{{\\mathbf w}}\\bhat \\in \\newcommand{\\reals}{\\mathbb{R}} \\reals^n$ qualquer,\n",
        "$$\n",
        "\\newcommand{\\err}{\\mathcal{E}}\n",
        "\\err = \\| \\beps \\|^2 = (\\newcommand{\\my}{\\mathbf y}\\my - \\newcommand{\\mX}{\\mathbf X}\\mX \\bhat)^T (\\my - \\mX \\bhat),\n",
        "$$\n",
        "onde $\\mw \\in \\reals^n$ s√£o os coeficientes da regress√£o. Assume-se, por simplicidade, que $n \\leq m$ e $\\mathrm{rank}(\\mX) = n$.\n",
        "\\begin{align}\n",
        "\\err &= \\my^T\\my-\\bhat^T\\mX^T\\my-\\my^T\\mX\\bhat+\\bhat^T\\mX^T\\mX\\bhat\\\\\n",
        "&=\\my^T\\my-2\\bhat^T\\mX^T\\my+\\bhat^T\\mX^T\\mX\\bhat \\tag{1}\n",
        "\\end{align}\n",
        "onde este desenvolvimento usa o fato de que a transposi√ß√£o de um escalar √© o escalar, ou seja $\\bhat^T\\mX^T\\my=\\my^T\\mX\\bhat$.\n",
        "\n",
        "Para encontrar o $\\bhat$ que minimiza a soma dos res√≠duos quadrados, precisamos tirar a derivada de (1) em rela√ß√£o a $\\bhat$. Isso nos d√° a seguinte equa√ß√£o:\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\err}{\\partial \\bhat} = -2\\mX^T\\my+ 2 \\mX^T\\mX\\bhat = 0\n",
        "\\end{equation}\n",
        "\n",
        "Da equa√ß√£o acima obtemos o que chamamos de *equa√ß√µes normais*,\n",
        "\\begin{equation}\n",
        "\\mX^T\\mX\\bhat = \\mX^T\\my \\tag{2}\n",
        "\\end{equation}\n",
        "onde, obviamente, $\\mX^T\\mX$ √© sempre quadrada sim√©trica.\n",
        "\n",
        "Lembre-se de que $\\mX^T\\mX$ e $\\mX^T\\my$ s√£o conhecidos de nossos dados, mas $\\bhat$ √© desconhecido. Se o inverso de $\\mX^T\\mX$ existe\n",
        "(ou seja, $\\left(\\mX^T\\mX\\right)^{-1}$, ent√£o a pr√©-multiplica√ß√£o de ambos os lados por esse inverso nos d√° a seguinte equa√ß√£o:\n",
        "\\begin{align}\n",
        "\\left(\\mX^T\\mX\\right)^{-1}\\left(\\mX^T\\mX\\right)\\bhat &= \\left(\\mX^T\\mX\\right)^{-1}\\mX^T\\my \\\\\n",
        "\\bhat &= \\left(\\mX^T\\mX\\right)^{-1}\\mX^T\\my\n",
        "\\end{align}\n",
        "\n",
        "Portanto, $\\err$ √© minimizado quando $\\bhat = (\\mX^T \\mX)^{-1} \\mX^T \\my$ e a matriz $\\left(\\mX^T \\mX\\right)^{-1}\\mX^T$ √© dita a *pseudo-inversa* de $\\mX$.\n",
        "\n",
        "**Nota**: Em geral n√£o √© eficiente calcular explicitamente esta matriz.\n",
        "\n",
        "Por√©m, lembre-se que definimos um res√≠duo, i√©, um erro da previs√£o em rela√ß√£o √† resposta exata $\\beps$,\n",
        "$$\n",
        "\\my=\\mX\\bhat+ \\beps\n",
        "$$\n",
        "\n",
        "Substituindo-se em (2),\n",
        "\\begin{align}\n",
        "\\left(\\mX^T\\mX\\right)\\bhat &= \\mX^T\\left(\\mX\\bhat+ \\beps\\right)\\\\\n",
        "\\left(\\mX^T\\mX\\right)\\bhat &= \\mX^T\\mX\\bhat+ \\mX^T\\beps \\\\\n",
        "\\therefore \\mX^T\\beps = 0\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 03\n",
        "\n",
        "Dadas as caracter√≠sticas ($x_1$ e $x_2$) e vari√°vel dependente ($y$),\n",
        "```\n",
        "x1 = np.array([4,2,1,3,1,6])\n",
        "x2 = np.array([1,8,0,2,4,7])\n",
        "y = np.array([2,-14,1,-1,-7,-8])\n",
        "```\n",
        "utilize a resposta anal√≠tica,\n",
        "$$\n",
        "\\newcommand{\\mw}{\\mathbf w}\\mw = (\\newcommand{\\mX}{\\mathbf X} \\mX^T \\mX)^{-1} \\mX^T \\newcommand{\\my}{\\mathbf y} \\my\n",
        "$$\n",
        "para encontrar os pesos da regress√£o linear."
      ],
      "metadata": {
        "id": "031JhNnx21dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.array([4,2,1,3,1,6])\n",
        "x2 = np.array([1,8,0,2,4,7])\n",
        "y = np.array([2,-14,1,-1,-7,-8])"
      ],
      "metadata": {
        "id": "mbbLOLP325F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MX = np.array([np.ones(len(x1)),x1,x2]).T\n",
        "W = np.linalg.solve(MX.T.dot(MX),MX.T.dot(y))\n",
        "print('w0 = '+str(W[0]) + '  w1 = '+str(W[1]) + '  w2 = '+str(W[2]))"
      ],
      "metadata": {
        "id": "QGavoTL-3VXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Agora, monte um procedimento iterativo para encontrar os pesos da regress√£o.__"
      ],
      "metadata": {
        "id": "6O1XA-g6f_RX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Sua resposta##"
      ],
      "metadata": {
        "id": "HaCtbhDaYxfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora veja o exemplo abaixo...\n",
        "\n",
        "A rela√ß√£o entre semanas de trabalho e carros vendidos da *Locadora Jack*."
      ],
      "metadata": {
        "id": "26aKmaVeX1S0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SemanasTrabalho = np.array([168.,428.,296.,392.,80.,56.,352.,444.,168.,200.,4.,52.,20.,228.,72.])\n",
        "CarrosVendidos =  np.array([272.,300.,311.,365.,167.,149.,366.,310.,192.,229.,88.,118.,62.,319.,193.])\n",
        "data = []\n",
        "for i,j in zip(SemanasTrabalho,CarrosVendidos):\n",
        "  data.append([i,j])\n",
        "data = np.array(data)"
      ],
      "metadata": {
        "id": "2gHnoE36WfJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w=[]\n",
        "w0,w1= 0.,100.\n",
        "w.append([w0,w1])\n",
        "\n",
        "alpha = 0.000025\n",
        "eps = 1e-4\n",
        "max_iter = 500000\n",
        "\n",
        "x=data[:,0]\n",
        "y=data[:,1]\n",
        "\n",
        "m = len(x)\n",
        "grad_w0 = 0.0\n",
        "grad_w1 = 0.0\n",
        "for i,j in zip(x,y):\n",
        "    grad_w0 += (w0 + w1*i - j)/m\n",
        "    grad_w1 += (w0 + w1*i - j)*i/m\n",
        "grad = [grad_w0,grad_w1]\n",
        "\n",
        "count = 0\n",
        "while np.linalg.norm(grad)>eps and count<max_iter:\n",
        "  grad_w0 = 0.0\n",
        "  grad_w1 = 0.0\n",
        "  for i,j in zip(x,y):\n",
        "    grad_w0 += (w0 + w1*i - j)/m\n",
        "    grad_w1 += (w0 + w1*i - j)*i/m\n",
        "  w0 = w0-alpha*grad_w0\n",
        "  w1 = w1-alpha*grad_w1\n",
        "  w.append([w0,w1])\n",
        "  grad = [grad_w0,grad_w1]\n",
        "  count += 1\n",
        "\n",
        "print('w0: '+str(w0))\n",
        "print('w1: '+str(w1))"
      ],
      "metadata": {
        "id": "MjIvMlzYX8Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=SemanasTrabalho, y=CarrosVendidos,alpha=0.75, label = \"Dados\")\n",
        "media = [np.mean(data[:,1])]*len(data)\n",
        "#\n",
        "plt.plot(data[:,0],w0+np.asarray(data[:,0])*w1,color='forestgreen', label = \"Regress√£o\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h6zt6SQ-W_PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_regr = w0+np.asarray(data[:,0])*w1\n",
        "\n",
        "x_bar = np.mean(data[:,0])\n",
        "y_bar = np.mean(data[:,1])\n",
        "\n",
        "dx = (data[:,0] - x_bar)\n",
        "dy = (data[:,1] - y_bar)\n",
        "dy_regr = (data[:,1] - data_regr)\n",
        "\n",
        "\n",
        "sxx = np.sum(dx**2)\n",
        "syy = np.sum(dy**2)\n",
        "sxy = np.sum(dx*dy)\n",
        "s_regr = np.sum(dy_regr**2)\n",
        "r2= (sxy/(np.sqrt(sxx*syy)))**2\n",
        "r2_regr = (syy - s_regr) / syy\n",
        "\n",
        "\n",
        "print('R2 √©: {:6.2f}'.format(r2))\n",
        "print('R2 √©: {:6.2f}'.format(r2_regr))"
      ],
      "metadata": {
        "id": "IiMrRcaMUvfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=SemanasTrabalho, y=dy_regr,alpha=0.75)\n",
        "media = [np.mean(data[:,1])]*len(data)\n",
        "plt.xlabel(\"Semanas de Trabalho\")\n",
        "plt.ylabel(\"Res√≠duo\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ym9lDTOaVHNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w0,w1,w2 = 63.851, 1.4095, -0.0019\n",
        "data_regr=w2*np.asarray(data[:,0])**2+w1*np.asarray(data[:,0])+w0\n",
        "regr = []\n",
        "for i,j in zip(SemanasTrabalho,data_regr):\n",
        "  regr.append([i,j])\n",
        "regr = np.array(regr)\n",
        "matrix = regr[np.argsort(regr[:,0])]"
      ],
      "metadata": {
        "id": "eSwv4yj9YbU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=SemanasTrabalho, y=y,alpha=0.75, label = \"Dados\")\n",
        "plt.plot(matrix[:,0],matrix[:,1], color='forestgreen', label = \"Regress√£o\")\n",
        "plt.xlabel(\"Semanas de Trabalho\")\n",
        "plt.ylabel(\"Res√≠duo\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FkrsWTtuYpLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_bar = np.mean(data[:,0])\n",
        "y_bar = np.mean(data[:,1])\n",
        "\n",
        "dx = (data[:,0] - x_bar)\n",
        "dy = (data[:,1] - y_bar)\n",
        "dy_regr = (data[:,1] - data_regr)\n",
        "sxx = np.sum(dx**2)\n",
        "syy = np.sum(dy**2)\n",
        "s_regr = np.sum(dy_regr**2)\n",
        "\n",
        "r2 = (syy - s_regr) / syy\n",
        "print('R2 √©: {:6.2f}'.format(r2_regr))"
      ],
      "metadata": {
        "id": "d4SeNoWcb77m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=SemanasTrabalho, y=dy_regr,alpha=0.75)\n",
        "media = [np.mean(data[:,1])]*len(data)\n",
        "plt.xlabel(\"Semanas de Trabalho\")\n",
        "plt.ylabel(\"Res√≠duo\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yHQrtHPjf8zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-x3LPYWEq2t"
      },
      "source": [
        "## Regress√£o Polinomial\n",
        "\n",
        "Dado $m$ conjuntos de $n$ dados dispostos na matriz $\\mathbf{X} \\in {\\mathbb{R}}^{m\\times n+1}$ rotulados por $\\mathbf{y} \\in {\\mathbb{R}}^m$ j√° definimos anteriormente que o erro ser√° minimizado quando\n",
        "$$\n",
        "\\newcommand{\\mw}{\\mathbf w}\\mw = (\\newcommand{\\mX}{\\mathbf X} \\mX^T \\mX)^{-1} \\mX^T \\newcommand{\\my}{\\mathbf y} \\my\n",
        "$$\n",
        "\n",
        "Derivar por um vetor pode parecer desconfort√°vel, mas n√£o h√° com o que se preocupar. Lembre-se de que aqui apenas usamos nota√ß√£o de matriz para representar convenientemente um sistema de f√≥rmulas lineares. Portanto, derivamos por cada componente do vetor e depois combinamos as derivadas resultantes em um vetor novamente,\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_{k}}\\sum_{i=1}^{m}\\left(y^{(i)}-\\sum_{j=0}^{n}x_{ij}w_{j}\\right)^{2}=0\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{m}2\\left(y^{(i)}-\\sum_{j=1}^{n}x_{ij}w_{j}\\right)\\left(\\frac{\\partial}{\\partial w_{k}}\\left[y^{(i)}-\\sum_{j=1}^{n}x_{ij}w_{j}\\right]\\right)=0\n",
        "$$\n",
        "\n",
        "$$\n",
        "-2\\sum_{i=1}^{m}x_{ik}\\left(y^{(i)}-\\sum_{j=1}^{n}x_{ij}w_{j}\\right)=0 \\tag{1}\n",
        "$$\n",
        "\n",
        "Pode-se reescrever as equa√ß√µes (1) na forma matricial. Por exemplo, para um polin√¥mio de ordem $4$,\n",
        "\\begin{equation}\n",
        "              \\begin{pmatrix}\n",
        "              m \\, \\, \\, \\, \\, \\, \\, & \\sum{x^{(i)}}\\, \\, & \\sum{\\left(x^{(i)}\\right)^2} & \\sum{\\left(x^{(i)}\\right)^3} & \\sum{\\left(x^{(i)}\\right)^4}  \\\\\n",
        "              \\sum{x^{(i)}}\\, \\, & \\sum{\\left(x^{(i)}\\right)^2} & \\sum{\\left(x^{(i)}\\right)^3} & \\sum{\\left(x^{(i)}\\right)^4} & \\sum{\\left(x^{(i)}\\right)^5}  \\\\\n",
        "              \\sum{\\left(x^{(i)}\\right)^2} & \\sum{\\left(x^{(i)}\\right)^3} & \\sum{\\left(x^{(i)}\\right)^4} & \\sum{\\left(x^{(i)}\\right)^5} & \\sum{\\left(x^{(i)}\\right)^6}  \\\\\n",
        "              \\sum{\\left(x^{(i)}\\right)^3} & \\sum{\\left(x^{(i)}\\right)^4} & \\sum{\\left(x^{(i)}\\right)^5} & \\sum{\\left(x^{(i)}\\right)^6} & \\sum{\\left(x^{(i)}\\right)^7}  \\\\\n",
        "              \\sum{\\left(x^{(i)}\\right)^4} & \\sum{\\left(x^{(i)}\\right)^5} & \\sum{\\left(x^{(i)}\\right)^6} & \\sum{\\left(x^{(i)}\\right)^7} & \\sum{\\left(x^{(i)}\\right)^8}  \\\\\n",
        "              \\end{pmatrix}\n",
        "              \\begin{pmatrix}\n",
        "              w_0 \\\\\n",
        "              w_1 \\\\\n",
        "              w_2 \\\\\n",
        "              w_3 \\\\\n",
        "              w_4 \\\\\n",
        "              \\end{pmatrix}\n",
        "              =\\begin{pmatrix}\n",
        "              \\sum{ y^{(i)}} \\\\\n",
        "              \\sum{x^{(i)} y^{(i)}} \\\\\n",
        "              \\sum{\\left(x^{(i)}\\right)^2 y^{(i)}} \\\\\n",
        "              \\sum{\\left(x^{(i)}\\right)^3 y^{(i)}} \\\\\n",
        "              \\sum{\\left(x^{(i)}\\right)^4 y^{(i)}} \\\\\n",
        "              \\end{pmatrix} \\tag{2}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w=[]\n",
        "w0,w1= 0.0,0.5\n",
        "w.append([w0,w1])\n",
        "\n",
        "alpha = 0.0005\n",
        "eps = 1e-4\n",
        "max_iter = 500000\n",
        "\n",
        "m = len(idade)\n",
        "grad_w0 = (w0 + w1*i - j)/m\n",
        "grad_w1 = (w0 + w1*i - j)*i/m\n",
        "grad = [grad_w0,grad_w1]\n",
        "\n",
        "x=data[:,0]\n",
        "y=data[:,1]\n",
        "\n",
        "count = 0\n",
        "while np.linalg.norm(grad)>eps and count<max_iter:\n",
        "  grad_w0 = 0.0\n",
        "  grad_w1 = 0.0\n",
        "  for i,j in zip(x,y):\n",
        "    grad_w0 += (w0 + w1*i - j)/m\n",
        "    grad_w1 += (w0 + w1*i - j)*i/m\n",
        "  w0 = w0-alpha*grad_w0\n",
        "  w1 = w1-alpha*grad_w1\n",
        "  w.append([w0,w1])\n",
        "  grad = [grad_w0,grad_w1]\n",
        "  count += 1\n",
        "\n",
        "print('w0: '+str(w0))\n",
        "print('w1: '+str(w1))"
      ],
      "metadata": {
        "id": "VcnX0E6GXTwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=idade, y=pressao,alpha=0.75)\n",
        "plt.plot(idade,w0_teo+np.asarray(idade)*w1_teo,color='crimson')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pA4SqPz0Xmke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 04\n",
        "\n",
        "Regress√£o polinomial √© principalmente aplic√°vel a estudos onde os ambientes s√£o altamente controlados e as observa√ß√µes s√£o feitas com um n√≠vel especificado de toler√¢ncia.\n",
        "\n",
        "Os dados abaixo s√£o os consumos de eletricidade, em quilowatts-hora por m√™s, de dez casas e suas respectivas √°reas, em metros quadrados,\n",
        "\n",
        "| √Årea | KW Hrs/Mes |\n",
        "| ---  | --- |\n",
        "| 120 | \t1182|\n",
        "| 135 | \t1172|\n",
        "| 147 | \t1264|\n",
        "| 160 | \t1493|\n",
        "| 171 | \t1571|\n",
        "| 184 | \t1711|\n",
        "| 198 | \t1804|\n",
        "| 223 | \t1840|\n",
        "| 240 | \t1956|\n",
        "| 293 | \t1954|\n",
        "\n",
        "Os dados podem ser modelados por meio de um polin√¥mio de segundo grau, com a seguinte equa√ß√£o:\n",
        "\n",
        "\\begin{equation}\n",
        "consumo = w_0 + w_1 area + w_2 area^2\n",
        "\\end{equation}\n",
        "\n",
        "Estime os coeficientes desconhecidos do modelo, $w_0$, $w_1$ e $w_2$, minimizando os desvios entre os dados e o resultado do modelo (ajuste de m√≠nimos quadrados).\n",
        "\n",
        "*Nota*: Use a fun√ß√£o ```np.linalg.solve(a, b)``` para resolver o sistema $ax=b$ da equa√ß√£o (2)."
      ],
      "metadata": {
        "id": "wWuSKujzjdyI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjUycOvmD_Nw"
      },
      "source": [
        "data = np.array([[1290 , 1182],\n",
        "[1350 , 1172],\n",
        "[1470 , 1264],\n",
        "[1600 , 1493],\n",
        "[1710 , 1571],\n",
        "[1840 , 1711],\n",
        "[1980 , 1804],\n",
        "[2230 , 1840],\n",
        "[2400 , 1956],\n",
        "[2930 , 1954]])\n",
        "\n",
        "area   = data[:,0]\n",
        "consumo = data[:,1]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(area, consumo, 'bo', color=\"black\")\n",
        "plt.xlabel(r'√Årea ($m^2$)')\n",
        "plt.ylabel(r'Consumo ($KW h/mes$)')\n",
        "plt.title('√Årea vs Consumo')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c26C1JGXQ9Re"
      },
      "source": [
        "aux = np.array([len(area), np.sum(area), np.sum(area**2), np.sum(area**3), np.sum(area**4)])\n",
        "\n",
        "b = np.array([[np.sum(consumo)],[np.sum(consumo*area)],[np.sum(consumo*area**2)]])\n",
        "n=2\n",
        "a=[]\n",
        "for i in range(n+1):\n",
        "  a.append([aux[i],aux[i+1],aux[i+2]])\n",
        "\n",
        "W = np.linalg.solve(a,b)\n",
        "print('w0 = '+str(W[0]) + '  w1 = '+str(W[1]) + '  w2 = '+str(W[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPBkrNLjSlCh"
      },
      "source": [
        "Como gabarito vamos usar uma biblioteca a fun√ß√£o `numpy.polyfit(x,y,n)` retorna os coeficientes para um polin√¥mio de grau `n` que melhor se ajusta aos dados. Os coeficientes retornados pela fun√ß√£o est√£o em forma descendente (par√¢metro que acompanha a maior pot√™ncia primeiro) e seu comprimento √© $n+1$.\n",
        "\n",
        "Depois de criar o modelo, vamos verificar se ele realmente se encaixa em nossos dados. Para fazer isso, usaremos o modelo para avaliar o polin√¥mio em tempos espa√ßados uniformemente. Para avaliar o modelo nos pontos especificados, podemos usar a fun√ß√£o `poly1d()`. Essa fun√ß√£o retorna o valor de um polin√¥mio de grau $n$ avaliado nos pontos fornecidos. O argumento de entrada √© um vetor de comprimento $n + 1$ cujos elementos s√£o os coeficientes das pot√™ncias descendentes do polin√¥mio a ser avaliado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SjUe_K3S8YQ"
      },
      "source": [
        "omegas = np.polyfit(area, consumo,2)\n",
        "y_bar = np.poly1d(omegas)\n",
        "x_bar = np.linspace(1000,3000, 1000)\n",
        "print('w0: '+str(omegas[0]),'w1: '+str(omegas[1]),'w2: '+str(omegas[2]))\n",
        "plt.figure()\n",
        "plt.plot(area, consumo, 'bo', color=\"black\", label='data')\n",
        "plt.xlabel(r'√Årea ($m^2$)')\n",
        "plt.ylabel(r'Consumo ($KW h/mes$)')\n",
        "plt.title('√Årea vs Consumo')\n",
        "plt.plot(x_bar, y_bar(x_bar), '-', color='darkcyan', label='regress√£o')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mude o grau do polin√¥mio para `5` e veja como se comporta sua resposta."
      ],
      "metadata": {
        "id": "joDipy5LnteH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "omegas = np.polyfit(area, consumo,5)\n",
        "y_bar = np.poly1d(omegas)\n",
        "x_bar = np.linspace(1000,3000, 1000)\n",
        "print('w0: '+str(omegas[0]),'w1: '+str(omegas[1]),'w2: '+str(omegas[2]))\n",
        "plt.figure()\n",
        "plt.plot(area, consumo, 'bo', color=\"black\", label='data')\n",
        "plt.xlabel(r'√Årea ($m^2$)')\n",
        "plt.ylabel(r'Consumo ($KW h/mes$)')\n",
        "plt.title('√Årea vs Consumo')\n",
        "plt.plot(x_bar, y_bar(x_bar), '-', color='darkcyan', label='regress√£o')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6yb4kMe2nmY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uma breve introdu√ß√£o a overfitting e underfitting\n"
      ],
      "metadata": {
        "id": "uIWpoc4Y1IDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vari√¢ncia (Variance)\n",
        "\n",
        "A vari√¢ncia √© um erro definido pela alta sensibilidade a pequenas flutua√ß√µes no conjunto de treinamento. Erro devido √† tend√™ncia do algoritmo de modelar o ru√≠do aleat√≥rio nos dados de treinamento, em vez das sa√≠das pretendidas. Esse fen√¥meno √© conhecido como *overfitting*.\n",
        "\n",
        "A vari√¢ncia ser expressa matematicamente como:\n",
        "\n",
        "\\begin{equation}\n",
        "Var[\\mathbf y] = E[(\\mathbf y-E[\\mathbf y])^2] = E[\\mathbf y^2] - E[\\mathbf y]^2 \\tag{2}\n",
        "\\end{equation}\n",
        "\n",
        "A igualdade acima √© essencial ao entendimento da rela√ß√£o entre vi√©s e vari√¢ncia e erro quadr√°tico m√©dio, e √© provada a seguir,\n",
        "\\begin{align}\n",
        "Var[\\mathbf y] =& E[(\\mathbf y- E[\\mathbf y])^2] \\\\\n",
        "=& E[\\mathbf y^2 - 2 \\mathbf y E[\\mathbf y] + E[\\mathbf y]^2] \\\\\n",
        "=& E[\\mathbf y^2] - 2E[\\mathbf y]^2 + E[\\mathbf y]^2 \\\\\n",
        "=& E[\\mathbf y^2] - E[\\mathbf y]^2\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "mFXpl_ty36xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vi√©s (Bias)\n",
        "Vi√©s √© a tend√™ncia de um estimador escolher um modelo para os dados que n√£o s√£o estruturalmente corretos. Um estimador tendencioso √© aquele que faz suposi√ß√µes incorretas no n√≠vel do modelo sobre o conjunto de dados.\n",
        "Para o caso de nossa regress√£o, erro devido √† incapacidade da hip√≥tese $h$ de se encaixar perfeitamente em $y$. Por exemplo, suponha que usamos um modelo de regress√£o linear em uma fun√ß√£o $y$ quadr√°tica ou c√∫bica.\n",
        "\n",
        "Matematicamente,\n",
        "\\begin{equation}\n",
        "\\text{Bias}[\\hat{y}^{(i)}(\\mathbf x^{(i)})] = E[\\hat{y}(\\mathbf x^{(i)}) - y(\\mathbf x^{(i)})] \\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "Vi√©s tamb√©m √© conhecido como underfitting."
      ],
      "metadata": {
        "id": "Zku_fORC3Okq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MSE e sua decomposi√ß√£o em vi√©s e vari√¢ncia\n",
        "Dessa forma, pode-se reescrever a f√≥rmula de $MSE$ como\n",
        "\n",
        "\\begin{align}\n",
        "MSE =& E[(y- \\hat{y})^2] \\\\\n",
        "=& E[y^2 + \\hat{y}^2 - 2 y\\hat{y}] \\\\\n",
        "=& \\text{Var}(y) + E[y]^2 + \\text{Var}[\\hat{y}] + E[\\hat{y}]^2 - 2 y E[\\hat{y}] \\\\\n",
        "=& \\text{Var}(y) + \\text{Var}(\\hat{y}) + E[(y^2 - 2 y E[\\hat{y}] + E[\\hat{y}]^2)] \\\\\n",
        "=& \\text{Var}(y) + \\text{Var}(\\hat{y}) +E[(y - E[\\hat{y}])^2] \\\\\n",
        "=& e^2 + \\text{Var}[\\hat{y}] + \\text{Bias}[\\hat{y}]^2\n",
        "\\end{align}\n",
        "\n",
        "Este resultado mostra que o erro quadr√°tico do estimador √© a soma da vari√¢ncia do estimador (qu√£o mal ele generaliza; seu n√≠vel de overfitting), o vi√©s (qu√£o pobre √© seu ajuste; seu n√≠vel de underfitting) e um erro irredut√≠vel no conjunto de dados subjacente, $e$.\n"
      ],
      "metadata": {
        "id": "A_4amqE63TTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Um modelo com __ALTO VI√âS__ aprende rela√ß√µes erradas e gera previs√µes longe do esperado. O modelo n√£o aprende corretamente com o conjunto de dados, assumindo informa√ß√µes sobre os dados que n√£o s√£o necessariamente corretas. Dessa forma, modelos com alto vi√©s possuem um problema de underfitting.\n",
        "\n",
        "Modelos com alta __ALTA VARI√ÇNCIA__ focam excessivamente se ajustar aos dados e, inclusive, ao ru√≠do. Assim, esses modelos t√™m um problema de overfitting, ou seja, se adaptam t√£o bem ao conjunto de dados que n√£o conseguem generalizar para al√©m dele.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1QIKLgbTJofvRur0RH8V3r2aRLFWqeqaN' width=\"600\"></center>\n",
        "\n",
        "\n",
        "Dado um erro constante, isso significa que sempre haver√° uma troca entre vi√©s e vari√¢ncia. Ter muito vi√©s ou muita varia√ß√£o n√£o √© bom para um modelo, mas por diferentes motivos. Um modelo de alto vi√©s e baixa varia√ß√£o provavelmente acabar√° impreciso nos conjuntos de dados de treinamento e valida√ß√£o, e suas previs√µes provavelmente n√£o se desviar√£o muito com base na amostra de dados em que ele √© treinado. Por outro lado, um modelo de vi√©s baixo e alta vari√¢ncia provavelmente fornecer√° bons resultados em um conjunto de dados de treinamento, mas falhar√° ao tentar generalizar para um conjunto de dados de valida√ß√£o."
      ],
      "metadata": {
        "id": "Cn6ydJhN4Baa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z8AQOib_4MWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 05\n",
        "\n",
        "Com os dados abaixo,\n",
        "```\n",
        "ùë• = [0., 0.18, 0.25, 0.4, 0.45, 0.55, 0.63, 0.75, 0.85, 1.]\n",
        "ùë¶ = [0.3, 0.8, 1., 0.95, 0.25, 0.3, -0.9, -0.7, -0.8, 0.35]\n",
        "```\n",
        "modifique a ordem de aproxima√ß√£o de 0 a 9 e responda:\n",
        "1. o que aconteceu com os valores dos par√¢metros a medida que o grau do polin√¥mio de interpola√ß√£o aumentou?\n",
        "2. porque isso aconteceu?\n"
      ],
      "metadata": {
        "id": "cfYIkA3W1RSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array([0.,0.18,0.25,0.4,0.45,0.55,0.63,0.75,0.85,1.])\n",
        "y=np.array([0.3,0.8,1.,0.95,0.25,0.3,-0.9,-0.7,-0.8,0.35])"
      ],
      "metadata": {
        "id": "4mQk1oyC7rJD"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "tx9Q_TC98Hka"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "colors = ['SkyBlue','DeepSkyBlue', 'CornflowerBlue', 'DodgerBlue', 'RoyalBlue', 'Blue', 'MediumBlue', 'DarkBlue', 'Navy']\n",
        "for i in range(0,10):\n",
        "  plt.scatter(x, y, color=\"black\", label = 'dataset')\n",
        "  #veja que constru√≠mos uma pipeline, para repetir a sequencia de gerar as features e calcular os par√¢metros da regress√£o\n",
        "  model = make_pipeline(PolynomialFeatures(degree=i), LinearRegression())\n",
        "  model.fit(np.array(x).reshape(-1, 1), y)\n",
        "  x_reg = np.arange(0,1,0.01)\n",
        "  y_reg = model.predict(x_reg.reshape(-1, 1))\n",
        "  plt.plot(x_reg, y_reg, color=colors[i-1], label = 'degree '+str(i))\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "oTU6vDUF7tvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ['SkyBlue','DeepSkyBlue', 'CornflowerBlue', 'DodgerBlue', 'RoyalBlue', 'Blue', 'MediumBlue', 'DarkBlue', 'Navy']\n",
        "for i in range(0,10):\n",
        "  plt.scatter(x, y, color=\"black\")\n",
        "  polynomial_features= PolynomialFeatures(degree=i)\n",
        "  x_poly = polynomial_features.fit_transform(x.reshape(-1,1))\n",
        "  model = LinearRegression()\n",
        "  model.fit(x_poly, y)\n",
        "  y_poly_pred = model.predict(x_poly)\n",
        "\n",
        "  rmse = np.sqrt(mean_squared_error(y,y_poly_pred))\n",
        "  r2 = r2_score(y,y_poly_pred)\n",
        "  print(\"RMSE:\", rmse)\n",
        "  print(\"R2:\", r2)\n",
        "  print(\"Coeficientes:\", model.coef_)\n",
        "  print(\"Intercepta√ß√£o:\", model.intercept_)\n",
        "  # sort the values of x before line plot\n",
        "  sort_axis = operator.itemgetter(0)\n",
        "  sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)\n",
        "  x_sort, y_poly_pred = zip(*sorted_zip)\n",
        "  plt.plot(x_sort, y_poly_pred, color=colors[i-1], label = 'degree '+str(i))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Q8q1fm_o7xGM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}