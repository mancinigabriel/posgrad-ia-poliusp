{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <font face=\"Verdana\" size=6 color='#6495ED'> ANÁLISE ESTATÍSTICA DE DADOS\n",
        "<font face=\"Verdana\" size=3 color='#40E0D0'> Profs. Larissa Driemeier e Arturo Forner-Cordero\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1nW_7p_LyFhbR0ipjSekPcAj6kDoyK73R' width=\"800\"></center>\n",
        "\n",
        "Este notebook faz parte da aula 05 do curso [IAD-001](https://alunoweb.net/moodle/pluginfile.php/140418/mod_resource/content/6/EST_04_Y2024.pdf)."
      ],
      "metadata": {
        "id": "zZpAi63A1jmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importando bibliotecas"
      ],
      "metadata": {
        "id": "pmIPYoHT1j3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm, colors\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "w1cORyXg2UT4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normas\n",
        "\n",
        "Normas são características importantes que definem um vetor em um único valor. São quaisquer funções, definidas por barras horizontais $\\|\\boldsymbol{u}\\|$, que são caracterizadas pelas seguintes propriedades:\n",
        "\n",
        "1. Normas são valores não negativos. Se você pensar nas normas como um comprimento, verá facilmente por que não pode ser negativo;\n",
        "\n",
        "2. As normas são $0$ se e somente se o vetor é um vetor zero;\n",
        "\n",
        "3. As normas respeitam a desigualdade $\\|{\\boldsymbol{u}+\\boldsymbol{v}}\\| \\leq \\|{\\boldsymbol{u}}\\|+\\|{\\boldsymbol{v}}\\|$;\n",
        "\n",
        "4. A norma de um vetor multiplicado por um escalar $\\alpha$ é igual ao valor absoluto desse escalar multiplicado pela norma do vetor, $\\|\\alpha \\boldsymbol{u}\\|= |\\alpha| \\|{\\boldsymbol{u}}\\|$.\n",
        "\n",
        "De maneira geral,\n",
        "\n",
        "$$\n",
        "|| x ||_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}\n",
        "$$\n",
        "\n",
        "\n",
        "## Norma L1\n",
        "\n",
        "Para $p=1$ tem-se a norma dita $L^1$:\n",
        "\n",
        "$$\n",
        "L^1 = || x ||_1 = \\sum_i |x_i| = |x_1| + |x_2| + \\ldots + |x_i|\n",
        "$$\n",
        "\n",
        "## Norma L2\n",
        "\n",
        "Para $p=2$ tem-se a norma dita $L^2$:\n",
        "\n",
        "$$\n",
        "L^2 = || x ||_2 = \\sqrt{\\left(\\sum_i x_i^2\\right)} = \\sqrt{x_1^2 + x_2^2 + \\ldots + x_i^2}\n",
        "$$\n",
        "\n",
        "\n",
        "## Norma Linf\n",
        "\n",
        "Para $p=\\infty$ tem-se a norma dita $L^\\infty$:\n",
        "\n",
        "$$\n",
        "L^\\infty = \\max_i|x_i|\n",
        "$$"
      ],
      "metadata": {
        "id": "YqDKM1Md7v4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Norma L1\n",
        "v = np.array([-1, -2, 3, 4, 5])\n",
        "np.linalg.norm(v,1)"
      ],
      "metadata": {
        "id": "wPGUpjdQ7usC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Norma L2\n",
        "v = np.array([-1, -2, 3, 4, 5])\n",
        "np.linalg.norm(v,2)\n",
        "#"
      ],
      "metadata": {
        "id": "JNGoI2Cp9xod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Norma Linfty\n",
        "v = np.array([-1, -2, 3, 4, 5])\n",
        "np.linalg.norm(v,np.inf)"
      ],
      "metadata": {
        "id": "uuw_yZVER6Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y = np.meshgrid(np.arange(-2, 2, .1), np.arange(-2, 2, .1))\n",
        "print('vetor X\\n',X,'\\n')\n",
        "print('vetor Y\\n',Y,'\\n')"
      ],
      "metadata": {
        "id": "EGvJC5Qu-FXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z=[X,Y]\n",
        "print(np.shape(Z))"
      ],
      "metadata": {
        "id": "n9jl5BtErQsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X, Y, marker='o');"
      ],
      "metadata": {
        "id": "j7OqQUQm2Lcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z_L1 = np.linalg.norm(Z,1, axis=0) # np.abs(X)+np.abs(Y)\n",
        "Z_L2 = np.linalg.norm(Z,2, axis=0) # np.sqrt(X**2+Y**2)\n",
        "Z_L2_2 = np.square(np.linalg.norm(Z,2, axis=0)) # X**2+Y**2\n",
        "Z_inf = np.linalg.norm(Z,np.inf, axis=0) # np.amax([np.absolute(X),np.absolute(Y)], axis=0)"
      ],
      "metadata": {
        "id": "BTS3okCX0uao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def makeplot(position,angle1,angle2,rotation,alpha,Z,L=1):\n",
        "    ax = fig.add_subplot(position,projection='3d')\n",
        "    ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
        "                cmap='viridis', edgecolor='none', alpha = alpha)\n",
        "    ax.view_init(angle1,angle2)\n",
        "    ax.set_xlabel(r'$x_1$', fontsize=12, labelpad=10)\n",
        "    ax.set_ylabel(r'$x_2$', fontsize=12, labelpad=10)\n",
        "    ax.zaxis.set_rotate_label(False)\n",
        "    if L ==1:\n",
        "      ax.set_zlabel(r'$L^1$', rotation=rotation, fontsize=12, labelpad=0.2)\n",
        "    elif L == 2:\n",
        "      ax.set_zlabel(r'$L^2$', rotation=rotation, fontsize=12, labelpad=0.2)\n",
        "    elif L == 3:\n",
        "      ax.set_zlabel(r'$L^\\infty$', rotation=rotation, fontsize=12, labelpad=0.2)\n",
        "    else:\n",
        "      ax.set_zlabel(r'$L$', rotation=rotation, fontsize=12, labelpad=0.2)\n",
        "    return ax"
      ],
      "metadata": {
        "id": "Zokyo1ELAYZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(40,8))\n",
        "ax = makeplot(121,30,-120,90,0.95,Z_L1)\n",
        "fig.colorbar(plt.cm.ScalarMappable(cmap='viridis'), ax = ax)\n",
        "ax.set_box_aspect(aspect=None, zoom=0.95)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fBGwXaI3El0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig.get_size_inches()"
      ],
      "metadata": {
        "id": "FS6GCOFpzbVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(40,8))\n",
        "ax1 = makeplot(121,30,-60,100,0.6,Z_L2, L=2)\n",
        "fig.colorbar(plt.cm.ScalarMappable(cmap='viridis'), ax = ax1)\n",
        "ax.set_box_aspect(aspect=None, zoom=0.95)\n",
        "plt.show()\n",
        "fig = plt.figure(figsize=(40,8))\n",
        "ax2 = makeplot(122,30,-60,100,0.6,Z_L2_2, L=2)\n",
        "fig.colorbar(plt.cm.ScalarMappable(cmap='viridis'), ax = ax2)\n",
        "ax.set_box_aspect(aspect=None, zoom=0.95)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nwclOqHJAQ5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(40,8))\n",
        "ax = makeplot(121,30,-60,100,0.6,Z_inf,L=3)\n",
        "fig.colorbar(plt.cm.ScalarMappable(cmap='viridis'), ax = ax)\n",
        "ax.set_box_aspect(aspect=None, zoom=0.95)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LbYedmnyw5Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vizualizando os círculos da norma $p$\n",
        "\n",
        "A norma L1 é formalmente definida como a soma do valor absoluto das coordenadas de um vetor. __Então, por que o diamante?__\n",
        "\n",
        "A norma L2 é formalmente definida como o quadrado da diferença das coordenada de um vetor. __Então, por que o círculo?__\n",
        "\n",
        "A norma L∞ é formalmente definida como a dimensão absoluta máxima das coordenada de um vetor. __Então, por que o quadrado?__\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=14CLkD07pBj77LrvOBENlGVuq5VDP54aE' width=\"120\"></center>"
      ],
      "metadata": {
        "id": "EN3RUN3VEAPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Circulo =np.array([[1,0],[0,1],[np.cos(np.deg2rad(45)),np.sin(np.deg2rad(45))],[np.cos(np.deg2rad(60)),np.sin(np.deg2rad(60))]])\n",
        "np.shape(Circulo)"
      ],
      "metadata": {
        "id": "-X44qWKPEAgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C_L1 = np.array([[1,0],[0.75,0.25],[0.5,0.5],[0.25,0.75],[0,1]])\n",
        "C_L2 = np.array([[1,0],[0,1],[np.cos(np.deg2rad(30)),np.sin(np.deg2rad(30))],[np.cos(np.deg2rad(45)),np.sin(np.deg2rad(45))],[np.cos(np.deg2rad(60)),np.sin(np.deg2rad(60))]])\n",
        "C_Linf = np.array([[1,0],[1,0.5],[1,1],[0.5,1],[0,1]])\n",
        "print(np.linalg.norm(C_L1,1, axis=1))\n",
        "print(np.linalg.norm(C_L2,2, axis=1))\n",
        "print(np.linalg.norm(C_Linf,np.inf, axis=1))"
      ],
      "metadata": {
        "id": "2QpnBMFv1lhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colormap = ('lightgreen', 'darkgreen','skyblue','navy','salmon','crimson','mistyrose','palevioletred')\n",
        "#diamante\n",
        "plt.plot(    [1, 0],\n",
        "    [0, 1],\n",
        "    color = colormap[0])\n",
        "#círculo\n",
        "angles = np.linspace(0 * np.pi, 2 * np.pi, 100 )\n",
        "r = 1.\n",
        "xs = r * np.cos(angles)\n",
        "ys = r * np.sin(angles)\n",
        "plt.plot(xs, ys , color = colormap[2])\n",
        "#quadrado\n",
        "plt.plot(\n",
        "    [1, 1, 0],\n",
        "    [0, 1, 1],\n",
        "    color = colormap[4]\n",
        ")\n",
        "\n",
        "for j,a in enumerate([C_L1, C_L2, C_Linf]):\n",
        "  plt.scatter( a[:,0], a[:,1],color = colormap[2*j+1])\n",
        "\n",
        "plt.xlim(0, 1.1)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3HV1mAPOMvAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Derivada\n",
        "\n",
        "Derivadas são muito importantes em problemas de otimização. Elas nos dizem como alterar as entradas de uma função de maneira a aumentar ou diminuir sua saída da função, para que possamos nos aproximar do mínimo ou máximo da função.\n",
        "\n",
        "Porém, lembrar exatamente como diferenciar equações pode ser um desafio. Ou, talvez, você tenha uma equação longa e complicada que deva derivar. Dependendo da equação, você pode levar de 10 a 15 minutos para fazer isso manualmente.\n",
        "\n",
        "Vamos conhecer, então, a biblioteca `SymPy` em Python, que pode fazer todo esse trabalho pesado para nós. Ela tem tudo o que precisamos para derivar. Essa biblioteca também integra, resolve sistema de equações, simplifica equações...  mas tudo isso está fora do nosso escopo aqui!\n",
        "\n",
        "Como exemplo, suponha a função:\n",
        "$$\n",
        "f(x,y)=xy+x^2+\\sin{2y}\n",
        "$$\n",
        "\n",
        "Usando a biblioteca `SymPy`, encontre:\n",
        "* a primeira derivada da função $f(x,y)$ com respeito a $x$;\n",
        "* a segunda derivada da função $f(x,y)$ com respeito a $y$.\n"
      ],
      "metadata": {
        "id": "gWxKf85d2aOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy"
      ],
      "metadata": {
        "id": "dAUMShLU2Wcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = sympy.Symbol('x')\n",
        "y = sympy.Symbol('y')"
      ],
      "metadata": {
        "id": "QSTS_Lll2qhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando a equação\n",
        "f = x * y + x ** 2 + sympy.sin(2 * y)"
      ],
      "metadata": {
        "id": "k5wXkidT3J7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Primeira derivada com respeito a x\n",
        "df_dx = sympy.diff(f, x)\n",
        "print(\"A derivada de f(x,y) com respeito a x é: \" + str(df_dx))"
      ],
      "metadata": {
        "id": "I7eWvOmx3SOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Segunda derivada comr espeito a y\n",
        "d2f_dy2 = sympy.diff(f, y, 2)\n",
        "print(\"A segunda derivada de f(x,y) com respeito a y é: \" + str(d2f_dy2))"
      ],
      "metadata": {
        "id": "Gv4b9gl73ij2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradiente\n",
        "\n",
        "O gradiente é a generalização da derivada para funções multivariadas. Ele captura a inclinação local da função, permitindo prever o efeito de dar um pequeno passo de um ponto em qualquer direção.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1xD6tvXQPF5WZW3dJUb9iJXNJeONBzgXA' width=\"800\"></center>\n",
        "\n",
        "\n",
        "Ou seja, no caso de uma função univariada, é simplesmente a primeira derivada em um ponto selecionado.\n",
        "\n",
        "No caso de uma função multivariada, é um vetor de derivadas em cada direção (ao longo dos eixos das variáveis).\n",
        "\n"
      ],
      "metadata": {
        "id": "gpmhiKvj2W0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def makeplot(position,angle1,angle2,rotation,alpha):\n",
        "    ax = fig.add_subplot(position,projection='3d')\n",
        "    ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
        "                cmap='viridis', edgecolor='none', alpha = alpha)\n",
        "    ax.view_init(angle1,angle2)\n",
        "    ax.set_xlabel(r'$x_1$', fontsize=12)\n",
        "    ax.set_ylabel(r'$x_2$', fontsize=12)\n",
        "    ax.zaxis.set_rotate_label(False)\n",
        "    ax.set_zlabel(r'$f(x_1,x_2)$', rotation=rotation, fontsize=12, labelpad=2)\n",
        "    return ax"
      ],
      "metadata": {
        "id": "rhuKptQOUj-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x, y):\n",
        "    return x*np.exp(-x**2 - y**2)"
      ],
      "metadata": {
        "id": "grmxdRXsVHL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y = np.meshgrid(np.arange(-2, 2, .1), np.arange(-2, 2, .1))\n",
        "Z = f(X, Y)"
      ],
      "metadata": {
        "id": "Z3TEsKVGNz1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(20,10))\n",
        "ax1 = makeplot(121,30,-120,90,0.95)\n",
        "ax2 = makeplot(122,30,-60,100,0.6)\n",
        "\n",
        "norm = colors.Normalize(np.min(Z), np.max(Z))\n",
        "cbar_ax = fig.add_axes([0.5, 0.5, 0.01, 0.38])\n",
        "fig.colorbar(plt.cm.ScalarMappable(norm=norm,cmap='viridis'), cax=cbar_ax, ax = ax2)\n",
        "\n",
        "ax1.set_box_aspect(aspect=None, zoom=0.95)\n",
        "ax2.set_box_aspect(aspect=None, zoom=0.95)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qUvVhhMOLXA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V,U = np.gradient(Z, .2, .1)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "cmap = plt.get_cmap()\n",
        "q = ax.quiver(X,Y,U,V, Z, cmap = 'viridis')\n",
        "plt.show();\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "plt.imshow(Z, interpolation='bilinear');"
      ],
      "metadata": {
        "id": "wPaWhZltVVlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veja o exemplo dos slides,\n",
        "$$\n",
        "f(x_1,x_2)=x_1^2+x_2^2\n",
        "$$"
      ],
      "metadata": {
        "id": "gmlbhMywTS9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x, y):\n",
        "    return x**2 + y**2"
      ],
      "metadata": {
        "id": "SPKB5axd1oU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y = np.meshgrid(np.arange(-8, 8, .3), np.arange(-8, 8, .3))\n",
        "Z = f(X, Y)"
      ],
      "metadata": {
        "id": "P9lAVcnUmhyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(20,10))\n",
        "\n",
        "def makeplot(position,angle1,angle2,rotation):\n",
        "    ax = fig.add_subplot(position,projection='3d')\n",
        "    ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
        "                cmap='viridis', edgecolor='none', alpha = 0.6)\n",
        "    ax.view_init(angle1,angle2)\n",
        "    ax.set_xlabel(r'$x_1$', fontsize=12)\n",
        "    ax.set_ylabel(r'$x_2$', fontsize=12)\n",
        "    ax.zaxis.set_rotate_label(False)\n",
        "    ax.set_zlabel(r'$f(x_1,x_2)$', rotation=rotation, fontsize=12, labelpad=2)\n",
        "    return ax\n",
        "\n",
        "ax1 = makeplot(121,30,60,95)\n",
        "ax2 = makeplot(122,60,35,100)\n",
        "norm = colors.Normalize(np.min(Z), np.max(Z))\n",
        "cbar_ax = fig.add_axes([0.5, 0.5, 0.01, 0.38])\n",
        "fig.colorbar(plt.cm.ScalarMappable(norm=norm,cmap='viridis'), cax=cbar_ax, ax = ax2)"
      ],
      "metadata": {
        "id": "1dXDRs1y84Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V,U = np.gradient(Z, 1,1)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "cmap = plt.get_cmap()\n",
        "q = ax.quiver(X,Y,U,V, Z, cmap = 'viridis')\n",
        "plt.show();\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "plt.imshow(Z, interpolation='bilinear');"
      ],
      "metadata": {
        "id": "V2jPb_fXQFeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(20,10))\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
        "x0,y0 = 2,3\n",
        "z0=x0**2+y0**2\n",
        "ax.scatter(x0, y0, z0, color='crimson', linewidth=0.5, s=50)\n",
        "ax.set_box_aspect(aspect=None, zoom=0.95)\n",
        "ax.set_xlabel(r'$x_1$', fontsize=12)\n",
        "ax.set_ylabel(r'$x_2$', fontsize=12)\n",
        "ax.set_zlabel(r'$f(x_1,x_2)$',fontsize=12);"
      ],
      "metadata": {
        "id": "VJxmd2eOHipv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(20,10))\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
        "x0,y0 = 2,3\n",
        "z0=x0**2+y0**2\n",
        "ax.scatter(x0, y0, z0, color='seagreen', linewidth=0.5)\n",
        "ax.set_xlabel(r'$x_1$', fontsize=12)\n",
        "ax.set_ylabel(r'$x_2$', fontsize=12)\n",
        "ax.set_zlabel(r'$f(x_1,x_2)$',fontsize=12)\n",
        "x0,y0 = 2,3\n",
        "z0=x0**2+y0**2\n",
        "x1,y1 = 2*x0,2*y0\n",
        "z1 = x1**2+y1**2\n",
        "u=(x1-x0)\n",
        "v=(y1-y0)\n",
        "w=(z1-z0)\n",
        "N = np.sqrt(u**2+v**2+w**2)  # there may be a faster numpy \"normalize\" function\n",
        "uN,vN,wN = u/N,v/N,w/N\n",
        "ax.scatter(x0, y0, z0, s=50, color = 'crimson', linewidth=0.5)\n",
        "ax.set_box_aspect(aspect=None, zoom=0.95)\n",
        "\n",
        "ax.quiver(\n",
        "        x0,y0,z0, # <-- starting point of vector\n",
        "        x1,y1,z1, # <-- directions of vector\n",
        "        length=0.3, linewidth = 4,\n",
        "        color = 'navy', alpha = .8, arrow_length_ratio=0.1\n",
        "    )\n",
        "\n",
        "ax.scatter(x1, y1, z1, color = 'crimson', s=50, linewidth=0.5);\n"
      ],
      "metadata": {
        "id": "26k3oT9So7BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E um gráfico 4D? Ainda dá para ver...."
      ],
      "metadata": {
        "id": "xWya8OkP4KVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "x = np.array(range(0, 50))\n",
        "y = np.array(range(0, 50))\n",
        "z = np.array(range(0, 50))\n",
        "colors = np.random.standard_normal(len(x))\n",
        "img = ax.scatter(x, y, z, c=colors, cmap='viridis')\n",
        "fig.colorbar(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ogESwZV4N1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Otimização\n",
        "\n",
        "Muitos algoritmos em aprendizado de máquina otimizam uma função objetivo em relação a um conjunto de parâmetros de modelo desejados que controlam quão bem um modelo explica os dados. __Encontrar bons parâmetros pode ser formulado como um problema de otimização.__\n"
      ],
      "metadata": {
        "id": "bid0fchIwbhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def makeplot(position,angle1,angle2,rotation,alpha):\n",
        "    ax = fig.add_subplot(position,projection='3d')\n",
        "    ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
        "                cmap='viridis', edgecolor='none', alpha = alpha)\n",
        "    ax.view_init(angle1,angle2)\n",
        "    ax.set_xlabel(r'$\\omega_0$', fontsize=12)\n",
        "    ax.set_ylabel(r'$\\omega_1$', fontsize=12)\n",
        "    ax.zaxis.set_rotate_label(False)\n",
        "    ax.set_zlabel(r'$f(\\omega_0,\\omega_1)$', rotation=rotation, fontsize=12, labelpad=2)\n",
        "    return ax"
      ],
      "metadata": {
        "id": "XfGFa6xPwjYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x, y):\n",
        "    return np.sin(x)+x*np.cos(np.sqrt(2)*x)+y*np.sin(y)"
      ],
      "metadata": {
        "id": "j7mHgNOFwomM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y = np.meshgrid(np.arange(-5, 5, .1), np.arange(-5, 5, .1))\n",
        "Z = f(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(25,10))\n",
        "ax1 = makeplot(121,20,-50,95,1)\n",
        "ax2 = makeplot(122,20,-50,95,0.6)\n",
        "plt.subplots_adjust(wspace = 0.001 )\n",
        "cbar_ax = fig.add_axes([0.5, 0.5, 0.01, 0.38])\n",
        "fig.colorbar(plt.cm.ScalarMappable(norm=norm,cmap='viridis'), cax=cbar_ax, ax = ax2)\n",
        "ax.set_box_aspect(aspect=None, zoom=0.95)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lZ7unvI5wqwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = mpl.cm.jet(np.hypot(X,Y))\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "ax.contourf(X,Y,Z, facecolors=colors);"
      ],
      "metadata": {
        "id": "Fc_nC4h5wtnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "ax.contour(X,Z, Y, [-4])"
      ],
      "metadata": {
        "id": "37sCxH-J010_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regressão multilinear\n",
        "\n",
        "A regressão multi-linear trata da relação de uma variável dependente com *múltiplas* variáveis independentes.\n",
        "\n",
        "\\begin{equation}\n",
        "y_i = \\omega_0 + \\omega_1 x_{i,1} + \\omega_2 x_{i,2} +\\ldots + w_m x_{i,m}\n",
        "\\end{equation}\n",
        "\n",
        "seja\n",
        "\n",
        "\\begin{equation}\n",
        "\\boldsymbol{y} = \\begin{bmatrix}y_1\\\\\n",
        "y_2\\\\\n",
        "\\vdots\\\\\n",
        "y_n\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        " um vetor com uma amostra do conjunto de variáveis independentes e\n",
        "\n",
        "\\begin{equation}\n",
        "\\boldsymbol{X} = \\begin{bmatrix}1 & x_{1,1} & x_{1,2} & \\ldots & x_{1,m} \\\\ 1 &\n",
        "x_{2,1} & x_{2,2} & \\ldots & x_{2,m} \\\\ 1 &\n",
        " \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\ 1 &\n",
        "x_{n,1} & x_{n,2} & \\ldots & x_{n,m} \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "uma matriz $n \\times (m+1)$ cuja  *primeira* coluna é composta da constante $1$ e as $m$ próximas colunas são compostas por uma amostra de cada uma das $m$ variáveis independetes.\n",
        "\n",
        "O modelo linear para o comportamento destas variáveis é dado pela equação:\n",
        "\n",
        "\\begin{equation}\n",
        "\\boldsymbol{y} =  \\boldsymbol{X \\omega} \\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "Onde:\n",
        "\n",
        "\\begin{equation}\n",
        "\\boldsymbol{\\omega} = \\begin{bmatrix}\n",
        "w_0\\\\\n",
        "w_1\\\\\n",
        "w_2\\\\\n",
        "\\vdots\\\\\n",
        "w_m\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "é o vetor  de $(m+1)$ componentes dos coeficientes de cada uma das variáveis\n",
        "independentes. Note que nessa notação o coeficiente $w_0$ é o *primeiro* coeficiente (há notações distintas nas quais ele é o último). $\\boldsymbol{e}$ é o vetor de *resíduos* (a diferença entre o modelo real e os dados realmente observados).\n",
        "\n",
        "Em muitas situações, não conseguiremos encontrar um vetor $\\boldsymbol{y}$, tal que satisfaça a equação (1). Então, em vez disso, nos contentaremos em encontrar um vetor $\\boldsymbol{\\omega}$ tal que $\\boldsymbol{X\\omega}$ seja o mais próximo possível $\\boldsymbol{y}$, medido pelo quadrado da norma,\n",
        "\n",
        "\\begin{equation}\n",
        "\\|\\boldsymbol{y}-\\boldsymbol{X\\omega}\\|^2\n",
        "\\end{equation}\n",
        "\n",
        "A solução ótima $\\boldsymbol{\\omega}^*$ foi desenvolvida em aula e pode ser escrita como:\n",
        "\\begin{equation}\n",
        "\\boldsymbol{\\omega}^*=\\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}\n",
        "\\end{equation}\n",
        "\n",
        "e a matriz $\\left(X^T X\\right)^{-1}X^T$ é dita a *pseudo-inversa* de $X$.\n",
        "\n",
        "*Nota*: Em geral não é eficiente calcular explicitamente esta matriz.\n",
        "\n",
        "## Exemplo do slide\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1xo7HfEOAgttK7JlfZL6y2eH-YMLiB4Sz' width=\"600\"></center>\n",
        "\n",
        "Com dados extraídos do [link](https://www.kaggle.com/datasets/yasserh/housing-prices-dataset), encontre os parâmetros ótimos para a previsão do preço de um imóvel. Primeiramente, considere apenas um parâmetro – dimensão – depois, aumente e inclua os demais. Compare os resultados.\n",
        "\n"
      ],
      "metadata": {
        "id": "mPxLCKUCz7X6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veja a tabela completa..."
      ],
      "metadata": {
        "id": "h-W6J7QCnohg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "W_x95dpch7LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados = pd.read_csv('Housing.csv',sep=',')\n",
        "dados.head()"
      ],
      "metadata": {
        "id": "aTI_PRDDiVmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mas vamos trabalhar com os poucos dados destacados na tabela abaixo, somente para ilustração.\n",
        "\n",
        "\n",
        "Casa  | Dimensão $(m^2)$ |# quartos |# Banheiros |Idade (anos)| Preço $(US\\$/10^6)$\n",
        "------|------------------|----------|----------|------------|-------------------\n",
        "01    | 689              | 4        | 2        |   3       | 13.300\n",
        "02    | 832              | 4        | 4        |   4       | 12.250\n",
        "03    | 613              | 4        | 2        |   2       | 9.100\n",
        "04    | 557              | 3        | 2        |   3       | 6.650\n",
        "04    | 370              | 2        | 2        |   1       | 3.150"
      ],
      "metadata": {
        "id": "5xa3yPhJnrye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1=np.array([[1,689],[1,832],[1,613],[1,557],[1,370]])\n",
        "X = np.array([[1,689,4,2,3],[1,832,4,4,4],[1,613,4,2,2],[1,557,3,2,3],[1,370,2,2,1]])\n",
        "y=np.array([13.300,12.250,9.100,6.650,3.150])"
      ],
      "metadata": {
        "id": "ns6lUNXikl8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vetor w* considerando somente a dimensão da casa\n",
        "w = np.dot(np.linalg.inv(np.dot(X1.T,X1)),np.dot(X1.T,y))\n",
        "print(\"w0: \" + str(w))"
      ],
      "metadata": {
        "id": "6VSTdeN6lOUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X1[:,1],y, color = 'black')\n",
        "area = np.linspace(300,800,2)\n",
        "y_area = w[0]+w[1]*area\n",
        "plt.plot(area,y_area, color = 'crimson',linewidth = 3)\n",
        "plt.xlabel(r'area $[m^2]$', fontsize=11)\n",
        "plt.ylabel(r'valor [$\\times 1000$ Reais]', fontsize=11)\n",
        "plt.show;"
      ],
      "metadata": {
        "id": "4iTPG50MNFJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vetor w* considerando todas as características\n",
        "X = np.array([[1,689,4,2,3],[1,832,4,4,4],[1,613,4,2,2],[1,557,3,2,3],[1,370,2,2,1]])\n",
        "y=np.array([13.300,12.250,9.100,6.650,3.150])"
      ],
      "metadata": {
        "id": "IjBM1fHCZv8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vetor w* considerando somente a dimensão da casa\n",
        "w = np.dot(np.linalg.inv(np.dot(X.T,X)),np.dot(X.T,y))\n",
        "print(\"w0: \" + str(w[0]) + \" w1: \" + str(w[1]) + \" w2: \" + str(w[2]) + \"  w3: \" + str(w[3])+ \"  w4: \" + str(w[4]))"
      ],
      "metadata": {
        "id": "BnAt_K07_j1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lembrando nossa tabela,\n",
        "\n",
        "Casa  | Dimensão $(m^2)$ |# Quartos |# Banheiros |Idade (anos)| Preço $(US\\$/10^6)$\n",
        "------|------------------|----------|----------|------------|-------------------\n",
        "01    | 689              | 4        | 2        |   3       | 13.300\n",
        "02    | 832              | 4        | 4        |   4       | 12.250\n",
        "03    | 613              | 4        | 2        |   2       | 9.100\n",
        "04    | 557              | 3        | 2        |   3       | 6.650\n",
        "04    | 370              | 2        | 2        |   1       | 3.150\n",
        "\n",
        "Agora temos a seguinte expressão para encontrar o valor de uma casa:\n",
        "$$\n",
        "Preço = w0 + w1 \\times Dimensão +w2 \\times Quartos + w3 \\times Banheiros + w4 \\times Idade\n",
        "$$\n",
        "\n",
        "Vamos testar os valores..."
      ],
      "metadata": {
        "id": "GcDnZPBRXmPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Preco = np.dot(X,w)\n",
        "print(Preco)"
      ],
      "metadata": {
        "id": "V0xrQF4FWTVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?export=view&id=1u9QRq9uYx1m0hhcG2twE5R-z4LAWov4F' width=\"200\"></center>\n",
        "\n",
        "Sério???"
      ],
      "metadata": {
        "id": "WWL0cYmopmCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "-4.694329896909746 + 0.11907216494844867*689 -9.067525773195257*4 -6.61391752577236 * 2 -4.84948453608218*3"
      ],
      "metadata": {
        "id": "YMRaHqYApKd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradiente descendente\n",
        "\n",
        "Gradiente descendente é o coração e a alma da maioria dos algoritmos aprendizado de máquina. É longe a estratégia de otimização mais popular usada em aprendizado de máquina e aprendizado profundo no momento.\n",
        "\n",
        "Ele é usado no treinamento de modelos de dados, pode ser combinado com todos os algoritmos e é fácil de entender e implementar. Todos que trabalham com aprendizado de máquina devem entender seu conceito.\n",
        "\n",
        "O Gradiente Descendente (GD) é um algoritmo utilizado para encontrar o mínimo de uma função de forma iterativa.\n",
        "\n",
        "![Wall-E](https://drive.google.com/uc?export=view&id=1CYGNFXo5ZF3MkPHtDzjBm40D85TFXTsd)\n",
        "\n",
        "\n",
        "Para entender como funciona o gradiente descendente, considere uma função multivariável $f(\\boldsymbol{\\omega})$, onde $\\boldsymbol{\\omega} = [\\omega_1, \\omega_2, \\ldots, \\omega_n]^T$. Para encontrar o $\\boldsymbol{\\omega}$ em que esta função atinge um mínimo, o método do gradiente descendente usa as seguintes etapas:\n",
        "\n",
        "1. Escolha um valor aleatório inicial de $\\boldsymbol{\\omega}$;\n",
        "2. Escolha o número de iterações máximas $T$;\n",
        "3. Escolha um valor para a taxa de aprendizado $\\alpha$\n",
        "4. Repita os seguintes passos até que $f$ não mude mais ou até que as iterações excedam $T$\n",
        " * Calcular: $\\Delta \\boldsymbol{\\omega} = - \\alpha \\boldsymbol{J}_\\boldsymbol{\\omega}\\left(f(\\boldsymbol{\\omega})\\right) $\n",
        " * Atualizar: $\\boldsymbol{\\omega}\\leftarrow \\boldsymbol{\\omega} + \\Delta \\boldsymbol{\\omega}$\n",
        "\n",
        "Aqui $\\boldsymbol{J}_\\boldsymbol{\\omega} $ denota o Jacobiano de $f(\\boldsymbol{\\omega}) $ dado por:\n",
        "$$\n",
        "\\boldsymbol{J}_\\boldsymbol{\\omega}\\left(f(\\boldsymbol{\\omega})\\right)  =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial f(\\boldsymbol{\\omega})}{\\partial \\omega_1} \\\n",
        "\\frac{\\partial f(\\boldsymbol{\\omega})}{\\partial \\omega_2} \\\n",
        "\\cdots\\ \\frac{\\partial f(\\boldsymbol{\\omega})}{\\partial \\omega_n}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Por exemplo, se considerarmos a seguinte função:\n",
        "$$\n",
        "f(\\omega_1,\\omega_2) = \\omega_1^2+\\omega_2^2,\n",
        "$$\n",
        "a cada iteração, o vetor $\\boldsymbol{\\omega}$ é atualizado como:\n",
        "$$\n",
        "\\begin {bmatrix}\n",
        "\\omega_1 \\ \\omega_2\n",
        "\\end {bmatrix} \\leftarrow\n",
        "\\begin {bmatrix}\n",
        "\\omega_1 \\ \\omega_2\n",
        "\\end {bmatrix} - \\alpha\n",
        "\\begin {bmatrix}\n",
        "2\\omega_1 \\ 2\\omega_2\n",
        "\\end {bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "tSjC30ugd7Qs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercício dos slides\n",
        "\n",
        "Encontre os mínimos locais da função $f(\\omega)=(\\omega+5)^2$ começando no ponto $\\omega=3$."
      ],
      "metadata": {
        "id": "FzZ5rqcD4tgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "w = np.linspace(-10, 0, 200)\n",
        "y = (w+5)**2\n",
        "plt.title(r'$f(\\omega)=(\\omega+5)^2$')\n",
        "plt.xlabel(r'$\\omega$')\n",
        "plt.ylabel(r'$f(\\omega)$')\n",
        "plt.plot(w, y, color = 'forestgreen', linewidth = 2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uAkW4AD2AMnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialização de parâmetros\n",
        "w0 = 3                 # valor inicial\n",
        "alfa = 0.01            # taxa de aprendizado\n",
        "T = 10000              # máximo número de iterações\n",
        "eps = 1e-6             # precisado\n",
        "iters = 0              # contador de iterações\n",
        "f = lambda w: (w+5)**2 # gradiente da função\n",
        "df = lambda w: 2*(w+5) # gradiente da função\n",
        "step = 1e9             # valo"
      ],
      "metadata": {
        "id": "M27Gq-3glp5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fw=[]\n",
        "while step > eps and iters < T:\n",
        "    w1 = w0 - alfa * df(w0)                        # Grad descendente\n",
        "    step = abs(w1 - w0)                            # Passo de w\n",
        "    iters = iters+1                                # Contador de iterações\n",
        "    print(\"Iter \",iters,\"\\nX valor \",w1)  # Print iterações\n",
        "    w0 = w1                                        # valor atual de w é armazenado em valor prévio de w\n",
        "    fw.append([iters,w1,f(w1)])\n",
        "fw = np.array(fw)\n",
        "print(\"O mínimo local ocorre em\", w1)"
      ],
      "metadata": {
        "id": "0Ixwy2aT8abU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os gráficos abaixo plotam a iteração pelo valor de $f(\\omega)$ e de $\\omega$. Pode-se perceber que há convergência do método após, aproximadamente, 300 iterações. Porém, dado o alto valor de precisão que selecionamos"
      ],
      "metadata": {
        "id": "x2uihqb-QLYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots()\n",
        "\n",
        "ax.plot(fw[:,0], fw[:,2], color='navy' )\n",
        "ax.set_xlabel('Número da iteração',fontsize=14)\n",
        "ax.set_ylabel(r'$f(\\omega)$',color='navy',fontsize=14)\n",
        "\n",
        "ax2=ax.twinx()\n",
        "\n",
        "ax2.plot(fw[:,0], fw[:,1], color='seagreen' )\n",
        "ax2.set_ylabel(r'$\\omega$',color='seagreen',fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEDPdP9eQxC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A função abaixo serve para ilustrar um dos problemas do Gradiente Descendente.\n",
        "\n",
        "*A descida do gradiente é um Método de Otimização de Primeira Ordem. Leva em consideração apenas as derivadas de primeira ordem da função e despreza as de mais altas ordens. O que isso significa basicamente é que o método não tem idéia sobre a curvatura da função. Ele pode dizer se a função está diminuindo e quão rápido, mas não pode diferenciar se a curva é um plano, uma curva para cima ou para baixo.*"
      ],
      "metadata": {
        "id": "QPhSWCjJef36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def makeplot(position,angle1,angle2,rotation,alpha):\n",
        "    ax = fig.add_subplot(position,projection='3d')\n",
        "    ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
        "                cmap='viridis', edgecolor='none', alpha = alpha)\n",
        "    ax.view_init(angle1,angle2)\n",
        "    ax.set_xlabel(r'$x_1$', fontsize=15, labelpad=10)\n",
        "    ax.set_ylabel(r'$x_2$', fontsize=15, labelpad=10)\n",
        "    ax.zaxis.set_rotate_label(False)\n",
        "    ax.set_zlabel(r'$L^1$', rotation=rotation, fontsize=16, labelpad=10)\n",
        "    return ax"
      ],
      "metadata": {
        "id": "K175EWCKa-9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y = np.meshgrid(np.arange(-2, 8, .1), np.arange(-2, 8, .1))\n",
        "Z = np.sin(X)-Y/10"
      ],
      "metadata": {
        "id": "wEUT2IiNbLMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(50,10))\n",
        "ax = makeplot(121,30,80,90,0.95)\n",
        "fig.colorbar(plt.cm.ScalarMappable(cmap='viridis'), ax = ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5uWabe3nbONW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = mpl.cm.jet(np.hypot(X,Y))\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "ax.contourf(X,Y,Z, facecolors=colors)"
      ],
      "metadata": {
        "id": "sqBU3Vv-fVql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y = np.meshgrid(np.arange(-2, 8, .3), np.arange(-2, 8, .3))\n",
        "Z = np.sin(X)-Y/10\n",
        "V,U = np.gradient(Z, .2, .1)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "cmap = plt.get_cmap()\n",
        "q = ax.quiver(X,Y,U,V, Z, cmap = 'viridis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GltaQSmff4J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradiente descendente com Momentum\n",
        "\n",
        "O Momentum propõe o seguinte ajuste para a descida gradiente.\n",
        "$$\n",
        "𝑚=\\beta 𝑚−\\alpha 𝐽(\\omega^{(j)})\n",
        "$$\n",
        "$$\n",
        "\\omega^{(j+1)} = \\omega^{(j)} + 𝑚\n",
        "$$\n",
        "onde $𝑚$ é o gradiente que é mantido nas iterações anteriores. Este gradiente retido é multiplicado por um valor denominado *Coeficiente de Momentum* $\\beta$, que é a porcentagem do gradiente retido a cada iteração. Em geral, adota-se $\\beta=0,9$.\n"
      ],
      "metadata": {
        "id": "V7KFknW-jVfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialização de parâmetros\n",
        "w0 = 3                 # valor inicial\n",
        "alfa = 0.01            # taxa de aprendizado\n",
        "beta = 0.4             # coeficiente de momentum\n",
        "T = 10000              # máximo número de iterações\n",
        "eps = 1e-6             # precisado\n",
        "iters = 0              # contador de iterações\n",
        "f = lambda w: (w+5)**2 # gradiente da função\n",
        "df = lambda w: 2*(w+5) # gradiente da função\n",
        "step = 1e9"
      ],
      "metadata": {
        "id": "y7dv8P6amlwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fw=[]\n",
        "m = 0\n",
        "while step > eps and iters < T:\n",
        "    m = beta*m - alfa * df(w0)                     #m\n",
        "    w1 = w0 + m                                      # Grad descendente\n",
        "    step = abs(w1 - w0)                            # Passo de w\n",
        "    iters = iters+1                                # Contador de iterações\n",
        "    print(\"Iter \",iters,\"\\nX valor \",w1)  # Print iterações\n",
        "    w0 = w1                                        # valor atual de w é armazenado em valor prévio de w\n",
        "    fw.append([iters,w1,f(w1)])\n",
        "fw = np.array(fw)\n",
        "print(\"O mínimo local ocorre em\", w1)"
      ],
      "metadata": {
        "id": "3o1Y1zbYmydJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots()\n",
        "\n",
        "ax.plot(fw[:,0], fw[:,2], color='navy' )\n",
        "ax.set_xlabel('Número da iteração',fontsize=14)\n",
        "ax.set_ylabel(r'$f(\\omega)$',color='navy',fontsize=14)\n",
        "\n",
        "ax2=ax.twinx()\n",
        "\n",
        "ax2.plot(fw[:,0], fw[:,1], color='seagreen' )\n",
        "ax2.set_ylabel(r'$\\omega$',color='seagreen',fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "etaxfdzWn5D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lição de casa\n",
        "\n",
        "Use o Gradiente descendente com momentum e ache o vetor $\\mathbf \\omega$ para a regressão linear do conjunto de dados da tabela abaixo. Na equação,\n",
        "$$\n",
        "y = \\omega_0 + \\omega_1\\times x_1 + w_2 \\times x_2\n",
        "$$\n",
        "$𝑦$ é a taxa de gordura no sangue, $\\omega_i$ são os pesos e $x_i$ os dados de entrada.\n",
        "\n",
        "Índice  | Peso $(Kg)$  |Idade (anos)| Taxa de Gordura no sangue\n",
        "--------|--------------|------------|--------------------------\n",
        " 1  |  84 | 46 | 354\n",
        " 2  |  73 | 20 | 190\n",
        " 3  |  65 | 52 | 405\n",
        " 4  |  70 | 30 | 263\n",
        " 5  |  76 | 57 | 451\n",
        " 6  |  69 | 25 | 302\n",
        " 7  |  63 | 28 | 288\n",
        " 8  |  72 | 36 | 385\n",
        " 9  |  79 | 57 | 402\n",
        "10  |  75 | 44 | 365\n",
        "11  |  27 | 24 | 209\n",
        "12  |  89 | 31 | 290\n",
        "13  |  65 | 52 | 346\n",
        "14  |  57 | 23 | 254\n",
        "15  |  59 | 60 | 395\n",
        "16  |  69 | 48 | 434\n",
        "17  |  60 | 34 | 220\n",
        "18  |  79 | 51 | 374\n",
        "19  |  75 | 50 | 308\n",
        "20  |  82 | 34 | 220\n",
        "21  |  59 | 46 | 311\n",
        "22  |  67 | 23 | 181\n",
        "23  |  85 | 37 | 274\n",
        "24  |  55 | 40 | 303\n",
        "25  |  63 | 30 | 244\n"
      ],
      "metadata": {
        "id": "DvQKiFuywLjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GABARITO"
      ],
      "metadata": {
        "id": "-1b9vJKmTcKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parte 1\n",
        "\n",
        "__Input:__ Dados de treinamento $(\\mathbf{X}, \\mathbf{y})$ para onde:\n",
        "$\\mathbf{X}$ é uma matriz $m \\times n$,\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix}\n",
        "x_{11} & x_{12} & \\cdots & x_{1n} \\\\\n",
        "x_{21} & x_{22} & \\cdots & x_{2n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{m1} & x_{m2} & \\cdots & x_{mn}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "\\mathbf{x}^{1} & \\mathbf{x}^{2} & \\cdots & \\mathbf{x}^{n}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "onde $m$ é o número de dados para treinamento e $n$ o número de características. Cada característica $j = 1, \\cdots n$ está armazenada no vetor $\\mathbf{x}^{j}$, de dimensão $m$. $\\mathbf{y}$ é um vetor de dimensão $m$, contendo a saída."
      ],
      "metadata": {
        "id": "eTC8Lmtszj0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "FjxxDP2kVc_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "data = pd.read_csv(io.StringIO(uploaded['LicaoCasa.csv'].decode('utf-8')),sep=',',index_col=False)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "mBxrdsInWfBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['Peso', 'Idade']].to_numpy()\n",
        "y = data[['Taxa de Gordura']].to_numpy()"
      ],
      "metadata": {
        "id": "6TA4Svfb5pgs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parte 2\n",
        "Veja a classe `LinearRegression` definida abaixo.\n",
        "\n",
        "O método construtor `__init__` define as dimensões $m$ (número de linhas e, portanto, número de dados) e $n$ (número de colunas e, portanto, número de características). O método também inicializa as variáveis randomicamente  $\\omega_0, \\omega_1$ e $\\omega_2$. A taxa de aprendizado $\\alpha$ e o número de iterações são valores definidos dentro da classe. Os demais métodos apenas retornam os valores dos dados de entrada $\\mathbf{X}$, saída $\\mathbf{y}$ e dos pesos $\\boldsymbol{\\omega}$\n"
      ],
      "metadata": {
        "id": "MQGbZmlb5h61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "    def __init__(self,X,Y):\n",
        "        ones=np.ones((X.shape[0],1))\n",
        "        X=np.append(ones,X,axis=1)\n",
        "        self.X=X\n",
        "        self.Y=Y[:,0]\n",
        "        self.m=X.shape[0]\n",
        "        self.n=X.shape[1]\n",
        "        self.w=np.random.randn(X.shape[1])\n",
        "        self.alpha = 0.001\n",
        "        self.num_of_iter = 10000\n",
        "\n",
        "    def returnW(self):\n",
        "        return self.w\n",
        "\n",
        "    def returnX(self):\n",
        "        return self.X\n",
        "\n",
        "    def returnY(self):\n",
        "        return self.Y"
      ],
      "metadata": {
        "id": "z5g7wmjBT-B1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=LinearRegression(X,y)"
      ],
      "metadata": {
        "id": "lB9yQtu6VTWj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w=lr.returnW()\n",
        "print('Valores aleatórios iniciais:\\n',\n",
        "      'w0:{:4.4f}\\n w1{:4.4f} \\n w2: {:4.4f}'.format(w[0],w[1],w[2]))"
      ],
      "metadata": {
        "id": "YvYYUEsZVWp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parte 3\n",
        "\n",
        "Nessa parte, os demais métodos são desenvolvidos. Particularmente, os métodos `GradientDescent` e `NormalEquation` encontram a solução de forma iterativa e direta, respectivamente.\n",
        "\n",
        "O métodos `CostFunction` calcula o erro da aproximação e o método `Predict` faz uma predição a partir do modelo aproximado encontrado.\n",
        "\n",
        "Veja abaixo o algoritmo implementado no método `GradientDescent`.\n",
        "\n",
        "__For__ $iter=1$ to $iters$:\n",
        "\n",
        "* Calcule as previsões: $h_\\omega(x^{1}_i,x^{2}_i) = \\omega_0 + \\omega_1 x^{1}_i + + \\omega_2 x^{2}_i$\n",
        "* Calcule o custo: $J(\\omega_0, \\omega_1) = \\frac{1}{2N} \\sum_{i=1}^{N} (h_\\omega(x_i) - y_i)^2$\n",
        "* Atualize os parâmetros:\n",
        "     * $\\omega_0 := \\omega_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\omega(x^{1}_i,x^{2}_i) - y_i)$\n",
        "     * $\\omega_1 := \\omega_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\omega(x^{1}_i,x^{2}_i) - y_i) \\cdot x^1_i$\n",
        "     * $\\omega_2 := \\omega_2 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\omega(x^{1}_i,x^{2}_i) - y_i) \\cdot x^2_i$\n",
        "\n",
        "__EndFor__\n",
        "\n",
        "__Output:__ Parâmetros $\\omega_0, \\omega_1$ e $\\omega_2$ que minimizam a função de custo"
      ],
      "metadata": {
        "id": "liNCxEKaHMCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "    def __init__(self,X,Y):\n",
        "        ones=np.ones((X.shape[0],1))\n",
        "        X=np.append(ones,X,axis=1)\n",
        "        self.X=X\n",
        "        self.Y=Y[:,0]\n",
        "        self.m=X.shape[0]\n",
        "        self.n=X.shape[1]\n",
        "        self.w=np.random.randn(X.shape[1])\n",
        "        self.alpha = 0.0003\n",
        "        self.num_of_iter = 600000\n",
        "\n",
        "    def CostFunction(self):\n",
        "        h=np.matmul(self.X,self.w)\n",
        "        self.J=(1/(2*self.m))*np.sum((h-self.Y)**2)\n",
        "        return self.J\n",
        "\n",
        "    def GradientDescent(self):\n",
        "        self.Cost_history=[]\n",
        "        self.w_history=[]\n",
        "        num_prints = round(self.num_of_iter/min(self.num_of_iter,10000))\n",
        "        step = self.num_of_iter / num_prints\n",
        "        boundaries = [round(step * i) for i in range(num_prints + 1)]\n",
        "        h=np.matmul(self.X,self.w)\n",
        "        J=self.CostFunction()\n",
        "        for i,x in enumerate(range(self.num_of_iter)):\n",
        "            h=np.matmul(self.X,self.w)\n",
        "            J=self.CostFunction()\n",
        "            self.Cost_history.append(J)\n",
        "            self.w_history.append(self.w)\n",
        "            temp = h-self.Y\n",
        "            value = self.alpha/self.m\n",
        "            self.w=self.w - value*np.matmul(temp,self.X)\n",
        "            divisible_numbers = []\n",
        "            if i>100:\n",
        "              if i in boundaries:\n",
        "                plt.scatter(i, J)\n",
        "                display.display(plt.gcf())\n",
        "                display.clear_output(wait=True)\n",
        "            else:\n",
        "              plt.scatter(i, J)\n",
        "              display.display(plt.gcf())\n",
        "              display.clear_output(wait=True)\n",
        "        plt.show()\n",
        "        return self.w,self.Cost_history,self.w_history\n",
        "\n",
        "    def PredictSurface(self,X1,X2):\n",
        "        X0 = np.ones((X1.shape[0],1))\n",
        "        y_pred = self.w[0] * X0 + self.w[1] * X1 + self.w[2] * X2\n",
        "        return y_pred\n",
        "\n",
        "    def NormalEquation(self):\n",
        "        self.w = np.matmul(np.linalg.inv(np.matmul(self.X.T,self.X)),np.matmul(self.X.T,self.Y))\n",
        "        y_pred=np.matmul(self.X,self.w)\n",
        "        return y_pred,(abs(self.Y-y_pred)/self.Y)*100\n",
        "\n",
        "    def returnW(self):\n",
        "        return self.w\n",
        "\n",
        "    def returnX(self):\n",
        "        return self.X\n",
        "\n",
        "    def returnY(self):\n",
        "        return self.Y"
      ],
      "metadata": {
        "id": "IqUCBAerGMiG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=LinearRegression(X,y)\n",
        "y_pred_normal,error_percentage=lr.NormalEquation()\n",
        "w = lr.returnW()\n",
        "print('Valores aleatórios iniciais:\\n',\n",
        "      'w0: {:4.4f}\\n w1: {:4.4f} \\n w2: {:4.4f}'.format(w[0],w[1],w[2]))"
      ],
      "metadata": {
        "id": "6VEKX8YdGjuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=LinearRegression(X,y)\n",
        "w,Cost_history,w_history = lr.GradientDescent()\n",
        "print('Valores finais dos pesos:\\n',\n",
        "      'w0:{:4.4f}\\n w1: {:4.4f} \\n w2: {:4.4f}'.format(w[0],w[1],w[2]))"
      ],
      "metadata": {
        "id": "1b0Y5M4xLHQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O gráfico plotado durante o _treinamento_ é um pequeno spolier do que estudaremos no próximo ciclo - os 100 primeiros valores são plotados, e depois, somente a cada aprox. 20mil iterações. Veja que o erro cai bastante no início e depois fica praticamente constante - isto é, quase não melhoramos nossa resposta aumentando o número de iterações. Se quisermos um resultado melhor, devemos tentar diferentes estratégias...\n",
        "\n",
        "Porém, repare a diferença entre os resultados de 400mil e 600mil épocas.\n",
        "\n",
        "Para isso você deve analisar a saída `w_history` do treinamento. Veja que o histórico dos pesos $\\omega_i$ e do custo $J$ são armazenados:\n",
        "\n",
        "\n",
        "```\n",
        "w,Cost_history,w_history = lr.GradientDescent()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "6ELJ8Sk9p0Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Valores dos pesos na iteração:\\n',\n",
        "      'w0:{:4.4f}\\n w1: {:4.4f} \\n w2: {:4.4f}'.format(w_history[399999][0],w_history[399999][1],w_history[399999][2]))"
      ],
      "metadata": {
        "id": "jJQR3x2ZwN5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo, a superfície encontrada é plotada junto com os pontos de treinamento. Obviamente, temos muito poucos pontos para analisar."
      ],
      "metadata": {
        "id": "gkui3mWqtjcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "\n",
        "x1data = lr.returnX()[:,1]\n",
        "x2data = lr.returnX()[:,2]\n",
        "ydata = lr.returnY()\n",
        "\n",
        "# Superfície\n",
        "x1_range = np.arange(x1data.min(), x1data.max())\n",
        "x2_range = np.arange(x2data.min(), x2data.max())\n",
        "\n",
        "X1, X2 = np.meshgrid(x1_range, x2_range)\n",
        "Y = lr.PredictSurface(X1,X2)\n",
        "ax.plot_surface(X1,X2,Y, rstride=1, cstride=1, alpha = 0.2)\n",
        "\n",
        "# Pontos experimentais\n",
        "ax.scatter3D(x1data, x2data, ydata, c=ydata, cmap='Greens');\n",
        "\n",
        "ax.view_init(10, 120)\n",
        "ax.set_xlabel(r'$x^1$');\n",
        "ax.set_ylabel(r'$x^2$');\n",
        "ax.set_zlabel('y');"
      ],
      "metadata": {
        "id": "04mIcUq4_DGv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}